[
  {
    "objectID": "03_Reachability.html",
    "href": "03_Reachability.html",
    "title": "3  Graph Reachability",
    "section": "",
    "text": "3.1 The Graph Reachability Problem",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#the-graph-reachability-problem",
    "href": "03_Reachability.html#the-graph-reachability-problem",
    "title": "3  Graph Reachability",
    "section": "",
    "text": "Definition 3.1 (The Graph Reachability Problem) Given a directed graph \\(G = (V,A)\\) and a node \\(s\\). Find the set \\(M\\) of all nodes that are reachable from \\(s:\\)\n\ni.e., all nodes that are connected to \\(s\\).\n\n\n\nExample 3.1 Consider the graph \\(G\\) given in Figure 6 (a). The set of reachable nodes from \\(s=1\\) is \\(M = \\{1,2,4,5\\}\\) (see Figure 6 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Directed graph \\(G\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M = \\{1,2,4,5\\}\\) for \\(s=1\\).\n\n\n\n\n\n\n\nFigure 3.1: Graph \\(G\\) and reachable set \\(M\\) from node \\(s=1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#algorithm-idea",
    "href": "03_Reachability.html#algorithm-idea",
    "title": "3  Graph Reachability",
    "section": "3.2 Algorithm Idea",
    "text": "3.2 Algorithm Idea\nThe following steps give an intuitive process for finding reachable nodes from a given node \\(s\\).\n\nExplore \\(s\\): start from \\(s\\) and follow the arcs in its forward star to find its neighbors.\nRepeat with each neighbor of neighbors of \\(s\\).\nRepeat with neighbor of neighbors of neighbors of \\(s\\), etc.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#illustration-of-idea",
    "href": "03_Reachability.html#illustration-of-idea",
    "title": "3  Graph Reachability",
    "section": "3.3 Illustration of Idea",
    "text": "3.3 Illustration of Idea\nLet’s apply this idea for the graph \\(G\\) given in Figure 6 (a) with source node \\(s = 1\\).\n\n3.3.1 Step 1\nThe forward star of \\(s\\) is \\[\n    \\delta^+(\\{1\\}) = \\{(1,2), (1,4)\\}.\n\\] This implies that \\(2\\) and \\(4\\) are both reachable from \\(1\\). That is, \\(M\\) contains \\(\\{1,2,4\\}\\). See Figure 6 (a).\n\n\n3.3.2 Step 2\nWe need to explore further from nodes \\(2\\) and \\(4\\). Let’s start with \\(2\\). The only node adjacent to \\(2\\) is node \\(5\\). We can conclude that \\(M\\) contains \\(\\{1,2,4,5\\}\\). See Figure 6 (b).\n\n\n3.3.3 Step 3\nLet’s explore from node \\(4\\): \\(2\\) is the only node adjacent to \\(4\\). Our partially computed reachable set \\(M\\) is unchanged. See Figure 6 (c).\n\n\n3.3.4 Step 4\nWe haven’t explored from node \\(5\\) yet. Let’s do that now. The only arc incident with \\(5\\) is \\((5,4)\\). Thus, \\(4\\) is adjacent to \\(5\\) and, hence, reachable from \\(1\\). We already knew that \\(4\\) is in \\(M\\), so \\(M\\) is unchanged. See Figure 6 (d).\n\n\n3.3.5 Step 5\nWe have a choice of nodes to explore from: \\(\\{2, 4, 5\\}\\). We have already explored each of these nodes and appear to be stuck in a loop. Can we conclude that \\(M = \\{1,2,4,5\\}\\)?\n\n\n\n\n\n\n\n\n\n\n\n(a) Step 1: \\(M \\supseteq \\{1,2,4\\}\\) so far\n\n\n\n\n\n\n\n\n\n\n\n(b) Step 2: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Step 3: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) Step 4: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\nFigure 3.2: Steps of the intuitive reachability process. We need to revise the algorithm to decide when to terminate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#avoiding-cycles",
    "href": "03_Reachability.html#avoiding-cycles",
    "title": "3  Graph Reachability",
    "section": "3.4 Avoiding Cycles",
    "text": "3.4 Avoiding Cycles\n\n3.4.1 Goal and Notation\nWe need to avoid exploring nodes we have already explored.\nLet’s define \\(M\\) as the set of nodes we have reached and explored.\n\nWhen we reach a node, we can check if it is already in \\(M\\).\nIf it is, we’ve explored it already and shouldn’t again.\n\nLet’s introduce another set \\(Q\\) as a queue of nodes which have been reached but not explored.\n\n\n3.4.2 Algorithm Idea\nInitialize \\(Q\\) as \\(Q = \\{s\\}\\). Each iteration:\n\nChoose vertex \\(i \\in Q\\) to explore.\nAdd to \\(Q\\) any neighbors of \\(i\\) that are not already in \\(Q\\) or \\(M\\).\nAdd \\(i\\) to \\(M\\) since it has been explored.\n\nIf \\(Q = \\{\\} = \\emptyset\\) (empty set), then stop:\n\nWe’ve explored all nodes reachable from \\(s\\).\n\\(M\\) is the set of nodes reachable from \\(s\\).\n\n\n\n3.4.3 Graph Reachability Algorithm (Pseudocode)\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#returning-to-the-example",
    "href": "03_Reachability.html#returning-to-the-example",
    "title": "3  Graph Reachability",
    "section": "3.5 Returning to the Example",
    "text": "3.5 Returning to the Example\nLet’s consider the graph \\(G\\) given in Figure 6 (a) and apply the algorithm to find the set of nodes reachable from \\(s=1\\).\n\n3.5.1 Iteration 1\nWe initialize \\(Q\\) and \\(M\\) as \\[\n    Q = \\{1\\}, \\hspace{0.25in}\n    M = \\emptyset.\n\\] Let’s explore node \\(1\\):\n\nNode 1 has adjacency list \\(L(1) = \\{2,4\\}\\).\nLet’s add {2,4} to \\(Q\\).\nSince we have explored node \\(1\\), we remove \\(1\\) from \\(Q\\) and add \\(1\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.3: Iteration 1: Explore node \\(1\\)\n\n\n\n\n\n3.5.2 Iteration 2\nAfter the first iteration, we have\n\\[\n    Q = \\{2, 4\\}, \\hspace{0.25in}\n    M = \\{1\\}.\n\\]\nWe can continue from node \\(2\\) or node \\(4\\). Let’s explore node \\(2\\):\n\n\\(L(2) = \\{5\\}\\).\nAdd \\(5\\) to \\(Q\\).\nRemove \\(2\\) from \\(Q\\) and add \\(2\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.4: Iteration 2: Explore node \\(2\\)\n\n\n\n\n\n3.5.3 Iteration 3\nWe now have \\[\n    Q = \\{4, 5\\}, \\hspace{0.25in}\n    M = \\{1, 2\\}.\n\\]\nLet’s explore node \\(4\\):\n\n\\(L(4) = \\{2\\}\\).\nWe already have \\(2 \\in M\\); we do not add \\(2\\) to \\(Q\\).\nWe remove \\(4\\) from \\(Q\\) and add \\(4\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.5: Iteration 3: Explore node \\(4\\)\n\n\n\n\n\n3.5.4 Iteration 4\nAfter the first three iterations, we have \\[\n    Q = \\{5\\}, \\hspace{0.25in}\n    M = \\{1, 2, 4\\}.\n\\]\nExploring node \\(5\\), we note:\n\n\\(L(5) = 4\\).\nSince \\(4 \\in M\\) already, we move \\(5\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.6: Iteration 4: Explore node \\(5\\)\n\n\n\n\n\n3.5.5 Iteration 5 – Termination\nAt this stage \\(Q\\) is empty, so we cannot proceed. We conclude that \\(M = \\{1,2,4,5\\}\\)!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html",
    "href": "01_Introduction.html",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "1.1 Problems and Problem Instances\nA problem \\(P\\) is a general class of questions to answer.\nProblems are (infinite) families of general questions.\nWe call a version of the problem with specific values a problem instance \\(I\\) (or just instance).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#problems-and-problem-instances",
    "href": "01_Introduction.html#problems-and-problem-instances",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "Example 1.1 (Quadratic Equations) Finding the roots of a quadratic equation \\(ax^2 + bx + c = 0\\) is an example of a problem.\nFinding the roots of a specific quadratic function is an instance of this problem. For example, finding \\(x\\) such that \\(2x^2 + 8x + 5 = 0\\) is a problem instance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#algorithms",
    "href": "01_Introduction.html#algorithms",
    "title": "1  Introduction to Algorithms",
    "section": "1.2 Algorithms",
    "text": "1.2 Algorithms\nWe are interested in finding procedures for solving every possible instance of a given problem. Such a procedure is called an algorithm.\n\nDefinition 1.1 (Algorithm) An algorithm for a given problem is a finite sequence of operations which return the correct solution for all problem instances.\n\n\nExample 1.2 (An Algorithm for Quadratic Equations) We solve quadratic equations of the form \\(ax^2 + bx + c = 0\\) using the quadratic formula: \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\\]\nWe can think of evaluating this formula as an algorithm consisting of the steps:\n\nCompute \\(u:= b^2 - 4ac\\).\nCompute \\(v:= 2a\\).\nCompute \\(z := \\sqrt{u} = \\sqrt{b^2 - 4ac}\\).\nCompute \\[\nx_1 = \\frac{-b + z}{v}, \\hspace{0.5in}\nx_2 = \\frac{-b - z}{v}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#goals-of-the-module",
    "href": "01_Introduction.html#goals-of-the-module",
    "title": "1  Introduction to Algorithms",
    "section": "1.3 Goals of the Module",
    "text": "1.3 Goals of the Module\nWe are interested in:\n\nDesigning algorithms for solving specific problems.\nProving that a proposed algorithm produces correct solutions.\nComplexity of algorithms: Analysing how long it takes for the algorithm to terminate, i.e., how many operations are required in general.\nComplexity of problems: how long it may take to solve a problem, regardless of which algorithm is used.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#combinatorial-optimization",
    "href": "01_Introduction.html#combinatorial-optimization",
    "title": "1  Introduction to Algorithms",
    "section": "1.4 Combinatorial Optimization",
    "text": "1.4 Combinatorial Optimization\n\n1.4.1 Combinatorial Optimization Problems\nWe will focus on combinatorial optimization problems: \\[\\min c(X) \\text{ such that } X \\in \\mathcal{X},\\] where \\(c: \\mathcal{X} \\mapsto \\mathbf{R}\\) is a cost function and \\(\\mathcal{X}\\) is a finite set of feasible solutions.\n\n\n1.4.2 Solving Combinatorial Problems\n\nSince \\(\\mathcal{X}\\) is finite, we can always solve by complete enumeration: exhaustively calculating \\(c(X)\\) for each \\(X \\in \\mathcal{X}\\) to find one with minimum cost.\nIn practice, \\(\\mathcal{X}\\) is extremely large and complete enumeration is prohibitively expensive.\nIn many cases, \\(\\mathcal{X}\\) is defined implicitly as description and enumerating all possible solutions is a difficult task on its own.\nWe want faster, specialized algorithms exploiting mathematical structure of the given problem.\n\n\n\n1.4.3 Examples of Combinatorial Problems\nCombinatorial optimization is ubiquitous in operations research and management science1\n\nExample 1.3 (Travelling Salesperson Problem) Find a tour of shortest length visiting each location exactly once. Figure 1.1 gives an instance of the TSP and a solution where a tour of 15 cities in Germany is sought.\n\n\n\n\n\n\nFigure 1.1: An optimal traveling salesperson tour through Germany’s 15 largest cities (among over \\(43\\) billion possible routes).\n\n\n\n\n\nExample 1.4 (The Shortest Path Problem) Find shortest/minimum length route in network from origin to destination.\n\n\nExample 1.5 (Knapsack/Assignment Problems) Find maximum value/minimum cost distribution of limited resources.\n\n\n\n1.4.4 Types of Problems\nWe will focus on three primary forms of combinatorial optimization problems.\n\nDefinition 1.2 (Decision Problems) Given set \\(\\mathcal{X}\\), cost \\(c:\\mathcal{X}\\mapsto\\mathbf{R}\\) and scalar \\(L \\in \\mathbf{R}\\): \\[\\text{Determine if there is a } X \\in \\mathcal{X}\n        \\text{ with } c(X)\n        \\le L.\\]\n\nA decision problem takes problem instance defined by \\(\\mathcal{X}\\), \\(c\\), and \\(L\\) as input. An algorithm for this problem would output Yes or No depending on the instance.\n\nDefinition 1.3 (Decision problem – Search version) Given \\(\\mathcal{X}\\), \\(c\\), and \\(L\\) as input. Find \\(X \\in \\mathcal{X}\\) with \\(c(X) \\le L\\).\n\n\nDefinition 1.4 (Optimization problem – Search version) Given \\(\\mathcal{X}\\) and \\(c\\) as input. Find \\(X \\in \\mathcal{X}\\) minimizing \\(c(X)\\).\n\n\n\n1.4.5 Exact and Approximation Algorithms\nThere are two primary classes of algorithms that we will consider: exact and approximation algorithms.\n\nDefinition 1.5 (Exact Algorithms) Exact algorithms provide an exact solution to the problem.\n\n\nDefinition 1.6 (Approximation Algorithms) Approximation Algorithms provide an approximate solution to the problem, but not the exact solution in general.\n\nAn \\(\\alpha\\)-approximate algorithm returns \\(\\tilde X\\) within \\(\\alpha\\)-ratio of optimal value: \\(c(X^*) \\le c(\\tilde X) \\le \\alpha\n\\cdot c(X^*),\\) for some \\(\\alpha &gt; 1\\).\n\n\nAside from these two classes, we also have heuristics which provide potential solutions without guarantees of quality.\n\nDefinition 1.7 (Heuristics) Heuristic algorithms or heuristics provide solutions without guarantee of closeness to the optimal solution \\(c(X^*)\\).\n\n\n\n1.4.6 Deterministic vs Random Algorithms\nWe can also classify algorithms based on whether steps are performed deterministically or at random.\n\nDefinition 1.8 (Deterministic Algorithms) Deterministic Algorithms follow the same series of steps for given input: \\[ \\text{{if A = B do X; else do Y}} \\]\n\n\n\nDefinition 1.9 (Randomized Algorithms) Randomized algorithms have steps depending on random operations: \\[\\text{{Toss coin: if heads, do X; else do Y}}\\]\nA randomized exact/\\(\\alpha\\)-algorithm may only return exact or \\(\\alpha\\) solution within a certain probability.\n\n\n\n1.4.7 Offline vs Online vs Robust Algorithms\nFinally, we can further classify algorithms based on how they process information into problem instances.\n\nDefinition 1.10 (Offline Algorithm) The problem instance \\(I\\) is completely known at the beginning of the algorithm.\n\n\nDefinition 1.11 (Online Algorithm) The problem instance \\(I\\) is not completely known at start of the algorithm. Instead, the algorithm adapts to \\(I\\) as \\(I\\) unfolds over time.\n\n\nDefinition 1.12 Robust Algorithm: Instance \\(I\\) is not completely known at the beginning of the algorithm and it is not revealed over time.\nThe algorithm finds a solution that works for all possible realizations that \\(I\\) can take.\n\n\n\n1.4.8 Example – Canadian Traveller Problem (CTP)\nShortest Path Problem with added complexity of not knowing which roads/routes are unavailable due to the weather beforehand2.\n\n1.4.8.1 Deterministic/Off-line Version\nWe want to find the shortest or minimum length path in a network from origin to destination.\nIf the network routes are deterministic and are known then we have a deterministic exact algorithm for the shortest path problem. We’ll discuss this algorithm at length later in the term.\n\n\n1.4.8.2 The Random Case\nLet’s suppose that the routes are random or partially observed. For example, this could occur when inclement weather causes certain roadways to close, but they are not known until the road closure is encountered en route.\n\nAn online algorithm would find a partial path using known routes (so far), and then dynamically update the solution as conditions change or become known.\nA robust algorithm would find a path/itinerary that works under any weather conditions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#footnotes",
    "href": "01_Introduction.html#footnotes",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Combinatorial_optimization↩︎\nhttps://en.wikipedia.org/wiki/Canadian_traveller_problem↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html",
    "href": "02_Introduction_to_Graphs.html",
    "title": "2  Introduction to Graphs",
    "section": "",
    "text": "2.1 Directed Graphs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#directed-graphs",
    "href": "02_Introduction_to_Graphs.html#directed-graphs",
    "title": "2  Introduction to Graphs",
    "section": "",
    "text": "2.1.1 Introduction\n\nDefinition 2.1 (Directed Graphs) A directed graph is an ordered pair \\(G := (V, A)\\) composed of:\n\na set \\(V\\) of vertices or nodes of size/cardinality \\(n:=|V|\\);\na set \\(A\\) of ordered pairs called arcs of cardinality \\(m:= |A|\\).\n\nFor an arc \\((i,j) \\in A\\), we call \\(i\\) the tail and \\(j\\) the head.\n\n\nWe will focus on graphs without self-loops: \\((i,i)\\) is not an arc!\n\n\nExample 2.1 Consider \\(G = (V, A)\\) with \\[\\begin{aligned}\n    V &= \\{1,2,3,4,5\\},  \\\\ \\\\\n    A &= \\{(1,2), (1,3), (1,5), (2,4), (3,2), (4,1), (5,1), (5,3)\\}.\n\\end{aligned}\n\\]\nWe can represent or visualize \\(G\\) as nodes \\(V\\) joined by lines/arrows corresponding to arcs \\(A\\). Figure 2.1 gives several different representations of this graph.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A Visualization of the Graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Visualization\n\n\n\n\n\n\n\n\n\n\n\n(c) A Third Visualization\n\n\n\n\n\n\n\nFigure 2.1: Three representations of the graph given in Example 2.1.\n\n\n\n\n\n2.1.2 Adjacency\nArcs in a graph define pairwise relationships between nodes.\n\nDefinition 2.2 (Adjacent nodes) Node \\(i \\in V\\) is adjacent to node \\(j \\in V\\) if arc \\((i,j)\\) belongs to \\(A\\).\nIn this case, the arc \\((i,j) \\in A\\) is incident to node \\(j\\).\n\n\nExample 2.2 Consider the graph \\(G=(V,A)\\) given in Example 2.1. Node \\(1\\) is adjacent to \\(3\\) because the arc \\((1,3) \\in A\\) is included in the graph. However, Node \\(3\\) is adjacent to \\(1\\) since \\((3,1) \\notin A\\). See Figure 2.2 for illustrations of these relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(1\\) is adjacent to \\(3\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(3\\) is not adjacent to \\(1\\)\n\n\n\n\n\n\n\nFigure 2.2: Adjacency relations of nodes \\(1\\) and \\(3\\) in \\(G\\) given in Example 2.1.\n\n\n\n\n\n2.1.3 Complete Graphs\n\nDefinition 2.3 (Complete Graphs) A directed graph \\(G = (V, A)\\) is complete if:\n\n\\(A\\) contains an arc for each pair of nodes in \\(V\\);\nEquivalently, every node in \\(V\\) is adjacent to every other node.\n\n\n\nExample 2.3 The graph given in Example 2.1 is not complete or incomplete. Indeed, there are several potential arcs, e.g., \\((4,5)\\), which are not present in this graph.\nHowever, Figure 2.3 (a) provides a visualization of the complete graph on \\(4\\) nodes. Each of the possible arcs between pairs of nodes is present.\n\n\n\n\n\n\n\n\n\n\n\n(a) A Complete Graph with 4 nodes\n\n\n\n\n\n\n\n\n\n\n\n(b) An Incomplete Graph\n\n\n\n\n\n\n\nFigure 2.3: The complete graph with \\(n=4\\) nodes and the incomplete graph from Example 2.1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#number-of-arcs-in-a-complete-graph",
    "href": "02_Introduction_to_Graphs.html#number-of-arcs-in-a-complete-graph",
    "title": "2  Introduction to Graphs",
    "section": "2.2 Number of Arcs in a Complete Graph",
    "text": "2.2 Number of Arcs in a Complete Graph\n\nTheorem 2.1 For any directed graph \\(G\\), we have \\(m \\le n(n-1)\\). If \\(G\\) is complete then \\(m = n(n-1)\\).\n\n\nProof. Each of the \\(n\\) nodes could be adjacent to each of the other \\(n-1\\) nodes. Therefore the number of edges is bounded above by \\[\n    m \\le n(n-1).\n\\] When the graph is complete, every possible edge is present, i.e., every node is adjacent to every other node. Therefore, \\[\n    m = n(n-1)\n\\] if \\(G\\) is complete. On the other hand, \\(m &lt; n(n-1)\\) if \\(G\\) is not complete.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#dense-and-sparse-graphs",
    "href": "02_Introduction_to_Graphs.html#dense-and-sparse-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.3 Dense and Sparse Graphs",
    "text": "2.3 Dense and Sparse Graphs\n\nDefinition 2.4 (Dense/Sparse Graphs) A directed graph \\(G\\) is sparse if \\(m &lt;&lt; n(n-1)\\). Otherwise \\(G\\) is dense.\n\n\nExample 2.4 Figure 2.4 provides visualisations of three graphs with increasing density.\n\nA sparse graph with \\(5\\%\\) of possible edges present (Figure 2.4 (a)).\nA dense graph with \\(50\\%\\) of possible edges present (Figure 2.4 (b)).\nA very dense graph with \\(95\\%\\) of possible edges present (Figure 2.4 (c)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sparse graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Dense graph\n\n\n\n\n\n\n\n\n\n\n\n(c) Very dense graph\n\n\n\n\n\n\n\nFigure 2.4: Three graphs with \\(n=25\\) with increasing density.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#paths-and-connectivity",
    "href": "02_Introduction_to_Graphs.html#paths-and-connectivity",
    "title": "2  Introduction to Graphs",
    "section": "2.4 Paths and Connectivity",
    "text": "2.4 Paths and Connectivity\nWe can extend our definition of adjacency to describe pairwise relationships of nodes via sequences of arcs.\n\nDefinition 2.5 (Path) A path is a sequence of arcs \\[(i_1, i_2), (i_2, i_3), \\dots, (i_k, i_{k + 1})\\] with \\(k+1 \\ge 2\\) nodes with distinct origin \\(i_1\\) and destination \\(i_{k+1}\\).\n\nWe can also use the notation \\[\n    (i_1, i_2, \\dots, i_k, i_{k+1})\n\\] to denote a \\((i_1, i_{k+1})\\)-path.\n\nExample 2.5 The graph visualised in Figure 2.5 contains several paths from \\(2\\) to \\(3\\). For example, both \\(P_1 = ((2,4), (4,1), (1,3))\\) and \\(P_2 = ((2,4), (4,1), (1,5), (5,3))\\) are \\(23\\)-paths.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Path \\(P_1 = ((2,4), (4,1), (1,3))\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Path \\(P_2 = ((2,4), (4,1), (1,5), (5,3))\\)\n\n\n\n\n\n\n\nFigure 2.5: Two \\(23\\)-paths, \\(P_1\\), \\(P_2\\), in graph \\(G\\).\n\n\n\n\nDefinition 2.6 (Connectivity) Node \\(v\\) is connected to node \\(w\\) if there is path in \\(G\\) with origin \\(i_1 = v\\) and destination \\(i_{k+1} = w\\).\n\n\nDefinition 2.7 A graph is connected if every pair of nodes is connected.\n\n\nExample 2.6 Node \\(2\\) is connected to Node \\(3\\) in the graph given in Example 2.5. Indeed, we saw that there are at least two \\(23\\)-paths in Example 2.5.\nMoreover, the graph \\(G\\) is connected because we every pair of nodes is connected via an arc. Indeed, we can use the subpaths of the paths \\(P_1\\), \\(P_2\\) and the \\(34\\)-path \\[\n    P_3 = ((3,2), (2,4))\n\\]\nto reach every node from every other node. For example, we can use parts of \\(P_2\\) and \\(P_3\\) to find the \\(14\\)-path \\[\n    P_{14} = ((1,5), (5,3), (3,2), (3,4)).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(P_3 = ((3,2), (2,4))\\)\n\n\n\n\n\n\n\nFigure 2.6: A \\(34\\)-path \\(P_3\\) in \\(G\\). This, along with \\(P_1, P_2\\) from Example 2.5 provides paths between every pair of nodes in \\(G\\); hence, \\(G\\) is connected.\n\n\n\n\n2.4.1 Cycles\nIf a path begins and ends at the same node, then we call it a cycle.\n\nDefinition 2.8 (Cycles) A cycle is a sequence \\[(i_1, i_2), (i_2, i_3), \\dots, (i_k, i_{k + 1})\\] of \\(k \\ge 2\\) consecutive and distinct arcs with \\(i_{k+1} = i_1\\) (i.e., the origin and destination coincide).\n\nNote that a cycle may visit some nodes more than once (other than the origin/destination).\n\nExample 2.7 The graph \\(G\\) given in Figure 2.7 contains the cycles \\(C_1 = (2,5,4,3,2)\\) and \\(C_2 = (1,3,5,4,2,1)\\). Note that \\(C_2\\) visits node \\(2\\) twice. This implies that we also have cycles \\(C_3 = (1,3,2,1)\\) and \\(C_4 = (2,5,4,2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cycle \\(C_1 = (2,5,4,3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Cycle \\(C_2 = (1,3,5,4,2,1)\\)\n\n\n\n\n\n\n\nFigure 2.7: A graph \\(G\\) containing cycles \\(C_1\\) and \\(C_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#directed-cuts",
    "href": "02_Introduction_to_Graphs.html#directed-cuts",
    "title": "2  Introduction to Graphs",
    "section": "2.5 Directed Cuts",
    "text": "2.5 Directed Cuts\nHaving introduced notions of connectivity, we now define sets of arcs whose removal disconnect the graph.\n\nDefinition 2.9 (Forward Cut) Let \\(S \\subseteq V\\), that is, \\(S\\) is a subset of the node set \\(V\\).\nThe set of arcs with tail in \\(S\\) and head in \\(V\\setminus S\\) is the forward directed cut induced by \\(S\\): \\[\\delta^+(S) := \\{(i,j) \\in A: i \\in S \\text{ and } j \\in V\\setminus S\\}.\\]\n\nInformally, the forward directed cut is the set of arcs that leave \\(S\\).\n\nDefinition 2.10 (Backward Cut) The backward directed cut induced by \\(S\\) is the set of arcs with tail in \\(V\\setminus S\\) and head in \\(S\\):\n\\[\\delta^-(S) := \\delta^+(V\\setminus S) = \\{(i,j) \\in A: i \\in V\\setminus S  \\text{ and } j \\in S \\}.\\]\n\nThe backward directed cut is the set of arcs entering \\(S\\).\n\nExample 2.8 Consider \\(S = \\{4, 5\\}\\) for the graph given in Figure 2.8 (a). This graph and set of nodes has forward and backward cuts \\[\n    \\delta^+(\\{4,5\\}) = \\{(4,2), (4,3), (5,1)\\}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\delta^+(\\{4,5\\}) = \\{(4,2), (4,3), (5,1)\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(\\delta^-(\\{4,5\\}) = \\{(2,5\\}\\)\n\n\n\n\n\n\n\nFigure 2.8: Forward and backward cuts of \\(S = \\{4,5\\}\\) is graph \\(G\\).\n\n\n\n\n2.5.1 Stars and Degrees\n\nDefinition 2.11 (Stars) Let \\(i \\in V\\). The forward and backward stars of \\(i\\) are the cuts \\[\\delta^+(\\{i\\}) \\text{ and } \\delta^-(\\{i\\})\\] respectively.\n\n\nDefinition 2.12 (Degree) The out-degree and in-degree of \\(i\\) are the number of edges with \\(i\\) as tail and head, respectively: \\[|\\delta^+(\\{i\\})| \\text{ and } |\\delta^-(\\{i\\})|.\\]\n\n\nExample 2.9 Consider \\(v = 3\\), i.e., \\(S = \\{v\\} = \\{3\\}\\), in the graph \\(G\\) considered in Example 2.8:\n\nThe forward star is \\(\\delta^+(\\{3\\}) = \\{(3,2)\\}\\).\n\n\n\n\n\n\n\n\n\n\nForward star \\(\\delta^+(\\{3\\}) = \\{(3,2)\\}\\)\n\n\n\n\n\n\n\nBackward star \\(\\delta^-(\\{3\\}) = \\{(1,3), (4,3), (5,3)\\}\\)\n\n\n\n\n\n\nFigure 2.9: Forward and backward star of \\(\\{3\\}\\) in \\(G\\). Note that \\(\\{3\\}\\) has out-degree \\(1\\) and in-degree \\(3\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#undirected-graphs",
    "href": "02_Introduction_to_Graphs.html#undirected-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.6 Undirected Graphs",
    "text": "2.6 Undirected Graphs\n\nDefinition 2.13 (Undirected Graphs) An undirected graph is an ordered pair \\(G := (V, E)\\) composed of\n\na set of vertices \\(V\\) \\((n = |V|)\\)\na set \\(E \\in V\n        \\times V\\) of unordered pairs called edges \\((m = |E|)\\).\n\nFor an edge \\(\\{i,j\\}\\), we call \\(i\\) and \\(j\\) its endpoints.\n\n\nExample 2.10 Consider \\(G=(V,E)\\) with \\[\n\\begin{aligned}\n    V &= \\{1,2,3,4,5\\} \\\\\nE &= \\{12, 13, 15, 23, 24, 25, 34, 35,45\\}.\n\\end{aligned}\n\\] Figure 2.10 gives a visualisation of this graph.\n\n\n\n\n\n\n\nFigure 2.10: Visualisation of graph \\(G\\) given in Example 2.10",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#properties-of-undirected-graphs",
    "href": "02_Introduction_to_Graphs.html#properties-of-undirected-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.7 Properties of Undirected Graphs",
    "text": "2.7 Properties of Undirected Graphs\n\n2.7.1 Adjacency\nProperties ofdirected graphs generalize to undirected graphs with minor differences:\n\n\\(ij \\in E\\) is incident to both \\(i\\) and \\(j\\).\nIf \\(ij \\in E\\) then \\(i\\) and \\(j\\) are (mutually) adjacent.\nIf there is a path from \\(i\\) to \\(j\\) then \\(i\\) and \\(j\\) are (mutually) connected.\n\\(G = (V,E)\\) is complete if each pair of vertices in \\(V\\) is adjacent: \\[E = \\{\\{i,j\\}: i, j \\in V,~i\\neq j\\}.\\]\n\n\nTheorem 2.2 Every undirected graph \\(G\\) has \\(|E| \\le n(n-1)/2\\) edges.\n\n\nExample 2.11 Consider the undirected graph \\(G\\) given in Figure 2.11:\n\n\\(0\\) and \\(3\\) are adjacent because \\(\\{0,3\\} \\in E\\).\n\\(0\\) and \\(2\\) are not adjacent because \\(\\{0,2\\} \\notin E\\).\n\\(1\\) and \\(2\\) are connected. Indeed, \\(G\\) contains \\(12\\)-paths \\((1,4,3,2)\\) and \\((1,0,3,2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(0\\) and \\(3\\) are adjacent\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(0\\) and \\(2\\) are not adjacent\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(12\\)-path \\((1,4,3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(12\\)-path \\((1,0,3,2)\\)\n\n\n\n\n\n\n\nFigure 2.11: Adjacency and connectivity properties of graph \\(G\\).\n\n\n\n\n\n2.7.2 Cuts, Stars, and Degree in Undirected Graphs\n\nDefinition 2.14 (Undirected Cuts) The undirected cut induced by \\(S \\subseteq V\\) is the set of edges with one end point in \\(S\\) and one in \\(V\\setminus S\\): \\[\\delta(S) := \\{ ij \\in E: i \\in S,~j\\notin S\\}.\\]\n\n\nDefinition 2.15 (Stars) The star of node \\(i \\in V\\) is the set \\(\\delta(\\{i\\})\\).\n\n\nDefinition 2.16 (Degree) The degree of \\(i \\in V\\) is the cardinality of its star: \\(|\\delta(\\{i\\})|\\).\n\n\nExample 2.12 Consider the graph given in Figure 2.12:\n\nThe cut for \\(S = \\{1,2,5\\}\\) is \\[\n  \\delta(S) = \\{01, 16, 02, 23, 24, 53, 54, 56\\}\n\\]\nThe star for node \\(3\\) is \\[\n  \\delta(\\{3\\}) = \\{23, 53, 63\\}.\n\\]\nNode \\(3\\) has degree \\(3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Undirected graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Undirected cut \\(\\delta(S)\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Undirected star \\(\\delta(\\{3\\})\\)\n\n\n\n\n\n\n\nFigure 2.12: Undirected cut and star in an undirected graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#adjacency-lists-and-matrices",
    "href": "02_Introduction_to_Graphs.html#adjacency-lists-and-matrices",
    "title": "2  Introduction to Graphs",
    "section": "2.8 Adjacency Lists and Matrices",
    "text": "2.8 Adjacency Lists and Matrices\n\nDefinition 2.17 (Adjacency Lists) An adjacency list \\(L\\) is a list of size \\(n\\) where each component \\(L(i)\\) is a list of nodes adjacent to \\(i\\):\n\n\\(L(i) = \\{j: (i,j) \\in \\delta^+(\\{i\\})\\}\\) for directed graphs;\n\\(L(i) = \\{j: \\{i,j\\} \\in \\delta(\\{i\\})\\}\\) for undirected graphs.\n\n\n\nDefinition 2.18 (Adjacency Matrices) The adjacency matrix \\(A = A(G)\\) of \\(G = (V, E)\\) is a binary matrix \\(A \\in \\{0,1\\}^{n\\times n}\\) such that \\[\na_{ij} =\n\\begin{cases} 1, & \\text{if } (i,j) \\in E, \\\\ \\\\\n    0, & (i,j) \\notin E.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A directed graph\n\n\n\n\n\n\n\n\n\n\n\n(b) An undirected graph\n\n\n\n\n\n\n\nFigure 2.13: Graphs for examples of adjacency lists.\n\n\n\n\nExample 2.13 Consider the directed graph given in Figure 2.13 (a).\nThis graph has the adjacency lists \\[\n\\begin{aligned}\n    L(0) &= \\{1,3\\},  &&& L(1) &= \\{3\\}, \\\\\n    L(2) &= \\{0,1\\}, &&& L(3) &= \\{0\\}.\n\\end{aligned}\n\\] Moreover, the adjacency matrix of this graph is \\[\nA =\n\\begin{pmatrix}\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix}.\n\\]\n\n\nExample 2.14 Consider the directed graph given in Figure 2.13 (b).\nThis graph has adjacency lists \\[\n\\begin{aligned}\n    L(0) &= \\{1\\}, &&& L(1) &= \\{0,3\\},  &&& L(2) &= \\{4\\}\\\\\n    L(3) &= \\{1\\}, &&& L(4) &= \\{2\\}.\n\\end{aligned}\n\\] The adjacency matrix is \\[\nA =\n\\begin{pmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0\n\\end{pmatrix}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html",
    "href": "04_Complexity.html",
    "title": "4  Complexity",
    "section": "",
    "text": "4.1 Big-\\(O\\) Notation\nBig-\\(O\\) notation captures the asymptotic behaviour of a function \\(f\\) when compared to another function \\(g\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#big-o-notation",
    "href": "04_Complexity.html#big-o-notation",
    "title": "4  Complexity",
    "section": "",
    "text": "Definition 4.1 (Big-\\(O\\) Notation) Given \\(f,g:\\mathbf{R}\\mapsto\\mathbf{R}\\), we say \\[f = O(g),\\] i.e., \\(f\\) is of order \\(g\\), if there are \\(n_0\\) and \\(c\\in \\mathbf{R}_+\\) such that \\[f(n) \\le c~g(n) \\hspace{0.15in} \\text{for all } n \\ge n_0.\\]\n\n\n\n4.1.1 Properties of Big-\\(O\\) Notation\nLet \\(f\\) and \\(g\\) be positive functions \\((f,g: \\mathbf{R}\\mapsto\\mathbf{R}_+)\\).\n\nIf \\(\\displaystyle \\lim_{n\\rightarrow \\infty}\n\\frac{f(n)}{g(n)}= \\ell \\in (0, \\infty)\\) then \\[\nf = O(g) \\text{  and  } g = O(f).\n\\] For example, \\(f(n) = 2n\\) and \\(g = 3n + \\sqrt{n}\\) satisfy \\(f=O(g)\\) and \\(g=O(f)\\).\nIf \\(\\displaystyle\\lim_{n\\rightarrow \\infty} \\frac{f(n)}{g(n)} = 0\\) then \\[\nf = O(g)\n        \\text{ but }\n        g \\neq O(f).\n\\] For example, \\(f(n)=n\\) and \\(g(n) = 3n^3 + 2n\\) satisfy \\(f=O(g)\\), but \\(g \\neq O(f)\\).\nMultiplicative and additive constants can be ignored: \\[\na f(n) + b = O(f(n)).\n\\] For example, \\(f(n) = 5n^2 + 2 = O(n^2)\\).\nLower order terms can be ignored: if \\(g = O(f)\\) then \\[\nf(n) + g(n) = O(f(n)).\n\\] For example, \\(6n^4 + e^n = O(e^n)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#elementary-operations",
    "href": "04_Complexity.html#elementary-operations",
    "title": "4  Complexity",
    "section": "4.2 Elementary Operations",
    "text": "4.2 Elementary Operations\nWe can count the number of elementary operations or EOs carried out by an algorithm:\n\nArithmetic operations\nAccesses to memory\nWriting operations, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#computational-complexity",
    "href": "04_Complexity.html#computational-complexity",
    "title": "4  Complexity",
    "section": "4.3 Computational Complexity",
    "text": "4.3 Computational Complexity\nLet \\(f_A(n)\\) and \\(f_B(n)\\) be the number of EOs required in worst case (the instance that requires the most EOs) by algorithms \\(A\\) and \\(B\\) to solve \\(P\\) with instance size \\(n\\).\n\nDefinition 4.2 We call \\(O(f_A)\\) and \\(O(f_B)\\) the computational complexity of \\(A\\) and \\(B\\). We choose \\(O(f_A)\\) and \\(O(f_B)\\) to be as small as possible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#cobham-edmonds-thesis-1965",
    "href": "04_Complexity.html#cobham-edmonds-thesis-1965",
    "title": "4  Complexity",
    "section": "4.4 Cobham-Edmonds Thesis (1965)",
    "text": "4.4 Cobham-Edmonds Thesis (1965)\nWe say that problem \\(P\\) is tractable or well-solved if:\n\nThere is an algorithm \\(A\\) for solving \\(P\\);\n\\(A\\) has polynomial computational complexity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#converting-eos-to-run-time",
    "href": "04_Complexity.html#converting-eos-to-run-time",
    "title": "4  Complexity",
    "section": "4.4 Converting EOs to Run-Time",
    "text": "4.4 Converting EOs to Run-Time\nIf each EO takes \\(1\\) \\(\\mu s\\) (1 microsecond):\n\n\n\n\\(n\\)\n\\(f_A(n) = n^2\\)\n\\(f_A(n) = 2^n\\)\n\n\n\n\n\\(10\\)\n\\(0.1~\\text{ms}\\)\n\\(1~\\text{ms}\\)\n\n\n\\(20\\)\n\\(0.4~\\text{ms}\\)\n\\(1.0~\\text{s}\\)\n\n\n\\(30\\)\n\\(0.9~\\text{ms}\\)\n\\(17.9\\text{ min}\\)\n\n\n\\(40\\)\n\\(1.6~\\text{ms}\\)\n\\(12.7\\text{ days}\\)\n\n\n\\(50\\)\n\\(2.5~\\text{ms}\\)\n\\(35.7\\text{ years}\\)\n\n\n\\(60\\)\n\\(3.6~\\text{ms}\\)\n\\(366\\text{ centuries}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#a-comparison-of-complexity-and-scaling",
    "href": "04_Complexity.html#a-comparison-of-complexity-and-scaling",
    "title": "4  Complexity",
    "section": "4.5 A Comparison of Complexity and Scaling",
    "text": "4.5 A Comparison of Complexity and Scaling\nAssume again that each EO requires \\(1~\\mu\\text{s}\\).\nThe following figure compares how complexity scales as a function of \\(n\\) for\n\nlogarithmic,\npolynomial,\npolylogarithmic, and\nexponential functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#scaling-examples",
    "href": "04_Complexity.html#scaling-examples",
    "title": "4  Complexity",
    "section": "4.3 Scaling Examples",
    "text": "4.3 Scaling Examples\n\n\n\n\n\n\nFigure 4.1: Scaling of functions of \\(n\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example-quadratic-equations",
    "href": "04_Complexity.html#example-quadratic-equations",
    "title": "4  Complexity",
    "section": "4.4 Example: Quadratic Equations",
    "text": "4.4 Example: Quadratic Equations\nRecall the following algorithm for solving quadratic equation \\(ax^2 + bx + c = 0\\).\nCompute u = b*b - 4*a*c\n\nCompute v = 2*a\n\nCompute w = sqrt(u)\n\nCompute x_plus = -(w - b)/v\n\nCompute x_minus = - (w + b)/v",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#section",
    "href": "04_Complexity.html#section",
    "title": "4  Complexity",
    "section": "4.5 ",
    "text": "4.5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example-graph-reachability",
    "href": "04_Complexity.html#example-graph-reachability",
    "title": "4  Complexity",
    "section": "4.6 Example: Graph Reachability",
    "text": "4.6 Example: Graph Reachability\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example-graph-reachability-2",
    "href": "04_Complexity.html#example-graph-reachability-2",
    "title": "4  Complexity",
    "section": "4.4 Example: Graph Reachability – 2",
    "text": "4.4 Example: Graph Reachability – 2\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example-graph-reachability-3",
    "href": "04_Complexity.html#example-graph-reachability-3",
    "title": "4  Complexity",
    "section": "4.5 Example: Graph Reachability – 3",
    "text": "4.5 Example: Graph Reachability – 3\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example-graph-reachability-4",
    "href": "04_Complexity.html#example-graph-reachability-4",
    "title": "4  Complexity",
    "section": "4.6 Example: Graph Reachability – 4",
    "text": "4.6 Example: Graph Reachability – 4\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#elementary-operations-and-computational-complexity",
    "href": "04_Complexity.html#elementary-operations-and-computational-complexity",
    "title": "4  Complexity",
    "section": "4.2 Elementary Operations and Computational Complexity",
    "text": "4.2 Elementary Operations and Computational Complexity\n\n4.2.1 Computational Complexity\nWe can count the number of elementary operations or EOs carried out by an algorithm:\n\nArithmetic operations\nAccesses to memory\nWriting operations, etc.\n\nLet \\(f_A(n)\\) and \\(f_B(n)\\) be the number of EOs required in worst case (the instance that requires the most EOs) by algorithms \\(A\\) and \\(B\\) to solve \\(P\\) with instance size \\(n\\).\n\nDefinition 4.2 We call \\(O(f_A)\\) and \\(O(f_B)\\) the computational complexity of \\(A\\) and \\(B\\). We choose \\(O(f_A)\\) and \\(O(f_B)\\) to be as small as possible.\n\n\n\n4.2.2 The Cobham-Edmonds Thesis (1965)\nWe say that problem \\(P\\) is tractable or well-solved if:\n\nThere is an algorithm \\(A\\) for solving \\(P\\);\n\\(A\\) has polynomial computational complexity.\n\n\n\n4.2.3 Converting EOs and Run-Time\nIf each EO takes \\(1\\) \\(\\mu s\\) (1 microsecond):\n\n\n\n\\(n\\)\n\\(f_A(n) = n^2\\)\n\\(f_A(n) = 2^n\\)\n\n\n\n\n\\(10\\)\n\\(0.1~\\text{ms}\\)\n\\(1~\\text{ms}\\)\n\n\n\\(20\\)\n\\(0.4~\\text{ms}\\)\n\\(1.0~\\text{s}\\)\n\n\n\\(30\\)\n\\(0.9~\\text{ms}\\)\n\\(17.9\\text{ min}\\)\n\n\n\\(40\\)\n\\(1.6~\\text{ms}\\)\n\\(12.7\\text{ days}\\)\n\n\n\\(50\\)\n\\(2.5~\\text{ms}\\)\n\\(35.7\\text{ years}\\)\n\n\n\\(60\\)\n\\(3.6~\\text{ms}\\)\n\\(366\\text{ centuries}\\)\n\n\n\n\n\n4.2.4 A Comparison of Complexity and Scaling\nAssume again that each EO requires \\(1~\\mu\\text{s}\\).\nThe following figure compares how complexity scales as a function of \\(n\\) for\n\nlogarithmic,\npolynomial,\npolylogarithmic, and\nexponential functions.\n\n\n\n\n\n\n\nFigure 4.1: Scaling of functions of \\(n\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#the-cobham-edmonds-thesis-1965",
    "href": "04_Complexity.html#the-cobham-edmonds-thesis-1965",
    "title": "4  Complexity",
    "section": "4.3 The Cobham-Edmonds Thesis (1965)",
    "text": "4.3 The Cobham-Edmonds Thesis (1965)\nWe say that problem \\(P\\) is tractable or well-solved if:\n\nThere is an algorithm \\(A\\) for solving \\(P\\);\n\\(A\\) has polynomial computational complexity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#example",
    "href": "04_Complexity.html#example",
    "title": "4  Complexity",
    "section": "4.3 Example",
    "text": "4.3 Example\n\n4.3.1 Complexity of Solving Quadratic Equations\nRecall the following algorithm for solving quadratic equation \\(ax^2 + bx + c = 0\\).\nCompute u = b*b - 4*a*c\n\nCompute v = 2*a\n\nCompute w = sqrt(u)\n\nCompute x_plus = -(w - b)/v\n\nCompute x_minus = - (w + b)/v\n\n\n4.3.2 Complexity of Graph Reachability\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#examples",
    "href": "04_Complexity.html#examples",
    "title": "4  Complexity",
    "section": "4.3 Examples",
    "text": "4.3 Examples\n\n4.3.1 Complexity of Solving Quadratic Equations\nRecall the following algorithm for solving quadratic equation \\(ax^2 + bx + c = 0\\):\nCompute u = b*b - 4*a*c\n\nCompute v = 2*a\n\nCompute w = sqrt(u)\n\nCompute x_plus = -(w - b)/v\n\nCompute x_minus = - (w + b)/v\nLet’s count the number of operations used in each step of the algorithm:\n\nComputing u requires 3 products and one subtraction (4 arithmetic operations total).\nComputing v requires 1 product.\nComputing w requires 1 call to the sqrt function, which uses a finite number of arithmetic operations. Let’s treat this as 1 operation for the purposes of estimating the complexity of applying the quadratic formula.\nEach of x_plus and x_minus require 3 arithmetic operations.\n\nThe total number of arithmetic operations used by the quadratic formula is \\(12 = O(1)\\).\n\n\n4.3.2 Complexity of Graph Reachability\nLet’s analyse the complexity of the graph reachability algorithm given below:\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.\nLet’s first count the number of operations need for each iteration of the algorithm:\n\nChoosing a node i in Q, removing i from Q, and adding i to M requires \\(O(1)\\) operations each.\nThe loop iterating over the adjacency list \\(L(i)\\) runs \\(|L(i)| \\le n-1\\) times. Each occurence of the loop code uses \\(O(1)\\) operations.\nWe perform \\(|Q| \\le |V| = n\\) steps of the outer-most loop.\n\nTherefore, we have the upper bound on complexity \\[\n    O(n(n-1)) = O(|E|).\n\\]\n\n4.3.2.1 An Improved Estimate\nThe steps of the inner for loop is executed for each node \\(i\\) at most once. This implies that each arc \\((i,j)\\) is explored at most once. Therefore, the steps of this loop are executed at most \\(m\\) times total.\nThis implies that the total complexity is \\[\n    O(m) + O(n) = O(m+n).\n\\] This is much smaller than the previous estimate if the graph is sparse, i.e., \\(O(m) &lt;&lt; O(n(n-1))\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 2014 Algorithms",
    "section": "",
    "text": "Preface\nThis document collects lecture notes for Math 2014 Algorithms.\nPlease follow the page navigation for each topic covered in the module.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html",
    "href": "05a_Shortest_Paths.html",
    "title": "5  The Shortest Path Problem",
    "section": "",
    "text": "5.1 Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#preliminaries",
    "href": "05a_Shortest_Paths.html#preliminaries",
    "title": "5  The Shortest Path Problem",
    "section": "",
    "text": "Definition 5.1 (The Shortest Path Problem – Unrestricted Lengths (SPP-U)) Given a directed graph \\(G = (V,A)\\) with:\n\ntwo nodes \\(s\\) and \\(t\\);\nlength function \\(\\ell:A\n    \\mapsto \\mathbf{R}\\) (unrestricted in sign).\n\nFind an \\((s,t)\\)-path with minimum total length.\n\n\nExample 5.1 Consider the graph given in Figure 5.1 (a). The shortest \\((1,5)\\)-path in this graph is \\((1,2,3,5)\\) with value equal to \\[\n    \\ell_{12} + \\ell_{23} + \\ell_{35} = 4 -6 -2 = -4.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A directed graph \\(G\\) with edge lengths \\(\\ell\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) The shortest \\((1,5)\\)-path\n\n\n\n\n\n\n\nFigure 5.1: The shortest \\((1,5)\\)-path in \\(G\\) is \\((1,2,3,5)\\) with length \\(-4\\).\n\n\n\n\n5.1.1 Applications\n\n5.1.1.1 Logistics and Transportation\nIn this field, we want to find \\((s,t)\\)-paths minimizing:\n\nTravel time; \\(\\ell_{ij}\\) is average time traveling along arc \\((i,j)\\).\nFuel consumption.\nLikelihood of delays; etc.\n\n\n\n5.1.1.2 Minimum Cardinality Path\nWe can find a path with minimum number of arcs by assigning \\(\\ell_{ij} = 1\\) for all \\((i,j)\\in A\\).\n\n\n5.1.1.3 Component of Other Algorithm\nSPP appears as a subproblem when solving other combinatorial optimization problems (more details later).\n\n\n\n5.1.2 Simple Paths and Cycles\n\nDefinition 5.2 An \\((s,t)\\)-path \\(P\\) is simple if it does not visit the same node twice.\n\nNote: if \\(P\\) visits the same node twice then \\(P\\) must contain at least one cycle.\n\nExample 5.2 Figure 5.2 gives an example of a simple path and a not simple path within a directed graph.\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A simple path\n\n\n\n\n\n\n\n\n\n\n\n(c) A not-simple path\n\n\n\n\n\n\n\nFigure 5.2: Simple and not simple \\((4,5)\\)-paths in directed graph \\(G\\).\n\n\n\n\n\n\n5.1.3 Unboundedness and Negative Cycles\n\nTheorem 5.1 Suppose that \\(G\\) contains an \\((s,t)\\)-path \\(P\\) with a negative length cycle. Then SPP-U is unbounded.\n\n\nProof. Suppose that \\(C\\) is a negative length with length \\[\n    \\ell_{C} = \\sum_{ij \\in C} \\ell_{ij} &lt; 0.\n\\]\nSuppose further that \\(P\\) is an \\((s,t)\\)-path that traverses \\(C\\) exactly \\(k\\) times for some integer \\(k&gt; 0\\). That is, \\(C\\) is contained in \\(P\\) exactly \\(k\\) times.\nWe can always augment this path to get a path with strictly smaller length. Indeed, consider the path \\(\\tilde P\\) which contains all arcs of \\(P\\), but traverses \\(C\\) exactly \\(k+1\\) times. Then the length of \\(\\tilde P\\) satisfies \\[\n    \\text{length }\\tilde P = \\text{length } P + \\ell_C\n    &lt; \\text{length } P\n\\] since \\(\\ell_C &lt; 0\\).\n\n\nTheorem 5.2 If no \\((s,t)\\)-path contains a negative-length cycle, then \\(G\\) admits a simple shortest \\((s,t)\\)-path.\n\nIf no \\((s,t)\\)-path in \\(G\\) contains a negative length cycle, then we can solve the SPP-U by restricting our search to simple paths.\n\nProof. Let \\(P\\) be a shortest \\((s,t)\\)-path in \\(G\\). Let’s also assume that every cycle \\(C\\) in \\(P\\) has nonnegative length \\(\\ell_C \\ge 0\\).\nLet’s assume that \\(P\\) contains a cycle \\(C\\) starting and ending with node \\(u\\). That is, we can think of \\(P\\) as the union of the directed \\((s,u)\\)-path \\(P_{su}\\), the cycle \\(C\\), and the \\((u,t)\\)-path \\(P_{ut}\\). Let \\(\\ell^*\\) denote the value of this path.\nNow consider the \\((s,t)\\)-path given by following \\(P_{su}\\) to \\(u\\), then \\(P_{ut}\\) to \\(t\\). This is the path obtained by removing \\(C\\) from \\(P\\). This gives an \\((s,t)\\)-path with length \\[\n    \\ell^* - \\ell_C \\le \\ell^*.\n\\] Thus, removing cycle \\(C\\) from \\(P\\) does not increase the length of the path. We can repeat this process until we have obtained a simple path with minimum path length \\(\\ell^*\\), or we obtain a simple path with length strictly less than \\(\\ell^*\\) (a contradiction).\n\n\nExample 5.3 To illustrate this phenomena, consider the graph given in Figure 5.3. This graph has \\((1,5)\\)-path \\((1,2,3,4, 2,5)\\) with length \\(5\\) (assuming all arcs have length \\(1\\)). However, this path contains the cycle \\(C= (2,3,4,2)\\). Removing the cycle \\(C\\) gives the shorter \\((1,5)\\)-path \\((1,2,5)\\) (with length \\(2\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Path of length \\(5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Cycle \\(C\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) Path with cycle removed\n\n\n\n\n\n\n\nFigure 5.3: Illustration of the cycle removal process to obtain a shorter path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#example",
    "href": "05a_Shortest_Paths.html#example",
    "title": "3  The Shortest Path Problem",
    "section": "3.2 Example",
    "text": "3.2 Example\nConsider the graph below. What is the shortest \\((1,5)\\)-path?\n\n\n\n\n\n\nFigure 3.1: A directed graph \\(G\\) with edge lengths \\(\\ell\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#applications",
    "href": "05a_Shortest_Paths.html#applications",
    "title": "3  The Shortest Path Problem",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nLogistics/Transportation: Find \\((s,t)\\)-path minimizing:\n\nTravel time; \\(\\ell_{ij}\\) is average time traveling along arc \\((i,j)\\).\nFuel consumption.\nLikelihood of delays; etc.\n\n\nFind a path with minimum number of arcs by assigning \\(\\ell_{ij} = 1\\) for all \\((i,j)\\in A\\).\n\n\nSPP appears as a subproblem when solving other combinatorial optimization problems (more details later).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#simple-paths-and-cycles",
    "href": "05a_Shortest_Paths.html#simple-paths-and-cycles",
    "title": "3  The Shortest Path Problem",
    "section": "3.3 Simple Paths and Cycles",
    "text": "3.3 Simple Paths and Cycles\n\nDefinition 3.2 An \\(st\\)-path \\(P\\) is simple if it does not visit the same node twice.\n\nNote: if \\(P\\) visits the same node twice then \\(P\\) must contain at least one cycle.\n\n\n\n\n\n\n\n\n\n\n\n(a) A simple path\n\n\n\n\n\n\n\n\n\n\n\n(b) A not-simple path\n\n\n\n\n\n\n\nFigure 3.2: Simple and not simple paths in a directed graph \\(G\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#unboundedness-of-spp",
    "href": "05a_Shortest_Paths.html#unboundedness-of-spp",
    "title": "3  The Shortest Path Problem",
    "section": "3.2 Unboundedness of SPP",
    "text": "3.2 Unboundedness of SPP\n\nTheorem 3.1 Suppose that \\(G\\) contains an \\((s,t)\\)-path \\(P\\) with a negative length cycle. Then SPP-U is unbounded.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section",
    "href": "05a_Shortest_Paths.html#section",
    "title": "3  The Shortest Path Problem",
    "section": "3.2 ",
    "text": "3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-1",
    "href": "05a_Shortest_Paths.html#section-1",
    "title": "3  The Shortest Path Problem",
    "section": "3.3 ",
    "text": "3.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-2",
    "href": "05a_Shortest_Paths.html#section-2",
    "title": "3  The Shortest Path Problem",
    "section": "3.5 ",
    "text": "3.5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#solvability-of-spp-u",
    "href": "05a_Shortest_Paths.html#solvability-of-spp-u",
    "title": "3  The Shortest Path Problem",
    "section": "3.2 Solvability of SPP-U",
    "text": "3.2 Solvability of SPP-U\n\nTheorem 3.2 If no \\((s,t)\\)-path contains a negative-length cycle, then \\(G\\) admits a simple shortest \\((s,t)\\)-path.\n\n\n\nIf no \\((s,t)\\)-path in \\(G\\) contains a negative length cycle, then we can solve the SPP-U by restricting our search to simple paths.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-3",
    "href": "05a_Shortest_Paths.html#section-3",
    "title": "3  The Shortest Path Problem",
    "section": "3.6 ",
    "text": "3.6",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-4",
    "href": "05a_Shortest_Paths.html#section-4",
    "title": "3  The Shortest Path Problem",
    "section": "3.8 ",
    "text": "3.8",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#removing-cycles",
    "href": "05a_Shortest_Paths.html#removing-cycles",
    "title": "3  The Shortest Path Problem",
    "section": "3.4 Removing Cycles",
    "text": "3.4 Removing Cycles\n\nExample 3.2 This graph has \\((1,5)\\)-path \\((1,2,3,4, 2,5)\\) with length \\(5\\) (assuming all arcs have length \\(1\\)).\n\n\n\n\n\n\n\nFigure 3.3: Graph with \\((1,5)\\)-path of length \\(5\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-5",
    "href": "05a_Shortest_Paths.html#section-5",
    "title": "3  The Shortest Path Problem",
    "section": "3.10 ",
    "text": "3.10",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#section-6",
    "href": "05a_Shortest_Paths.html#section-6",
    "title": "3  The Shortest Path Problem",
    "section": "3.11 ",
    "text": "3.11",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html",
    "href": "05b_Bellman_Ford_Algorithm.html",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "",
    "text": "6.0.1 Single Source Shortest Path Problem – Unrestricted Lengths (SSPP-U)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section",
    "href": "05b_Bellman_Ford_Algorithm.html#section",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.1 ",
    "text": "6.1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#example-k2",
    "href": "05b_Bellman_Ford_Algorithm.html#example-k2",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.2 Example (\\(k=2\\))",
    "text": "6.2 Example (\\(k=2\\))\n\n\n\n\n\n\nFigure 6.8: Shortest Paths of length \\(k=2\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section-1",
    "href": "05b_Bellman_Ford_Algorithm.html#section-1",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.3 ",
    "text": "6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#example-k3",
    "href": "05b_Bellman_Ford_Algorithm.html#example-k3",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.2 Example (\\(k=3\\))",
    "text": "6.2 Example (\\(k=3\\))\n\n\n\n\n\n\nFigure 6.9: Shortest Paths of length \\(k=3\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section-2",
    "href": "05b_Bellman_Ford_Algorithm.html#section-2",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.5 ",
    "text": "6.5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#example-k4",
    "href": "05b_Bellman_Ford_Algorithm.html#example-k4",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.4 Example (\\(k=4\\))",
    "text": "6.4 Example (\\(k=4\\))\nLet’s try one more iteration!\n\n\n\n\n\n\nFigure 6.10: Shortest Paths of length \\(k=4\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section-3",
    "href": "05b_Bellman_Ford_Algorithm.html#section-3",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.6 ",
    "text": "6.6",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section-4",
    "href": "05b_Bellman_Ford_Algorithm.html#section-4",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.7 ",
    "text": "6.7",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#section-5",
    "href": "05b_Bellman_Ford_Algorithm.html#section-5",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.9 ",
    "text": "6.9",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "05b_Bellman_Ford_Algorithm.html#iteration-4-k4-1",
    "href": "05b_Bellman_Ford_Algorithm.html#iteration-4-k4-1",
    "title": "6  The Bellman-Ford Algorithm",
    "section": "6.1 Iteration 4 (\\(k=4\\))",
    "text": "6.1 Iteration 4 (\\(k=4\\))\nLet’s try one more iteration!\nNote that \\[\n    f_3(4) + \\ell_{42} = 3 - 3 = 0 &lt; f_2(2).\n\\]\n\n\n\n\n\n\nFigure 6.10: Shortest Paths of length \\(k=4\\)\n\n\n\n\n\n\nTable 6.15: Length of shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\\(f_4(i)\\)\n0\n0\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 6.16: Predecessors in shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\\(P_4(i)\\)\n\\(\\sim\\)\n4\n2\n3\n\n\n\n\n\n\nThis implies that the path \\((1,2,3,4,2)\\) with \\(5\\) arcs has smaller total length than the path \\((1,2)\\). This contradicts ?lem-5-shortest-path and, hence, implies that the shortest path problem is unbounded for this graph. Indeed, further iterations will lead further decrease in \\(f_k(i)\\) for \\(i\\) in the negative length cycle \\((2,3,4,2)\\). We can stop the Bellman-Ford Algorithm after \\(k=n\\) iterations and declare the problem instance unbounded.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Bellman-Ford Algorithm</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html",
    "href": "06_Minimum_Cost_Spanning_Trees.html",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "",
    "text": "6.1 Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#subgraphs",
    "href": "06_Minimum_Cost_Spanning_Trees.html#subgraphs",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "",
    "text": "Definition 6.1 Let \\(G = (V,E)\\) be an undirected graph.\nWe call \\(G_S = (V_S, E_S)\\) a subgraph of \\(G\\) if\n\n\\(G_S\\) is a graph;\n\\(V_S \\subseteq V\\) and \\(E_S \\subseteq E\\).\n\n\n\nExample 6.1 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The graph given in Figure 6.2 (b) is a subgraph of \\(G\\). However, the graph given in Figure 6.2 (c) is not a subgraph of \\(G\\) since \\(\\{1,4\\} \\notin E\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A subgraph\n\n\n\n\n\n\n\n\n\n\n\n(c) Not a subgraph\n\n\n\n\n\n\n\nFigure 6.1: A subgraph of \\(G\\) and a graph that is not a subgraph of \\(G\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#trees",
    "href": "06_Minimum_Cost_Spanning_Trees.html#trees",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.2 Trees",
    "text": "6.2 Trees\n\nDefinition 6.2 Let \\(G = (V,E)\\) be an undirected graph.\nThe subgraph \\(G_T = (V_T, E_T)\\) is a tree if\n\n\\(G_T\\) is a connected;\n\\(G_T\\) is acyclic, i.e., contains no cycles.\n\nA tree \\(G_T\\) is a spanning tree if \\(V_T = V\\) (\\(G_T\\) spans/reaches all nodes in \\(V\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A Tree\n\n\n\n\n\n\n\n\n\n\n\n(c) A Spanning Tree\n\n\n\n\n\n\n\nFigure 6.2: Trees in given graph \\(G\\).\n\n\n\n\nExample 6.2 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The subgraph given in Figure 6.2 (b) is a connected and acyclic; therefore, it is a tree. On the otherhand, the subgraph given in Figure 6.2 (c) is a tree and has node set \\(V\\); therefore, it is a spanning tree.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#motivating-example",
    "href": "06_Minimum_Cost_Spanning_Trees.html#motivating-example",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 Motivating Example",
    "text": "6.3 Motivating Example\nWe want to build a new high-speed network at the University:\n\nshould connect all buildings; while\ncosting as little as possible.\n\nWe can model this problem as that of finding a minimum cost spanning tree.\nConsider the graph \\(G\\) with:\n\nBuildings as vertices;\nPotential connections as edges.\n\nWe want to find a connected, acyclic subgraph to minimize cost while ensuring all buildings are connected. Such a tree is highlighted in red in Figure 6.3.\n\n\n\n\n\n\nFigure 6.3: Campus map with minimum spanning tree",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#model-as-a-graph",
    "href": "06_Minimum_Cost_Spanning_Trees.html#model-as-a-graph",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 Model as a Graph",
    "text": "6.4 Model as a Graph\n\nBuildings are vertices;\nPotential connections are edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Campus map with minimum spanning trees\n\n\n\n\n\n\nWe want to find a to minimize cost while ensuring all buildings are connected.\nThe optimal spanning tree is highlighted on the figure.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#minimum-cost-spanning-tree-mst-problem",
    "href": "06_Minimum_Cost_Spanning_Trees.html#minimum-cost-spanning-tree-mst-problem",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 Minimum Cost Spanning Tree (MST) Problem",
    "text": "6.5 Minimum Cost Spanning Tree (MST) Problem\n\nDefinition 6.3 Given an undirected graph \\(G = (V,E)\\) and cost function \\(c: E\\mapsto \\mathbf{R}\\).\nFind a spanning tree \\(G_T = (V_T, E_T)\\) of minimum total cost: \\[\\min~ c(E_T) = \\sum_{e \\in E_T} c_e.\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#cayleys-theorem",
    "href": "06_Minimum_Cost_Spanning_Trees.html#cayleys-theorem",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 Cayley’s Theorem",
    "text": "6.5 Cayley’s Theorem\n\nTheorem 6.1 (Cayley – 1889) A complete undirected graph with \\(n\\) nodes contains \\(n^{n-2}\\) spanning trees.\n\n\nNoncomplete graphs contain fewer spanning trees, but the number is still exponentially large in \\(n\\).\n\n\nWe cannot solve MST using complete enumeration for even small graphs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#leaves",
    "href": "06_Minimum_Cost_Spanning_Trees.html#leaves",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 Leaves",
    "text": "6.5 Leaves\n\nDefinition 6.4 The nodes of a graph with degree 1 are called leaves.\n\n\nTheorem 6.2 A tree contains at least one leaf.\n\n\nProof. Suppose, on the contrary, that tree \\(T\\) does not contain a leaf. Then every node in \\(T\\) has degree at least \\(2\\). This implies that \\(T\\) has a cycle; a contradiction.\n\n\n\n\n\n\n\nFigure 6.4: A tree with leaves ${1, 2, 3, 6}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 ",
    "text": "6.5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-1",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-1",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.1 ",
    "text": "7.1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-number-of-edges-of-a-tree",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-number-of-edges-of-a-tree",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.6 The Number of Edges of A Tree",
    "text": "6.6 The Number of Edges of A Tree\n\nTheorem 6.3 A tree \\(G_T\\) with \\(n\\) nodes has \\(m = n-1\\) edges.\n\n\nProof. We’ll prove Theorem 6.3 by induction.\nAs a base case, consider \\(n=1\\). This tree consists of a single node and \\(0 = n-1\\) edges.\nNow suppose that every tree with \\(k\\) nodes has \\(n-1\\) edges. Consider tree \\(T\\) with \\(k+1\\) nodes. We want to show that \\(T\\) has \\(k\\) edges.\nTo do so, note that \\(T\\) contains at least one leaf by Theorem 6.2. Without loss of generality, let’s assume that node \\(1\\) is a leaf; if not, we can relabel vertices so that \\(1\\) is a leaf. Let’s remove leaf \\(1\\) and its incident edge (there is only one such edge because \\(1\\) has degree-\\(1\\)).\nAfter deleting this node and edge, we have a connected, acyclic graph with \\(k\\) nodes. By the inductive hypothesis, this tree has \\(k-1\\) edges. Since we deleted \\(1\\) node and \\(1\\) edge, we can conclude that \\(T\\) had \\(k\\) edges. This completes the proof.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\) with leaf \\(1\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) After removing \\(\\{1,5\\}\\)\n\n\n\n\n\n\n\nFigure 6.5: Illustration of the pruning process. After removing node \\(1\\) and edge \\(\\{1,5\\}\\), we are left with a tree with \\(5\\) nodes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-2",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-2",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.8 ",
    "text": "6.8",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-3",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-3",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.12 ",
    "text": "6.12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-4",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-4",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.15 ",
    "text": "6.15",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-5",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-5",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.13 ",
    "text": "6.13",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-swap-property",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-swap-property",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.7 The Swap Property",
    "text": "6.7 The Swap Property\n\nLemma 6.1 Given tree \\(G_T = (V_T, E_T)\\), removing an edge \\(e \\in E_T\\) creates two subtrees.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A tree\n\n\n\n\n\n\n\n\n\n\n\n(b) A subtree\n\n\n\n\n\n\n\n\n\n\n\n(c) Another subtree\n\n\n\n\n\n\n\nFigure 6.6: Subtrees obtained after deleting edge \\(\\{4,5\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#swapping-edges-within-a-cut",
    "href": "06_Minimum_Cost_Spanning_Trees.html#swapping-edges-within-a-cut",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.8 Swapping Edges Within a Cut",
    "text": "6.8 Swapping Edges Within a Cut\n\nLemma 6.2 Let \\(S\\) be the vertex set of one of the two subtrees.\nFor every edge \\(f \\in \\delta(S)\\) other than \\(e\\), the set \\[E_T' := E_T \\cup \\{f\\} \\setminus \\{e\\}\\] is the edge set of a tree spanning the same set of nodes.\n\n\nExample 6.3 Consider the graph \\(G\\) with tree \\(T\\) given by Figure 6.7 (a). The example in Figure 6.6 illustrates that we obtain two subtrees after removing edge \\(e=45\\).\nThe set \\(S = \\{1,2,3, 5\\}\\) is the vertex set of one of these trees. The cut induced by \\(S\\) is \\[\n    \\delta(S) = \\{02, 16, 24\\}.\n\\] Lemma 6.2 implies that exchanging \\(02\\) with \\(45\\) yields a new tree \\(\\tilde{T}\\) with edge set \\[\nE_{\\tilde T} = E_T \\setminus \\{4,5\\} \\cup \\{0,2\\}\n    = \\{15, 25, 35, 46, 02\\}.\n\\] See Figure 6.7 for an illustration of this process.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Tree \\(\\tilde T\\) after exchange\n\n\n\n\n\n\n\nFigure 6.7: Example of swapping edges \\(02\\) and \\(45\\) using cuts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#example",
    "href": "06_Minimum_Cost_Spanning_Trees.html#example",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.9 Example",
    "text": "6.9 Example\n\n\n\n\n\n\n\n\n\n\n\n(a) Subtrees after removing \\(\\{4,5\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Subtree after Swap\n\n\n\n\n\n\n\nFigure 6.7: Example of swapping edges using cuts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-6",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-6",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.19 ",
    "text": "6.19",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-7",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-7",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.21 ",
    "text": "6.21",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-8",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-8",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.23 ",
    "text": "6.23",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-9",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-9",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.25 ",
    "text": "6.25",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-minimum-cost-spanning-tree-mst-problem",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-minimum-cost-spanning-tree-mst-problem",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 The Minimum Cost Spanning Tree (MST) Problem",
    "text": "6.4 The Minimum Cost Spanning Tree (MST) Problem\n\nDefinition 6.3 Given an undirected graph \\(G = (V,E)\\) and cost function \\(c: E\\mapsto \\mathbf{R}\\).\nThe minimum cost spanning tree problem aims to find a spanning tree \\(G_T = (V_T, E_T)\\) of minimum total cost: \\[\\min~ c(E_T) = \\sum_{e \\in E_T} c_e.\\]\n\n\nTheorem 6.1 (Cayley – 1889) A complete undirected graph with \\(n\\) nodes contains \\(n^{n-2}\\) spanning trees.\n\nNoncomplete graphs contain fewer spanning trees, but the number is still exponentially large in \\(n\\). This implies that we cannot solve MST using complete enumeration for even small graphs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#the-bellman-ford-algorithm",
    "href": "05a_Shortest_Paths.html#the-bellman-ford-algorithm",
    "title": "5  The Shortest Path Problem",
    "section": "5.2 The Bellman-Ford Algorithm",
    "text": "5.2 The Bellman-Ford Algorithm\n\n5.2.1 Single Source Shortest Path Problem – Unrestricted Lengths (SSPP-U)\n\nDefinition 5.3 Given a directed graph \\(G = (V,A)\\) with length function \\(\\ell:A     \\mapsto \\mathbf{R}\\) (unrestricted in sign).\nThe Single Source Shortest Path Problem (SSPP-U) aims to find an \\((s,t)\\)-path with minimum total length for every node \\(t\\) in \\(V\\setminus\\{s\\}\\).\n\n\nExample 5.4 Figure 5.4 gives the shortest \\((1,t)\\)-paths in the graph given in Figure 5.4 (a) for \\(t = 2,3,4,5,6\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Shortest \\((1,2)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(c) Shortest \\((1,3)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Shortest \\((1,4)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(e) Shortest \\((1,5)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(f) Shortest \\((1,6)\\)-path\n\n\n\n\n\n\n\nFigure 5.4: Shortest \\((1,t)\\)-paths in graph \\(G\\).\n\n\n\n\n\n5.2.2 Subpath Optimality\nThe following theorem characterizes a very useful properties of shortest paths: any subpath of a shortest path is itself a shortest path.\n\nTheorem 5.3 Let \\(P = (s,i_2), (i_2, i_3), \\dots, (i_{k-1}, t)\\) be a shortest \\((s,t)\\)-path.\nConsider pair of nodes \\(i_u, i_v\\) visited by \\(P\\) with \\(u &lt; v\\).\nThen the subpath from \\(i_u\\) to \\(i_v\\) is a shortest \\((i_u,i_v)\\) path.\n\n\nProof. Suppose, on the contrary, that the subpath \\(S\\) from \\(i_u\\) to \\(i_v\\) in a shortest \\((s,t)\\)-path is not the shortest \\((i_u, i_v)\\)-path. In particular, suppose that \\(T\\) is the shortest \\((i_u, i_v)\\)-path.\nWe can construct another \\((s,t)\\)-path \\(tilde P\\) using \\(P\\) and \\(T\\). Indeed, let \\(\\tilde P = P \\setminus S \\cup T\\) be the path obtained by removing \\(S\\) from \\(P\\) and adding \\(T\\). Then \\(\\tilde P\\) is also a \\((s,t)\\)-path. The length of \\(\\tilde P\\) is equal to \\[\n    |P| - |S| + |T| &lt; |P| - |S| + |S| = |P|\n\\] since \\(|T| &lt; |S|\\). This implies that \\(\\tilde P\\) is shorter than \\(P\\); this is a contradiction. Therefore, \\(S\\) must be the shortest \\((i_u, i_v)\\)-path.\n\n\n\n5.2.3 Shortest Paths of Fixed Length\n\nDefinition 5.4 For all \\(i\\in V\\), we define \\(f_k(i)\\) as the length of a shortest \\((s,i)\\)-path containing at most k arcs,\n\nWe set \\(f_k(i) = \\infty\\) if there is no \\((s,i)\\)-path with length at most \\(k\\).\n\nLemma 5.1 If \\(k = n-1\\), then \\(f_{n-1}(i)\\) is the length of a shortest \\((s,i)\\)-path (without restriction on the number of arcs).\n\n\nProof. We can ignore paths that aren’t simple. On the other hand, simple paths contain at most \\(n\\) nodes. Indeed, a simple path does not contain a loop and, hence, visits at most \\(n\\) nodes. Therefore, the shortest \\((s,i)\\)-path contains at most \\(n-1\\) arcs.\n\n\n\n5.2.4 The Bellman-Ford Theorem\nThe following theorem is the basis for our algorithm for calculating shortest paths.\n\nTheorem 5.4 \\(f_k(i)\\) can be computed recursively as \\[\nf_k(i) = \\min \\left\\{\n        f_{k-1}(i),\n        \\;\n        \\min_{(j,i) \\in \\delta^-(i)}\n        \\Big\\{ f_{k-1}(j) + \\ell_{ji} \\Big\\}\n        \\right\\}.\n\\]\n\n\nProof. Assume that \\(f_{k-1}(i)\\) has been computed for all \\(i \\in V\\setminus\\{s\\}\\). The shortest \\((s,i)\\)-path with at most \\(k\\) arcs \n\nis the shortest \\((s,i)\\)-path with at most \\(k-1\\) arcs; or\ncontains one more arc than the shortest \\((s,i)\\)-path with at most \\(k-1\\) arcs.\n\nIn the second case, we can decompose the shortest \\((s,i)\\)-path as:\n\nshortest \\((s,j)\\)-path with \\(k-1\\) arcs for some node \\(j\\); and\narc \\((j,i)\\).\n\nWe choose node \\(j\\) so that \\(j\\) gives \\[\n    \\min_{(q,i) \\in \\delta^-(i)} f_{k-1}(q) + \\ell_{qi}.\n\\]\n\n\n\n5.2.5 The Bellman-Ford Algorithm\n\n5.2.5.1 Setup\n\nDefinition 5.5 Let \\(P_k(i)\\) be the predecessor of \\(i\\) in the shortest \\((s,i)\\)-path with at most \\(k\\) arcs found by the algorithm (so far).\n\nWe will maintain two tables:\n\nOne encoding \\(f_k(i)\\) for each \\(i\\) and \\(k\\);\nThe other encoding \\(P_k(i)\\) for all \\(i\\) and \\(k\\).\n\n\n\n5.2.5.2 Updates\nThe algorithm calculates \\(f_k(i)\\) and \\(P_k(i)\\) for all \\(i \\in V\\) recursively from \\(k=1\\) to \\(k=n-1\\).\nWe’ll update \\(f_k(i)\\) from \\(f_{k-1}(i)\\) by:\n\nInitially setting \\(f_k(i) = f_{k-1}(i)\\);\nScanning all arcs \\((j,i)\\) in \\(\\delta^{-}(i)\\) and update \\(f_k(i)\\) if necessary.\n\n\n\n5.2.5.3 The Bellman-Ford Algorithm (Pseudocode)\nf_0(s) = 0 and P_0(s) = ~ \nfor i in V \\{s}:\n    f_0(i) = +inf and  P_0(i) = ~\n\n# Calculate shortest paths of length k.\nfor k in {1, 2, ..., n-1}:\n   \n    # Update shortest si-path.\n    for i in V:\n        f_k(i) = f_{k-1}(i) \n        P_k(i) = P_{k-1}(i)\n        \n        # Check each edge incident at i.\n        for (j,i) in delta^{-}(i):            \n            # Update if length-k path is shorter than k-1.\n            if f_{k-1}(j) + l_{ji} &lt; f_k(i)\n                f_k(i) = f_{k-1}(j) + l_{ji} \n                P_k(i) = j\n\n\n\n5.2.6 Example\n\n5.2.6.1 Iteration 1 (\\(k=1\\))\nNote that nodes \\(2\\) and \\(6\\) are adjacent to \\(1\\). Thus, they are reachable from \\(1\\) by a path with length at most \\(1\\). The only paths from \\(1\\) to another node are the single arcs \\((1,2)\\) and \\((1,6)\\), of lengths \\(4\\) and \\(3\\) respectively. Figure 5.5 highlights these paths, and we update the tables Table 5.1 and Table 5.2 accordingly.\n\n\n\n\n\n\nFigure 5.5: Shortest Paths of length \\(k=1\\)\n\n\n\n\n\n\nTable 5.1: Lengths of shortest paths of length at most \\(1\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\n\n\n\n\n\n\nTable 5.2: Predecessors in shortest paths of length at most \\(1\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\n\n\n\n\n\n5.2.6.2 Iteration 2 (\\(k=2\\))\nNodes \\(3\\) and \\(4\\) can both be reached from \\(1\\) using a path of length \\(2\\) (with lengths \\(-2\\) and \\(1\\) respectively). Similarly, we have path \\((1,2,6)\\) of length \\(2\\): \\[\n    \\ell_{12} + \\ell_{26} = f_1(2) + \\ell_{26} = 4 - 2 = 2 &lt; f_1(6) = 3.\n\\]\nThis implies that the path \\((1,2,6)\\) is the shortest \\((1,6)\\)-path with at most \\(2\\) arcs. We update the shortest path and predecessor tables below.\n\n\n\n\n\n\nFigure 5.6: Shortest Paths of length at most \\(k=2\\)\n\n\n\n\n\n\nTable 5.3: Lengths of shortest paths of length at most \\(2\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\n\n\n\n\n\n\nTable 5.4: Predecessors in shortest paths of length at most \\(2\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\n\n\n\n\n\n5.2.6.3 Iteration 3 (\\(k=3\\))\nNode \\(5\\) is reachable from \\(1\\) by the path \\((1,2,3,5)\\) using \\(3\\) arcs; this path has length \\(-4\\). On the other hand, every other \\((s,i)\\)-path consisting of \\(3\\) arcs is longer than the shortest \\((s,i)\\)-path of length at most \\(2\\).\n\n\n\n\n\n\nFigure 5.7: Shortest Paths of length at most \\(k=3\\)\n\n\n\n\n\n\nTable 5.5: Lengths of shortest paths of length at most \\(3\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\\(f_3\\)\n0\n4\n-2\n1\n-4\n2\n\n\n\n\n\n\n\n\n\nTable 5.6: Predecessors in shortest paths of length at most \\(3\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\\(P_3\\)\n\\(\\sim\\)\n1\n2\n2\n3\n2\n\n\n\n\n\n\n\n\n5.2.6.4 Iteration 4 (\\(k=4\\))\nNote that we have the \\((1,4)\\)-path \\((1,2,3,5,4)\\) containing \\(k=4\\) arcs. This path has length \\[\n    f_3(5) + \\ell_{54} = -4 + 4 = 0 &lt; f_3(4) = 1 = f_2(4).\n\\] We set \\(f_4(4) = 0\\) and \\(P_4(4) = 5\\).\nAfter checking all other paths containing \\(k=4\\) arcs, we do not change the list of shortest paths.\n\n\n\n\n\n\nFigure 5.8: Shortest Paths of length at most \\(k=4\\)\n\n\n\n\n\n\nTable 5.7: Lengths of shortest paths of length at most \\(4\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\\(f_3\\)\n0\n4\n-2\n1\n-4\n2\n\n\n\\(f_4\\)\n0\n4\n-2\n0\n-4\n2\n\n\n\n\n\n\n\n\n\nTable 5.8: Predecessors in shortest paths of length at most \\(4\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n0\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\\(P_3\\)\n\\(\\sim\\)\n1\n2\n2\n3\n2\n\n\n\\(P_4\\)\n\\(\\sim\\)\n1\n2\n5\n3\n2\n\n\n\n\n\n\n\n\n5.2.6.5 Termination\nThe shortest path using at most \\(4\\) arcs are also shortest paths using at most \\(5\\) arcs for this graph. Since \\(k=5 = n-1\\), we have found the shortest paths of any length by Lemma 5.1.\n\n\n\n5.2.7 Complexity of the Bellman-Ford Algorithm\nLet’s analyses how many operations are needed by the Bellman-Ford Algorithm.\nf_0(s) = 0 and P_0(s) = ~ \nfor i in V \\{s}:\n    f_0(i) = +inf and  P_0(i) = ~\n\n# Calculate shortest paths of length k.\nfor k in {1, 2, ..., n-1}:\n   \n    # Update shortest si-path.\n    for i in V:\n        f_k(i) = f_{k-1}(i) \n        P_k(i) = P_{k-1}(i)\n        \n        # Check each edge incident at i.\n        for (j,i) \\in delta^{-}(i):            \n            # Update if length-k path is shorter than k-1.\n            if f_{k-1}(j) + l_{ji} &lt; f_k(i)\n                f_k(i) = f_{k-1}(j) + l_{ji} \n                P_k(i) = j\nInitiailization (Lines 1-3) requires \\(O(n)\\) elementary operations.\nThe loop from Line 9 to Line 18 is repeated for each \\(i \\in V\\) (\\(O(n)\\) times) and consists of the following steps for each \\(i\\):\n\nInitializing \\(f_k(i) = f_{k-1}(i)\\) and \\(P_k(i) = P_{k-1}(i)\\) (requires \\(O(1)\\) operations).\nChecking if each arc \\((j,i)\\) with tail \\(i\\) yields a shorter path with \\(k\\) arcs:\n\nComparing \\(f_{k-1}(j) + \\ell_{ji}\\) with \\(f_k(i)\\) requires \\(O(1)\\) operations for each \\(j\\).\nWe have \\(|\\delta^-(i)| = O(n)\\) of these arcs.\n\n\nIt follows that the loop from Lines 9–18 requires \\[\n    O(n) \\Big( O(1) + O(n) \\Big) = O(n^2)\n\\] for each \\(k \\in \\{1,2,\\dots, n-1\\}\\). Taking the total over all \\(k\\), the Bellman-Ford Algorithm requires \\(O(n^3)\\) elementary operations.\n\n5.2.7.1 Complexity in Terms of Arcs\nEach arc \\((j,i) \\in A\\) is considered exactly once for each \\(k \\in \\{1,2,\\dots, n-1\\}\\) in the loop from Lines 9–18. This implies that the steps of this loop require \\(O(|A| + n) = O(m + n)\\) elementary operations. This implies that the total complexity of the Bellman-Ford Algorithm require \\[\n    O(nm + n^2)\n\\] elementary operations, which is much smaller than \\(O(n^3)\\) if \\(m &lt;&lt; n^2\\).\n\n\n\n5.2.8 Detecting Negative Length Cycles\nRecall: SPP-U (and SSPP-U) are unbounded if the graph contains a negative length cycle. Indeed, we can endlessly apply rounds of update operations and reduce the shortest path lengths \\(f_k(i)\\) indefinitely.\nWe can detect whether the problem is unbounded by running one more iteration of the algorithm beyond iteration \\(k = n-1\\).\n\nIf the graph contains a negative-length cycle then an update will take place.\nThis indicates the problem is unbounded and we can stop the algorithm.\n\n\n5.2.8.1 Example\nConsider the graph given in Figure 5.9. This graph has cycle \\(C = (2,3,4,2)\\) with length \\(-4 &lt; 0\\).\n\n\n\n\n\n\nFigure 5.9: Graph with negative cycle\n\n\n\nLet’s apply the Bellman-Ford algorithm to find the shortest \\((1,t)\\)-paths in this graph.\n\n5.2.8.1.1 Iteration 1 (\\(k=1\\))\nThe only node adjacent to \\(1\\) is \\(2\\). Thus, the\n\n\n\n\n\n\nFigure 5.10: Shortest Paths of length at most \\(k=1\\)\n\n\n\n\n\n\nTable 5.9: Length of shortest paths of length at most \\(1\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n\n\n\n\n\n\nTable 5.10: Predecessors in shortest paths of length at most \\(1\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\n\n\n\n\n\n5.2.8.1.2 Iteration 2 (\\(k=2\\))\nNext, we can reach \\(3\\) from \\(1\\) via a path consisting of \\(2\\) arcs. This path has length \\(-2\\).\n\n\n\n\n\n\nFigure 5.11: Shortest Paths of length at most \\(k=2\\)\n\n\n\n\n\n\nTable 5.11: Length of shortest paths of length at most \\(2\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\n\n\n\n\n\n\nTable 5.12: Predecessors in shortest paths of length at most \\(2\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\n\n\n\n\n\n5.2.8.1.3 Iteration 3 (\\(k=3\\))\nNode \\(4\\) is the only node that is reachable from \\(1\\) by a path containing \\(3\\) arcs; the path \\((1,2,3,4)\\) has length \\(3\\).\n\n\n\n\n\n\nFigure 5.12: Shortest Paths of length \\(k=3\\)\n\n\n\n\n\n\nTable 5.13: Length of shortest paths of length at most \\(3\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 5.14: Predecessors in shortest paths of length at most \\(3\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\n\n\n\n\n\n5.2.8.1.4 Iteration 4 (\\(k=4\\))\nLet’s try one more iteration!\nNote that \\[\n    f_3(4) + \\ell_{42} = 3 - 3 = 0 &lt; f_2(2).\n\\]\n\n\n\n\n\n\nFigure 5.13: Shortest Paths of length \\(k=4\\)\n\n\n\n\n\n\nTable 5.15: Length of shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\\(f_4(i)\\)\n0\n0\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 5.16: Predecessors in shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\\(P_4(i)\\)\n\\(\\sim\\)\n4\n2\n3\n\n\n\n\n\n\nThis implies that the path \\((1,2,3,4,2)\\) with \\(5\\) arcs has smaller total length than the path \\((1,2)\\). This contradicts ?lem-5-shortest-path and, hence, implies that the shortest path problem is unbounded for this graph. Indeed, further iterations will lead further decrease in \\(f_k(i)\\) for \\(i\\) in the negative length cycle \\((2,3,4,2)\\). We can stop the Bellman-Ford Algorithm after \\(k=n\\) iterations and declare the problem instance unbounded.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#iteration-4-k4-1",
    "href": "05a_Shortest_Paths.html#iteration-4-k4-1",
    "title": "5  The Shortest Path Problem",
    "section": "5.3 Iteration 4 (\\(k=4\\))",
    "text": "5.3 Iteration 4 (\\(k=4\\))\nLet’s try one more iteration!\nNote that \\[\n    f_3(4) + \\ell_{42} = 3 - 3 = 0 &lt; f_2(2).\n\\]\n\n\n\n\n\n\nFigure 5.13: Shortest Paths of length \\(k=4\\)\n\n\n\n\n\n\nTable 5.15: Length of shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\\(f_4(i)\\)\n0\n0\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 5.16: Predecessors in shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\\(P_4(i)\\)\n\\(\\sim\\)\n4\n2\n3\n\n\n\n\n\n\nThis implies that the path \\((1,2,3,4,2)\\) with \\(5\\) arcs has smaller total length than the path \\((1,2)\\). This contradicts ?lem-5-shortest-path and, hence, implies that the shortest path problem is unbounded for this graph. Indeed, further iterations will lead further decrease in \\(f_k(i)\\) for \\(i\\) in the negative length cycle \\((2,3,4,2)\\). We can stop the Bellman-Ford Algorithm after \\(k=n\\) iterations and declare the problem instance unbounded.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jarniks-theorem",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jarniks-theorem",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.1 Jarnik’s Theorem",
    "text": "7.1 Jarnik’s Theorem\n\nTheorem 7.1 (Jarnik’s Theorem)  \n\nLet \\(F\\) be the edge set of a tree strictly contained in an MST.\nLet \\(S\\subseteq V\\) be the set of nodes it spans.\n\nFor every edge \\(e \\in \\delta(S)\\):\n\n\\(F \\cup \\{e\\}\\) is part of a MST if and only if \\(e\\) has minimum cost in \\(\\delta(S)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-jpd-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-jpd-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.11 The Jarnik-Prim-Dijkstra (JPD) Algorithm",
    "text": "7.11 The Jarnik-Prim-Dijkstra (JPD) Algorithm\nInitialize E_T = {} and S = {1}.\n\nWhile |E_T| &lt; n-1:\n\n    Choose e = {v,w} in delta(S) of minimum cost c_e.  \n\n    Update E_T = E_T + {e} \n\n    Update S = S + {w}.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#complexity-of-the-jpd-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#complexity-of-the-jpd-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 Complexity of the JPD Algorithm",
    "text": "6.3 Complexity of the JPD Algorithm\nInitialize E_T = {} and S = {1}.\n\nWhile |E_T| &lt; n-1:\n\n    Choose e = {v,w} in delta(S) of minimum cost c_e.  \n\n    Update E_T = E_T + {e} \n\n    Update S = S + {w}.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#example-mst-for-flight-routing",
    "href": "06_Minimum_Cost_Spanning_Trees.html#example-mst-for-flight-routing",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 Example: MST for Flight Routing",
    "text": "6.3 Example: MST for Flight Routing\nThe following graph \\(G = (V, E)\\) gives available/potential flight routes between 8 cities, with distances in miles.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.10: Potential flight routes\n\n\n\n\n\nApplication: The MST of \\(G\\) gives an airline a means of servicing each city while minimizing travel distance and fuel costs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-1",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-1",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 JPD - Step 1",
    "text": "6.3 JPD - Step 1\n\n\n\n\n\n\nFigure 6.11: JPD Step 1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-10",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-10",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.27 ",
    "text": "6.27",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-2",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-2",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 JPD - Step 2",
    "text": "6.5 JPD - Step 2\n\n\n\n\n\n\nFigure 6.12: JPD Step 2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-11",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-11",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.30 ",
    "text": "6.30",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-3",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-3",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 JPD - Step 3",
    "text": "6.4 JPD - Step 3\n\n\n\n\n\n\nFigure 6.13: JPD Step 3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-12",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-12",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.33 ",
    "text": "6.33",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-4",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-4",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 JPD - Step 4",
    "text": "6.4 JPD - Step 4\n\n\n\n\n\n\nFigure 6.14: JPD Step 4",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-13",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-13",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.22 ",
    "text": "7.22",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-5",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-5",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.6 JPD - Step 5",
    "text": "6.6 JPD - Step 5\n\n\n\n\n\n\nFigure 6.15: JPD Step 5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-14",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-14",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.24 ",
    "text": "7.24",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-6",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-6",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.8 JPD - Step 6",
    "text": "6.8 JPD - Step 6\n\n\n\n\n\n\nFigure 6.16: JPD Step 6",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-15",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-15",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.26 ",
    "text": "7.26",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-7",
    "href": "06_Minimum_Cost_Spanning_Trees.html#jpd---step-7",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.10 JPD - Step 7",
    "text": "6.10 JPD - Step 7\n\n\n\n\n\n\nFigure 6.17: JPD Step 7",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-16",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-16",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.28 ",
    "text": "7.28",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#improving-the-algorithm-idea",
    "href": "06_Minimum_Cost_Spanning_Trees.html#improving-the-algorithm-idea",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.12 Improving the Algorithm – Idea",
    "text": "6.12 Improving the Algorithm – Idea\nProblem: The algorithm scans many edges that will never be selected.\n\nExample 6.4 Consider \\(S = \\{1, 3, 5\\}\\) for the graph below.\n\n\n\n\n\n\n\nFigure 6.18: Search via nodes",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-17",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-17",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.30 ",
    "text": "7.30",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#better-implementation-labels",
    "href": "06_Minimum_Cost_Spanning_Trees.html#better-implementation-labels",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 Better Implementation – Labels",
    "text": "6.4 Better Implementation – Labels\nIdea: If we knew which edge incident with each node has minimum cost, then we can search among nodes, not edges.\n\n\\(C(j) := \\min_{i \\in S}~c_{ij}\\) (minimum cost of edge connecting \\(S\\) to \\(j\\));\n\\(P(j) := \\text{argmin}_{i \\in S}~c_{ij}\\) (endpoint in \\(S\\) of this minimum cost edge).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#example-labels",
    "href": "06_Minimum_Cost_Spanning_Trees.html#example-labels",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 Example – Labels",
    "text": "6.5 Example – Labels\n\n\n\n\n\n\nFigure 6.20: Search via nodes",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#better-implementation-continued",
    "href": "06_Minimum_Cost_Spanning_Trees.html#better-implementation-continued",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.6 Better Implementation – Continued",
    "text": "6.6 Better Implementation – Continued\nIf we have \\(C\\) and \\(P\\), finding vertex \\(w\\) to add to minimum spanning tree should only required \\(O(n)\\) EOs (scanning the vertices in \\(V\\setminus S\\)).\n\nWe must update \\(C\\) and \\(P\\) whenever \\(S\\) is updated.\nTo do so, we search the star of \\(w\\), which requires \\(O(n)\\) EOs.\n\n\nComplexity is \\(O(n)\\) per iteration and \\(O(n^2)\\) total.\nCan be much smaller than \\(O(m)\\)!!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#better-algorithm-pseudocode",
    "href": "06_Minimum_Cost_Spanning_Trees.html#better-algorithm-pseudocode",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 Better Algorithm – Pseudocode",
    "text": "6.4 Better Algorithm – Pseudocode\nLet S = {1}, E_T = {}, C(1) = 0.\n\n% Initialize costs.\nFor j in V - {1}\n    Let C(j) = c_{1j} and P(j) = 1\n    (assume c_{1j} = inf if 1j not in E).\n\nWhile |E_T| &lt; n-1\n\n    Find w in V \\ S minimizing C(w). \n\n    Let S = S + {w} and E_T = E_T + {P(w), w}.  \n\n    % Update costs\n    For j in V \\ S such that wj in delta(w)\n\n        If c_{wj} &lt; C(j): \n            Update C(j) = c_{wj} and P(j) = w.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#another-example",
    "href": "06_Minimum_Cost_Spanning_Trees.html#another-example",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.5 Another Example",
    "text": "6.5 Another Example\n\n\n\nGraph \\(G\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-18",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-18",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.36 ",
    "text": "7.36",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#iteration-2",
    "href": "06_Minimum_Cost_Spanning_Trees.html#iteration-2",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 Iteration 2",
    "text": "6.4 Iteration 2\n\n\n\nGraph \\(G\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-19",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-19",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.38 ",
    "text": "7.38",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#iteration-3",
    "href": "06_Minimum_Cost_Spanning_Trees.html#iteration-3",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.4 Iteration 3",
    "text": "6.4 Iteration 3\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.23: Step 3 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after three steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n5\n3\n\n\n5\n2\n2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-20",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-20",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.40 ",
    "text": "7.40",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#iteration-4",
    "href": "06_Minimum_Cost_Spanning_Trees.html#iteration-4",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.7 Iteration 4",
    "text": "6.7 Iteration 4\n\n\n\nGraph \\(G\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#section-21",
    "href": "06_Minimum_Cost_Spanning_Trees.html#section-21",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "7.42 ",
    "text": "7.42",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#preliminaries",
    "href": "06_Minimum_Cost_Spanning_Trees.html#preliminaries",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "",
    "text": "6.1.1 Subgraphs\n\nDefinition 6.1 Let \\(G = (V,E)\\) be an undirected graph.\nWe call \\(G_S = (V_S, E_S)\\) a subgraph of \\(G\\) if\n\n\\(G_S\\) is a graph;\n\\(V_S \\subseteq V\\) and \\(E_S \\subseteq E\\).\n\n\n\nExample 6.1 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The graph given in Figure 6.2 (b) is a subgraph of \\(G\\). However, the graph given in Figure 6.2 (c) is not a subgraph of \\(G\\) since \\(\\{1,4\\} \\notin E\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A subgraph\n\n\n\n\n\n\n\n\n\n\n\n(c) Not a subgraph\n\n\n\n\n\n\n\nFigure 6.1: A subgraph of \\(G\\) and a graph that is not a subgraph of \\(G\\).\n\n\n\n\n\n6.1.2 Trees\n\nDefinition 6.2 Let \\(G = (V,E)\\) be an undirected graph.\nThe subgraph \\(G_T = (V_T, E_T)\\) is a tree if\n\n\\(G_T\\) is a connected;\n\\(G_T\\) is acyclic, i.e., contains no cycles.\n\nA tree \\(G_T\\) is a spanning tree if \\(V_T = V\\) (\\(G_T\\) spans/reaches all nodes in \\(V\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A Tree\n\n\n\n\n\n\n\n\n\n\n\n(c) A Spanning Tree\n\n\n\n\n\n\n\nFigure 6.2: Trees in given graph \\(G\\).\n\n\n\n\nExample 6.2 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The subgraph given in Figure 6.2 (b) is a connected and acyclic; therefore, it is a tree. On the otherhand, the subgraph given in Figure 6.2 (c) is a tree and has node set \\(V\\); therefore, it is a spanning tree.\n\n\n\n6.1.3 Motivating Example\nWe want to build a new high-speed network at the University:\n\nshould connect all buildings; while\ncosting as little as possible.\n\nWe can model this problem as that of finding a minimum cost spanning tree.\nConsider the graph \\(G\\) with:\n\nBuildings as vertices;\nPotential connections as edges.\n\nWe want to find a connected, acyclic subgraph to minimize cost while ensuring all buildings are connected. Such a tree is highlighted in red in Figure 6.3.\n\n\n\n\n\n\nFigure 6.3: Campus map with minimum spanning tree\n\n\n\n\n\n6.1.4 The Minimum Cost Spanning Tree (MST) Problem\n\nDefinition 6.3 Given an undirected graph \\(G = (V,E)\\) and cost function \\(c: E\\mapsto \\mathbf{R}\\).\nThe minimum cost spanning tree problem aims to find a spanning tree \\(G_T = (V_T, E_T)\\) of minimum total cost: \\[\\min~ c(E_T) = \\sum_{e \\in E_T} c_e.\\]\n\n\nTheorem 6.1 (Cayley – 1889) A complete undirected graph with \\(n\\) nodes contains \\(n^{n-2}\\) spanning trees.\n\nNoncomplete graphs contain fewer spanning trees, but the number is still exponentially large in \\(n\\). This implies that we cannot solve MST using complete enumeration for even small graphs.\n\n\n6.1.5 Leaves\n\nDefinition 6.4 The nodes of a graph with degree 1 are called leaves.\n\n\nTheorem 6.2 A tree contains at least one leaf.\n\n\nProof. Suppose, on the contrary, that tree \\(T\\) does not contain a leaf. Then every node in \\(T\\) has degree at least \\(2\\). This implies that \\(T\\) has a cycle; a contradiction.\n\n\n\n\n\n\n\nFigure 6.4: A tree with leaves ${1, 2, 3, 6}\n\n\n\n\n\n6.1.6 The Number of Edges of A Tree\n\nTheorem 6.3 A tree \\(G_T\\) with \\(n\\) nodes has \\(m = n-1\\) edges.\n\n\nProof. We’ll prove Theorem 6.3 by induction.\nAs a base case, consider \\(n=1\\). This tree consists of a single node and \\(0 = n-1\\) edges.\nNow suppose that every tree with \\(k\\) nodes has \\(n-1\\) edges. Consider tree \\(T\\) with \\(k+1\\) nodes. We want to show that \\(T\\) has \\(k\\) edges.\nTo do so, note that \\(T\\) contains at least one leaf by Theorem 6.2. Without loss of generality, let’s assume that node \\(1\\) is a leaf; if not, we can relabel vertices so that \\(1\\) is a leaf. Let’s remove leaf \\(1\\) and its incident edge (there is only one such edge because \\(1\\) has degree-\\(1\\)).\nAfter deleting this node and edge, we have a connected, acyclic graph with \\(k\\) nodes. By the inductive hypothesis, this tree has \\(k-1\\) edges. Since we deleted \\(1\\) node and \\(1\\) edge, we can conclude that \\(T\\) had \\(k\\) edges. This completes the proof.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\) with leaf \\(1\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) After removing \\(\\{1,5\\}\\)\n\n\n\n\n\n\n\nFigure 6.5: Illustration of the pruning process. After removing node \\(1\\) and edge \\(\\{1,5\\}\\), we are left with a tree with \\(5\\) nodes.\n\n\n\n\n\n6.1.7 The Swap Property\n\nLemma 6.1 Given tree \\(G_T = (V_T, E_T)\\), removing an edge \\(e \\in E_T\\) creates two subtrees.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A tree\n\n\n\n\n\n\n\n\n\n\n\n(b) A subtree\n\n\n\n\n\n\n\n\n\n\n\n(c) Another subtree\n\n\n\n\n\n\n\nFigure 6.6: Subtrees obtained after deleting edge \\(\\{4,5\\}\\).\n\n\n\n\n\n6.1.8 Swapping Edges Within a Cut\n\nLemma 6.2 Let \\(S\\) be the vertex set of one of the two subtrees.\nFor every edge \\(f \\in \\delta(S)\\) other than \\(e\\), the set \\[\nE_T' := E_T \\cup \\{f\\} \\setminus \\{e\\}\n\\] is the edge set of a tree spanning the same set of nodes.\n\n\nExample 6.3 Consider the graph \\(G\\) with tree \\(T\\) given by Figure 6.7 (a). The example in Figure 6.6 illustrates that we obtain two subtrees after removing edge \\(e=45\\).\nThe set \\(S = \\{1,2,3, 5\\}\\) is the vertex set of one of these trees. The cut induced by \\(S\\) is \\[\n    \\delta(S) = \\{02, 16, 24\\}.\n\\] Lemma 6.2 implies that exchanging \\(02\\) with \\(45\\) yields a new tree \\(\\tilde{T}\\) with edge set \\[\nE_{\\tilde T} = E_T \\setminus \\{4,5\\} \\cup \\{0,2\\}\n    = \\{15, 25, 35, 46, 02\\}.\n\\] See Figure 6.7 for an illustration of this process.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Tree \\(\\tilde T\\) after exchange\n\n\n\n\n\n\n\nFigure 6.7: Example of swapping edges \\(02\\) and \\(45\\) using cuts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.2 The Jarnik-Prim-Dijkstra Algorithm",
    "text": "6.2 The Jarnik-Prim-Dijkstra Algorithm\n\n6.2.1 Jarnik’s Theorem\n\nTheorem 6.4 (Jarnik’s Theorem)  \n\nLet \\(F\\) be the edge set of a tree strictly contained in an MST.\nLet \\(S\\subseteq V\\) be the set of nodes it spans.\n\nFor every edge \\(e \\in \\delta(S)\\):\n\n\\(F \\cup \\{e\\}\\) is part of a MST if and only if \\(e\\) has minimum cost in \\(\\delta(S)\\).\n\n\n\nProof. We start by proving the “only if” part. Let’s suppose that \\(F \\cup \\{e\\}\\) is part of a MST. We want to show that \\(e\\) has minimum cost in \\(\\delta(S)\\) in this case. We’ll use a proof by contradiction. Let’s assume that there is \\(f\\in \\delta(S)\\) with \\(c_f &lt; c_e\\).\nLet \\(T = (V, E_T)\\) be a minimum spanning tree such that \\(F \\subseteq E_T\\) and \\(F \\cup \\{e\\} \\subseteq E_T\\). By the Swap Property (Lemma 6.1) the subgraph \\[\n\\tilde T = (V, E_T\\setminus\\{e\\} \\cup \\{f\\})\n\\] is also a spanning tree. Moreover, the cost of \\(\\tilde T\\) satisfies \\[\n    c(\\tilde T) = c(T) - c_e + c_f &lt; c(T)\n\\] since \\(c_e &gt; c_f\\).\nThis implies that \\(T\\) is not a minimum cost spanning tree; a contradiction. Therefore, we can conclude that if \\(F \\cup \\{e\\}\\) is part of a minimum spanning tree then \\(c_e \\le c_f\\) for all \\(f \\in \\delta(S)\\).\nThis is illustrated in Figure 6.8. Consider the spanning tree \\(T\\) given in Figure 6.8 (a) and the edge set \\(F = \\{12\\} \\subseteq E_T\\); here, \\(S = \\{1,2\\}\\). Note that \\(e = 15\\) is in the both \\(E_T\\) and \\(\\delta(S)\\), while \\(f=23\\) belongs to \\(\\delta(S)\\), but not \\(E_T\\). The swap property implies that we can obtain a spanning tree \\(\\tilde T\\) by exchanging the edges \\(e\\) and \\(f\\). If \\(c_f &lt; c_e\\) then \\(\\tilde T\\) is a spanning tree with strictly lower cost than \\(T\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(T\\), \\(S = \\{1,2\\}\\), and \\(e=15\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\delta(S)\\) and \\(f=23\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(\\tilde T\\) and \\(f=23\\)\n\n\n\n\n\n\n\nFigure 6.8: Graph with spanning tree \\(T\\) containing \\(F = \\{12\\}\\) and edge \\(e = 15\\) and edge \\(f = 23\\). If \\(c_e &gt; c_f\\), then \\(\\tilde T\\) is a spanning tree is smaller cost than \\(T\\).\n\n\n\nWe next prove the “if” part. That is, we prove that if \\(e\\) has minimum cost in \\(\\delta(S)\\) then \\(F \\cup \\{e\\}\\) is part of a minimum spanning tree. To do so, suppose that \\(c_e \\le c_f\\) for all \\(f \\in \\delta(S)\\). We want to show that \\(F \\cup \\{e\\}\\) is part of a MST.\nWe consider two cases. First, suppose that \\(e \\in E_T\\), where \\(T\\) is the MST containing \\(f\\). This immediately implies that \\(e\\) belongs to a MST and we’re done.\nNext, suppose that \\(e \\notin E_T\\). Specifically, let \\(e = uv\\) for nodes \\(u, v \\in V\\) such that \\(e \\notin E_T\\) and \\(e \\in \\delta(S)\\); we assume that \\(u \\in S\\) and \\(v \\in V \\setminus S\\).\nLet \\(P\\) be the \\((u,v)\\)-path joining the end points of \\(e\\) belonging to \\(T\\). Such a path always exists because \\(T\\) is a spanning tree. Since \\(P\\) starts in \\(S\\) at \\(u\\) and ends in \\(V\\setminus S\\) at \\(v\\) there is at least one edge \\(f\\) in both \\(P\\) and \\(\\delta(S)\\), i.e., there is edge \\(f \\in P \\cap \\delta(S)\\).\nBy the swap property, the subgraph \\[\nT' = (V, E_T \\setminus \\{f\\} \\cup \\{e\\})\n\\] is a spanning tree. Moreover, \\[\nc(T) \\ge c(T') = c(T) - c_f + c_e \\le c(T)\n\\] since \\(T\\) is a minimum spanning tree and \\(c_e \\le c_f\\). This is only possible if \\(c_e = c_f\\) and \\(T'\\) is also a minimum spanning tree.\nTo illustrate this phenomena, consider the edge \\(e = 15\\) in Figure 6.9. This edge is not included in the minimum spanning tree \\(T\\). However, there is a path \\(P = (1,2,3,5)\\) in \\(T\\) containing the edge \\(f = 23 \\in \\delta(\\{1,2\\})\\). Applying the swap property and the assumption the \\(c_e \\le c_f\\) shows that \\(T'\\) is also a minimum cost spanning tree.\n\n\n\n\n\n\n\n\n\n\n\n(a) Minimum spanning tree \\(T\\) not including \\(e = 15\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) Path in \\(T\\) from endpoints of \\(e\\) containing \\(f=23\\).\n\n\n\n\n\n\n\n\n\n\n\n(c) Minimum spanning tree \\(T'\\) obtained by exchanging \\(e\\) and \\(f\\).\n\n\n\n\n\n\n\nFigure 6.9: Minimum spanning trees \\(T\\) and \\(T'\\) obtained by exchanging arcs \\(e\\) and \\(f\\) with minimum cost in \\(\\delta(\\{1,2\\})\\).\n\n\n\n\n\n\n6.2.2 The Jarnik-Prim-Dijkstra (JPD) Algorithm\nTheorem 6.4 suggests the following algorithm for identifying a minimum cost spanning tree in a given graph.\nInitialize E_T = {} and S = {1}.\n\nWhile |E_T| &lt; n-1:\n\n    Choose e = {v,w} in delta(S) of minimum cost c_e.  \n\n    Update E_T = E_T + {e} \n\n    Update S = S + {w}.  \n\n6.2.2.1 Complexity of the JPD Algorithm\nFor each step of the while loop, choosing \\(e\\) in Line 5$ costs \\(O(m)\\) operations to search over the set of edges in \\(\\delta(S)\\). The loop is repeated \\(O(n)\\) times, so the total complexity is \\(O(mn)\\) elementary operations. This may be as large as \\(O(n^3)\\) when the graph is very dense of as small as \\(O(n^2)\\) when the graph is very sparse but still connected.\n\n\n\n6.2.3 Example: MST for Flight Routing\nThe following graph \\(G = (V, E)\\) gives available/potential flight routes between 8 cities, with distances in miles. The MST of \\(G\\) gives an airline a means of servicing each city while minimizing travel distance and fuel costs.\n\n\n\n\n\n\nFigure 6.10: Potential flight routes\n\n\n\nLet’s apply the Jarnik-Prim-Dijsktra Algorithm to find the minimum cost spanning tree in \\(G\\).\n\n6.2.3.1 Step 1\nWe start with \\(S = \\{1\\}\\), which induces the cut \\(\\delta(S) = \\{14, 18\\}\\). Since \\[\nc_{14} = 355 &lt; 695 = c_{18},\n\\] we add \\(14\\) to \\(E_T\\) and add \\(\\{4\\}\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.11: Step 1 of the JPD Algorithm\n\n\n\n\n\n6.2.3.2 Step 2\nNext, \\(\\delta(S) = \\{18, 24, 34, 74\\}\\). We add \\(2\\) to \\(S\\) and \\(24\\) to \\(E_T\\) because \\(c_{24} = 74\\) is the minimum cost edge in \\(\\delta(S)\\).\n\n\n\n\n\n\nFigure 6.12: Step 2 of the JPD Algorithm\n\n\n\n\n\n6.2.3.3 Step 3\nNext, \\(c_{34} = 262\\) has minimum cost among edges in \\(\\delta(S) = \\{18, 34, 47, 27\\}\\). We add \\(34\\) to \\(E_T\\) and \\(3\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.13: Step 3 of the JPD Algorithm\n\n\n\n\n\n6.2.3.4 Step 4\nWe next add \\(37\\) to \\(E_T\\) and \\(7\\) to \\(S\\) because \\(c_{37} = 242\\) has minimum cost among edges in \\(\\delta(S) = \\{37, 47, 27, 18\\}\\).\n\n\n\n\n\n\nFigure 6.14: Step 4 of the JPD Algorithm\n\n\n\n\n\n6.2.3.5 Step 5\nWe next add \\(67\\) to \\(E_T\\) and \\(6\\) to \\(S\\) because \\(c_{67} = 83\\) has minimum cost among edges in \\(\\delta(S) = \\{18, 75, 76, 78\\}\\).\n\n\n\n\n\n\nFigure 6.15: Step 5 of the JPD Algorithm\n\n\n\n\n\n6.2.3.6 Step 6\nNext, \\(c_{78} = 151\\) is minimum cost among edges in \\(\\delta(S) = \\{18, 56, 57, 78\\}\\). Add \\(8\\) to \\(S\\) and \\(78\\) to \\(E_T\\).\n\n\n\n\n\n\nFigure 6.16: Step 6 of the JPD Algorithm\n\n\n\n\n\n6.2.3.7 Step 7\nFinally, \\(\\delta(S) = \\{56, 57\\}\\). We have \\[\nc_{56} = 230 &lt; 306 = c_{57},\n\\] so we add \\(56\\) to \\(E_T\\), \\(5\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.17: Step 7 of the JPD Algorithm\n\n\n\n\n\n6.2.3.8 Termination\nNote that \\(S = \\{V\\}\\) and \\(E_T\\) contains \\(n-1=7\\) edges. This implies that we have constructed the minimum spanning tree \\(T\\).\n\n\n\n\n\n\nFigure 6.18: Minimum spanning tree identified by the JPD Algorithm",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#improving-the-jpd-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#improving-the-jpd-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 Improving the JPD Algorithm",
    "text": "6.3 Improving the JPD Algorithm\n\n6.3.1 Inefficiency of Searching over Edges\nProblem: The algorithm scans many edges that will never be selected.\nTo see why this may be the the case, consider the set \\(S = \\{1, 3, 5\\}\\) in the graph given in Figure 6.19.\n\n\n\n\n\n\nFigure 6.19: Graph \\(G\\) with \\(S = \\{1,3,5\\}\\)\n\n\n\nFor \\(S = \\{1,3,5\\}\\), we have \\(\\delta(S) = \\{21, 23, 25, 41, 45 \\}\\). The JPD Algorithm will add edge \\(45\\) with minimum cost to \\(E_T\\), but will consider every edge in \\(G\\). However, we know that the only candidates to be added to \\(S\\) are Nodes \\(2\\) and \\(4\\). Since \\(23\\) and \\(45\\) are the minimum cost edges incident with \\(2\\) and \\(4\\) respectively, we really only need to consider these two edges as candidates for \\(E_T\\).\nThis example implies that if we knew which edge incident with each node has minimum cost, then we can search among nodes, not edges. Since we usually have far fewer nodes than edges, this can lead to substantial improvement in computational cost.\n\n\n6.3.2 Node Labels\nTo facilitate this, let’s introduce two labels for each node \\(j \\in V \\setminus S\\):\n\n\\(C(j) := \\min_{i \\in S}~c_{ij}\\) (minimum cost of edge connecting \\(S\\) to \\(j\\));\n\\(P(j) := \\text{argmin}_{i \\in S}~c_{ij}\\) (endpoint in \\(S\\) of this minimum cost edge).\n\n\nExample 6.4 For the graph given in Figure 6.19, we have \\[\n\\begin{aligned}\n    C(2) &= 4, & C(4) &= 2  \\\\\n    P(2) &= 3, & P(3) &= 5.\n\\end{aligned}\n\\]\n\nIf we have \\(C\\) and \\(P\\), choosing the vertex \\(w\\) to add to minimum spanning tree should only require \\(O(n)\\) EOs to compare the values of \\(C\\) for the vertices in \\(V\\setminus S\\).\nFurther, we must update \\(C\\) and \\(P\\) whenever \\(S\\) is updated. To do so, we search the star of \\(w\\), which requires \\(O(n)\\) EOs.\nThe total complexity is \\(O(n)\\) per iteration and \\(O(n^2)\\) total. This can be much smaller than \\(O(mn)\\)!!\n\n\n6.3.3 Better Algorithm – Pseudocode\nThis suggests the following algorithm.\nLet S = {1}, E_T = {}, C(1) = 0.\n\n% Initialize costs.\nFor j in V - {1}\n    Let C(j) = c_{1j} and P(j) = 1\n    (assume c_{1j} = inf if 1j not in E).\n\nWhile |E_T| &lt; n-1\n\n    Find w in V \\ S minimizing C(w). \n\n    Let S = S + {w} and E_T = E_T + {P(w), w}.  \n\n    % Update costs\n    For j in V \\ S such that wj in delta(w)\n\n        If c_{wj} &lt; C(j): \n            Update C(j) = c_{wj} and P(j) = w.\n\n\n6.3.4 Another Example\nTo illustrate the application of the revised algorithm, let’s consider the graph \\(G\\) given in Figure 6.20.\n\n\n\n\n\n\nFigure 6.20: Graph \\(G\\)\n\n\n\n\n6.3.4.1 Step 1\nInitially, we have \\(S = \\{1\\}\\). Note that \\[\n\\begin{aligned}\n    C(2) &= c_{12} = 1 \\\\\n    C(3) &= c_{13} = 1 \\\\\n    C(4) &= + \\infty \\\\\n    C(5) &= c_{15} = 8.\n\\end{aligned}\n\\] We choose \\(w=2\\). We add \\(2\\) to \\(S\\) and \\(12\\) to \\(E_T\\). Note that we could also choose to add \\(3\\) to \\(S\\) and \\(13\\) to \\(E_T\\) since \\(c_{12} = c_{13} = 1\\).\nIt remains to update \\(C\\) and \\(P\\). Note that \\[\n\\begin{aligned}\n    C(3) &= c_{13} = 1 &lt; c_{23} = c_{w3} \\\\\n    c_{w5} &=c_{25} = 2 &lt; C(5).\n\\end{aligned}\n\\] Thus, we set \\(C(5) = 2\\) and \\(P(5) = 2\\) and leave \\(C(3)\\), \\(P(3)\\) unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.21: Step 1 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found so far.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\n6.3.4.2 Step 2\nWe have \\(S = \\{1,2\\}\\). Nodes \\(3\\) and \\(5\\) are adjacent to \\(1\\) and \\(2\\). Since \\[\nC(3) = 1 &lt; 2 = C(5),\n\\] we add \\(3\\) to \\(S\\); since \\(P(3)=1\\), we add \\(13\\) to \\(E_T\\). Finally, we update \\[\nC(4) = 5, P(4) = 3\n\\] since \\(4\\) is adjacent to \\(3\\); \\(C(5)\\) and P(5)$ are unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.22: Step 2 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after two steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n5\n3\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\n6.3.4.3 Step 3\nSince $ C(5) &lt; C(4), $ we add \\(5\\) to \\(S\\) and \\(25\\) to \\(E_T\\). Note that \\[\n    c_{45} = 4 &lt; C(4) = 5,\n\\] we set \\(C(4) = 4\\) and \\(P(4) = 5\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.23: Step 3 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after three steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n5\n3\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\n6.3.4.4 Step 4\nThe only edge left to add \\(E_T\\) is \\(54\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.24: Step 4 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after four steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n4\n5\n\n\n5\n2\n2\n\n\n\n\n\n\nThis gives the minimum cost spanning tree \\(T\\) found in Figure 6.25.\n\n\n\n\n\n\nFigure 6.25: Mininum spanning tree in \\(G\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  }
]