[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 2014 Algorithms",
    "section": "",
    "text": "Preface\nThis document collects lecture notes for Math 2014 Algorithms.\nPlease follow the page navigation for each topic covered in the module.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_Introduction.html",
    "href": "01_Introduction.html",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "1.1 Problems and Problem Instances\nA problem \\(P\\) is a general class of questions to answer.\nProblems are (infinite) families of general questions.\nWe call a version of the problem with specific values a problem instance \\(I\\) (or just instance).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#problems-and-problem-instances",
    "href": "01_Introduction.html#problems-and-problem-instances",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "Example 1.1 (Quadratic Equations) Finding the roots of a quadratic equation \\(ax^2 + bx + c = 0\\) is an example of a problem.\nFinding the roots of a specific quadratic function is an instance of this problem. For example, finding \\(x\\) such that \\(2x^2 + 8x + 5 = 0\\) is a problem instance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#algorithms",
    "href": "01_Introduction.html#algorithms",
    "title": "1  Introduction to Algorithms",
    "section": "1.2 Algorithms",
    "text": "1.2 Algorithms\nWe are interested in finding procedures for solving every possible instance of a given problem. Such a procedure is called an algorithm.\n\nDefinition 1.1 (Algorithm) An algorithm for a given problem is a finite sequence of operations which return the correct solution for all problem instances.\n\n\nExample 1.2 (An Algorithm for Quadratic Equations) We solve quadratic equations of the form \\(ax^2 + bx + c = 0\\) using the quadratic formula: \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\\]\nWe can think of evaluating this formula as an algorithm consisting of the steps:\n\nCompute \\(u:= b^2 - 4ac\\).\nCompute \\(v:= 2a\\).\nCompute \\(z := \\sqrt{u} = \\sqrt{b^2 - 4ac}\\).\nCompute \\[\nx_1 = \\frac{-b + z}{v}, \\hspace{0.5in}\nx_2 = \\frac{-b - z}{v}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#goals-of-the-module",
    "href": "01_Introduction.html#goals-of-the-module",
    "title": "1  Introduction to Algorithms",
    "section": "1.3 Goals of the Module",
    "text": "1.3 Goals of the Module\nWe are interested in:\n\nDesigning algorithms for solving specific problems.\nProving that a proposed algorithm produces correct solutions.\nComplexity of algorithms: Analysing how long it takes for the algorithm to terminate, i.e., how many operations are required in general.\nComplexity of problems: how long it may take to solve a problem, regardless of which algorithm is used.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#combinatorial-optimization",
    "href": "01_Introduction.html#combinatorial-optimization",
    "title": "1  Introduction to Algorithms",
    "section": "1.4 Combinatorial Optimization",
    "text": "1.4 Combinatorial Optimization\n\n1.4.1 Combinatorial Optimization Problems\nWe will focus on combinatorial optimization problems: \\[\\min c(X) \\text{ such that } X \\in \\mathcal{X},\\] where \\(c: \\mathcal{X} \\mapsto \\mathbf{R}\\) is a cost function and \\(\\mathcal{X}\\) is a finite set of feasible solutions.\n\n\n1.4.2 Solving Combinatorial Problems\n\nSince \\(\\mathcal{X}\\) is finite, we can always solve by complete enumeration: exhaustively calculating \\(c(X)\\) for each \\(X \\in \\mathcal{X}\\) to find one with minimum cost.\nIn practice, \\(\\mathcal{X}\\) is extremely large and complete enumeration is prohibitively expensive.\nIn many cases, \\(\\mathcal{X}\\) is defined implicitly as description and enumerating all possible solutions is a difficult task on its own.\nWe want faster, specialized algorithms exploiting mathematical structure of the given problem.\n\n\n\n1.4.3 Examples of Combinatorial Problems\nCombinatorial optimization is ubiquitous in operations research and management science1\n\nExample 1.3 (Travelling Salesperson Problem) Find a tour of shortest length visiting each location exactly once. Figure 1.1 gives an instance of the TSP and a solution where a tour of 15 cities in Germany is sought.\n\n\n\n\n\n\nFigure 1.1: An optimal traveling salesperson tour through Germany’s 15 largest cities (among over \\(43\\) billion possible routes).\n\n\n\n\n\nExample 1.4 (The Shortest Path Problem) Find shortest/minimum length route in network from origin to destination.\n\n\nExample 1.5 (Knapsack/Assignment Problems) Find maximum value/minimum cost distribution of limited resources.\n\n\n\n1.4.4 Types of Problems\nWe will focus on three primary forms of combinatorial optimization problems.\n\nDefinition 1.2 (Decision Problems) Given set \\(\\mathcal{X}\\), cost \\(c:\\mathcal{X}\\mapsto\\mathbf{R}\\) and scalar \\(L \\in \\mathbf{R}\\): \\[\\text{Determine if there is a } X \\in \\mathcal{X}\n        \\text{ with } c(X)\n        \\le L.\\]\n\nA decision problem takes problem instance defined by \\(\\mathcal{X}\\), \\(c\\), and \\(L\\) as input. An algorithm for this problem would output Yes or No depending on the instance.\n\nDefinition 1.3 (Decision problem – Search version) Given \\(\\mathcal{X}\\), \\(c\\), and \\(L\\) as input. Find \\(X \\in \\mathcal{X}\\) with \\(c(X) \\le L\\).\n\n\nDefinition 1.4 (Optimization problem – Search version) Given \\(\\mathcal{X}\\) and \\(c\\) as input. Find \\(X \\in \\mathcal{X}\\) minimizing \\(c(X)\\).\n\n\n\n1.4.5 Exact and Approximation Algorithms\nThere are two primary classes of algorithms that we will consider: exact and approximation algorithms.\n\nDefinition 1.5 (Exact Algorithms) Exact algorithms provide an exact solution to the problem.\n\n\nDefinition 1.6 (Approximation Algorithms) Approximation Algorithms provide an approximate solution to the problem, but not the exact solution in general.\n\nAn \\(\\alpha\\)-approximate algorithm returns \\(\\tilde X\\) within \\(\\alpha\\)-ratio of optimal value: \\(c(X^*) \\le c(\\tilde X) \\le \\alpha\n\\cdot c(X^*),\\) for some \\(\\alpha &gt; 1\\).\n\n\nAside from these two classes, we also have heuristics which provide potential solutions without guarantees of quality.\n\nDefinition 1.7 (Heuristics) Heuristic algorithms or heuristics provide solutions without guarantee of closeness to the optimal solution \\(c(X^*)\\).\n\n\n\n1.4.6 Deterministic vs Random Algorithms\nWe can also classify algorithms based on whether steps are performed deterministically or at random.\n\nDefinition 1.8 (Deterministic Algorithms) Deterministic Algorithms follow the same series of steps for given input: \\[ \\text{{if A = B do X; else do Y}} \\]\n\n\n\nDefinition 1.9 (Randomized Algorithms) Randomized algorithms have steps depending on random operations: \\[\\text{{Toss coin: if heads, do X; else do Y}}\\]\nA randomized exact/\\(\\alpha\\)-algorithm may only return exact or \\(\\alpha\\) solution within a certain probability.\n\n\n\n1.4.7 Offline vs Online vs Robust Algorithms\nFinally, we can further classify algorithms based on how they process information into problem instances.\n\nDefinition 1.10 (Offline Algorithm) The problem instance \\(I\\) is completely known at the beginning of the algorithm.\n\n\nDefinition 1.11 (Online Algorithm) The problem instance \\(I\\) is not completely known at start of the algorithm. Instead, the algorithm adapts to \\(I\\) as \\(I\\) unfolds over time.\n\n\nDefinition 1.12 Robust Algorithm: Instance \\(I\\) is not completely known at the beginning of the algorithm and it is not revealed over time.\nThe algorithm finds a solution that works for all possible realizations that \\(I\\) can take.\n\n\n\n1.4.8 Example – Canadian Traveller Problem (CTP)\nShortest Path Problem with added complexity of not knowing which roads/routes are unavailable due to the weather beforehand2.\n\n1.4.8.1 Deterministic/Off-line Version\nWe want to find the shortest or minimum length path in a network from origin to destination.\nIf the network routes are deterministic and are known then we have a deterministic exact algorithm for the shortest path problem. We’ll discuss this algorithm at length later in the term.\n\n\n1.4.8.2 The Random Case\nLet’s suppose that the routes are random or partially observed. For example, this could occur when inclement weather causes certain roadways to close, but they are not known until the road closure is encountered en route.\n\nAn online algorithm would find a partial path using known routes (so far), and then dynamically update the solution as conditions change or become known.\nA robust algorithm would find a path/itinerary that works under any weather conditions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#footnotes",
    "href": "01_Introduction.html#footnotes",
    "title": "1  Introduction to Algorithms",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Combinatorial_optimization↩︎\nhttps://en.wikipedia.org/wiki/Canadian_traveller_problem↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Algorithms</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html",
    "href": "02_Introduction_to_Graphs.html",
    "title": "2  Introduction to Graphs",
    "section": "",
    "text": "2.1 Directed Graphs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#directed-graphs",
    "href": "02_Introduction_to_Graphs.html#directed-graphs",
    "title": "2  Introduction to Graphs",
    "section": "",
    "text": "2.1.1 Introduction\n\nDefinition 2.1 (Directed Graphs) A directed graph is an ordered pair \\(G := (V, A)\\) composed of:\n\na set \\(V\\) of vertices or nodes of size/cardinality \\(n:=|V|\\);\na set \\(A\\) of ordered pairs called arcs of cardinality \\(m:= |A|\\).\n\nFor an arc \\((i,j) \\in A\\), we call \\(i\\) the tail and \\(j\\) the head.\n\n\nWe will focus on graphs without self-loops: \\((i,i)\\) is not an arc!\n\n\nExample 2.1 Consider \\(G = (V, A)\\) with \\[\\begin{aligned}\n    V &= \\{1,2,3,4,5\\},  \\\\ \\\\\n    A &= \\{(1,2), (1,3), (1,5), (2,4), (3,2), (4,1), (5,1), (5,3)\\}.\n\\end{aligned}\n\\]\nWe can represent or visualize \\(G\\) as nodes \\(V\\) joined by lines/arrows corresponding to arcs \\(A\\). Figure 2.1 gives several different representations of this graph.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A Visualization of the Graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Visualization\n\n\n\n\n\n\n\n\n\n\n\n(c) A Third Visualization\n\n\n\n\n\n\n\nFigure 2.1: Three representations of the graph given in Example 2.1.\n\n\n\n\n\n2.1.2 Adjacency\nArcs in a graph define pairwise relationships between nodes.\n\nDefinition 2.2 (Adjacent nodes) Node \\(i \\in V\\) is adjacent to node \\(j \\in V\\) if arc \\((i,j)\\) belongs to \\(A\\).\nIn this case, the arc \\((i,j) \\in A\\) is incident to node \\(j\\).\n\n\nExample 2.2 Consider the graph \\(G=(V,A)\\) given in Example 2.1. Node \\(1\\) is adjacent to \\(3\\) because the arc \\((1,3) \\in A\\) is included in the graph. However, Node \\(3\\) is adjacent to \\(1\\) since \\((3,1) \\notin A\\). See Figure 2.2 for illustrations of these relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(1\\) is adjacent to \\(3\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(3\\) is not adjacent to \\(1\\)\n\n\n\n\n\n\n\nFigure 2.2: Adjacency relations of nodes \\(1\\) and \\(3\\) in \\(G\\) given in Example 2.1.\n\n\n\n\n\n2.1.3 Complete Graphs\n\nDefinition 2.3 (Complete Graphs) A directed graph \\(G = (V, A)\\) is complete if:\n\n\\(A\\) contains an arc for each pair of nodes in \\(V\\);\nEquivalently, every node in \\(V\\) is adjacent to every other node.\n\n\n\nExample 2.3 The graph given in Example 2.1 is not complete or incomplete. Indeed, there are several potential arcs, e.g., \\((4,5)\\), which are not present in this graph.\nHowever, Figure 2.3 (a) provides a visualization of the complete graph on \\(4\\) nodes. Each of the possible arcs between pairs of nodes is present.\n\n\n\n\n\n\n\n\n\n\n\n(a) A Complete Graph with 4 nodes\n\n\n\n\n\n\n\n\n\n\n\n(b) An Incomplete Graph\n\n\n\n\n\n\n\nFigure 2.3: The complete graph with \\(n=4\\) nodes and the incomplete graph from Example 2.1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#number-of-arcs-in-a-complete-graph",
    "href": "02_Introduction_to_Graphs.html#number-of-arcs-in-a-complete-graph",
    "title": "2  Introduction to Graphs",
    "section": "2.2 Number of Arcs in a Complete Graph",
    "text": "2.2 Number of Arcs in a Complete Graph\n\nTheorem 2.1 For any directed graph \\(G\\), we have \\(m \\le n(n-1)\\). If \\(G\\) is complete then \\(m = n(n-1)\\).\n\n\nProof. Each of the \\(n\\) nodes could be adjacent to each of the other \\(n-1\\) nodes. Therefore the number of edges is bounded above by \\[\n    m \\le n(n-1).\n\\] When the graph is complete, every possible edge is present, i.e., every node is adjacent to every other node. Therefore, \\[\n    m = n(n-1)\n\\] if \\(G\\) is complete. On the other hand, \\(m &lt; n(n-1)\\) if \\(G\\) is not complete.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#dense-and-sparse-graphs",
    "href": "02_Introduction_to_Graphs.html#dense-and-sparse-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.3 Dense and Sparse Graphs",
    "text": "2.3 Dense and Sparse Graphs\n\nDefinition 2.4 (Dense/Sparse Graphs) A directed graph \\(G\\) is sparse if \\(m &lt;&lt; n(n-1)\\). Otherwise \\(G\\) is dense.\n\n\nExample 2.4 Figure 2.4 provides visualisations of three graphs with increasing density.\n\nA sparse graph with \\(5\\%\\) of possible edges present (Figure 2.4 (a)).\nA dense graph with \\(50\\%\\) of possible edges present (Figure 2.4 (b)).\nA very dense graph with \\(95\\%\\) of possible edges present (Figure 2.4 (c)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sparse graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Dense graph\n\n\n\n\n\n\n\n\n\n\n\n(c) Very dense graph\n\n\n\n\n\n\n\nFigure 2.4: Three graphs with \\(n=25\\) with increasing density.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#paths-and-connectivity",
    "href": "02_Introduction_to_Graphs.html#paths-and-connectivity",
    "title": "2  Introduction to Graphs",
    "section": "2.4 Paths and Connectivity",
    "text": "2.4 Paths and Connectivity\nWe can extend our definition of adjacency to describe pairwise relationships of nodes via sequences of arcs.\n\nDefinition 2.5 (Path) A path is a sequence of arcs \\[(i_1, i_2), (i_2, i_3), \\dots, (i_k, i_{k + 1})\\] with \\(k+1 \\ge 2\\) nodes with distinct origin \\(i_1\\) and destination \\(i_{k+1}\\).\n\nWe can also use the notation \\[\n    (i_1, i_2, \\dots, i_k, i_{k+1})\n\\] to denote a \\((i_1, i_{k+1})\\)-path.\n\nExample 2.5 The graph visualised in Figure 2.5 contains several paths from \\(2\\) to \\(3\\). For example, both \\(P_1 = ((2,4), (4,1), (1,3))\\) and \\(P_2 = ((2,4), (4,1), (1,5), (5,3))\\) are \\(23\\)-paths.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Path \\(P_1 = ((2,4), (4,1), (1,3))\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Path \\(P_2 = ((2,4), (4,1), (1,5), (5,3))\\)\n\n\n\n\n\n\n\nFigure 2.5: Two \\(23\\)-paths, \\(P_1\\), \\(P_2\\), in graph \\(G\\).\n\n\n\n\nDefinition 2.6 (Connectivity) Node \\(v\\) is connected to node \\(w\\) if there is path in \\(G\\) with origin \\(i_1 = v\\) and destination \\(i_{k+1} = w\\).\n\n\nDefinition 2.7 A graph is connected if every pair of nodes is connected.\n\n\nExample 2.6 Node \\(2\\) is connected to Node \\(3\\) in the graph given in Example 2.5. Indeed, we saw that there are at least two \\(23\\)-paths in Example 2.5.\nMoreover, the graph \\(G\\) is connected because we every pair of nodes is connected via an arc. Indeed, we can use the subpaths of the paths \\(P_1\\), \\(P_2\\) and the \\(34\\)-path \\[\n    P_3 = ((3,2), (2,4))\n\\]\nto reach every node from every other node. For example, we can use parts of \\(P_2\\) and \\(P_3\\) to find the \\(14\\)-path \\[\n    P_{14} = ((1,5), (5,3), (3,2), (3,4)).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(P_3 = ((3,2), (2,4))\\)\n\n\n\n\n\n\n\nFigure 2.6: A \\(34\\)-path \\(P_3\\) in \\(G\\). This, along with \\(P_1, P_2\\) from Example 2.5 provides paths between every pair of nodes in \\(G\\); hence, \\(G\\) is connected.\n\n\n\n\n2.4.1 Cycles\nIf a path begins and ends at the same node, then we call it a cycle.\n\nDefinition 2.8 (Cycles) A cycle is a sequence \\[(i_1, i_2), (i_2, i_3), \\dots, (i_k, i_{k + 1})\\] of \\(k \\ge 2\\) consecutive and distinct arcs with \\(i_{k+1} = i_1\\) (i.e., the origin and destination coincide).\n\nNote that a cycle may visit some nodes more than once (other than the origin/destination).\n\nExample 2.7 The graph \\(G\\) given in Figure 2.7 contains the cycles \\(C_1 = (2,5,4,3,2)\\) and \\(C_2 = (1,3,5,4,2,1)\\). Note that \\(C_2\\) visits node \\(2\\) twice. This implies that we also have cycles \\(C_3 = (1,3,2,1)\\) and \\(C_4 = (2,5,4,2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cycle \\(C_1 = (2,5,4,3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Cycle \\(C_2 = (1,3,5,4,2,1)\\)\n\n\n\n\n\n\n\nFigure 2.7: A graph \\(G\\) containing cycles \\(C_1\\) and \\(C_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#directed-cuts",
    "href": "02_Introduction_to_Graphs.html#directed-cuts",
    "title": "2  Introduction to Graphs",
    "section": "2.5 Directed Cuts",
    "text": "2.5 Directed Cuts\nHaving introduced notions of connectivity, we now define sets of arcs whose removal disconnect the graph.\n\nDefinition 2.9 (Forward Cut) Let \\(S \\subseteq V\\), that is, \\(S\\) is a subset of the node set \\(V\\).\nThe set of arcs with tail in \\(S\\) and head in \\(V\\setminus S\\) is the forward directed cut induced by \\(S\\): \\[\\delta^+(S) := \\{(i,j) \\in A: i \\in S \\text{ and } j \\in V\\setminus S\\}.\\]\n\nInformally, the forward directed cut is the set of arcs that leave \\(S\\).\n\nDefinition 2.10 (Backward Cut) The backward directed cut induced by \\(S\\) is the set of arcs with tail in \\(V\\setminus S\\) and head in \\(S\\):\n\\[\\delta^-(S) := \\delta^+(V\\setminus S) = \\{(i,j) \\in A: i \\in V\\setminus S  \\text{ and } j \\in S \\}.\\]\n\nThe backward directed cut is the set of arcs entering \\(S\\).\n\nExample 2.8 Consider \\(S = \\{4, 5\\}\\) for the graph given in Figure 2.8 (a). This graph and set of nodes has forward and backward cuts \\[\n    \\delta^+(\\{4,5\\}) = \\{(4,2), (4,3), (5,1)\\}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\delta^+(\\{4,5\\}) = \\{(4,2), (4,3), (5,1)\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(\\delta^-(\\{4,5\\}) = \\{(2,5\\}\\)\n\n\n\n\n\n\n\nFigure 2.8: Forward and backward cuts of \\(S = \\{4,5\\}\\) is graph \\(G\\).\n\n\n\n\n2.5.1 Stars and Degrees\n\nDefinition 2.11 (Stars) Let \\(i \\in V\\). The forward and backward stars of \\(i\\) are the cuts \\[\\delta^+(\\{i\\}) \\text{ and } \\delta^-(\\{i\\})\\] respectively.\n\n\nDefinition 2.12 (Degree) The out-degree and in-degree of \\(i\\) are the number of edges with \\(i\\) as tail and head, respectively: \\[|\\delta^+(\\{i\\})| \\text{ and } |\\delta^-(\\{i\\})|.\\]\n\n\nExample 2.9 Consider \\(v = 3\\), i.e., \\(S = \\{v\\} = \\{3\\}\\), in the graph \\(G\\) considered in Example 2.8:\n\nThe forward star is \\(\\delta^+(\\{3\\}) = \\{(3,2)\\}\\).\n\n\n\n\n\n\n\n\n\n\nForward star \\(\\delta^+(\\{3\\}) = \\{(3,2)\\}\\)\n\n\n\n\n\n\n\nBackward star \\(\\delta^-(\\{3\\}) = \\{(1,3), (4,3), (5,3)\\}\\)\n\n\n\n\n\n\nFigure 2.9: Forward and backward star of \\(\\{3\\}\\) in \\(G\\). Note that \\(\\{3\\}\\) has out-degree \\(1\\) and in-degree \\(3\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#undirected-graphs",
    "href": "02_Introduction_to_Graphs.html#undirected-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.6 Undirected Graphs",
    "text": "2.6 Undirected Graphs\n\nDefinition 2.13 (Undirected Graphs) An undirected graph is an ordered pair \\(G := (V, E)\\) composed of\n\na set of vertices \\(V\\) \\((n = |V|)\\)\na set \\(E \\in V\n        \\times V\\) of unordered pairs called edges \\((m = |E|)\\).\n\nFor an edge \\(\\{i,j\\}\\), we call \\(i\\) and \\(j\\) its endpoints.\n\n\nExample 2.10 Consider \\(G=(V,E)\\) with \\[\n\\begin{aligned}\n    V &= \\{1,2,3,4,5\\} \\\\\nE &= \\{12, 13, 15, 23, 24, 25, 34, 35,45\\}.\n\\end{aligned}\n\\] Figure 2.10 gives a visualisation of this graph.\n\n\n\n\n\n\n\nFigure 2.10: Visualisation of graph \\(G\\) given in Example 2.10",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#properties-of-undirected-graphs",
    "href": "02_Introduction_to_Graphs.html#properties-of-undirected-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.7 Properties of Undirected Graphs",
    "text": "2.7 Properties of Undirected Graphs\n\n2.7.1 Adjacency\nProperties ofdirected graphs generalize to undirected graphs with minor differences:\n\n\\(ij \\in E\\) is incident to both \\(i\\) and \\(j\\).\nIf \\(ij \\in E\\) then \\(i\\) and \\(j\\) are (mutually) adjacent.\nIf there is a path from \\(i\\) to \\(j\\) then \\(i\\) and \\(j\\) are (mutually) connected.\n\\(G = (V,E)\\) is complete if each pair of vertices in \\(V\\) is adjacent: \\[E = \\{\\{i,j\\}: i, j \\in V,~i\\neq j\\}.\\]\n\n\nTheorem 2.2 Every undirected graph \\(G\\) has \\(|E| \\le n(n-1)/2\\) edges.\n\n\nExample 2.11 Consider the undirected graph \\(G\\) given in Figure 2.11:\n\n\\(0\\) and \\(3\\) are adjacent because \\(\\{0,3\\} \\in E\\).\n\\(0\\) and \\(2\\) are not adjacent because \\(\\{0,2\\} \\notin E\\).\n\\(1\\) and \\(2\\) are connected. Indeed, \\(G\\) contains \\(12\\)-paths \\((1,4,3,2)\\) and \\((1,0,3,2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(0\\) and \\(3\\) are adjacent\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(0\\) and \\(2\\) are not adjacent\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(12\\)-path \\((1,4,3,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(12\\)-path \\((1,0,3,2)\\)\n\n\n\n\n\n\n\nFigure 2.11: Adjacency and connectivity properties of graph \\(G\\).\n\n\n\n\n\n2.7.2 Cuts, Stars, and Degree in Undirected Graphs\n\nDefinition 2.14 (Undirected Cuts) The undirected cut induced by \\(S \\subseteq V\\) is the set of edges with one end point in \\(S\\) and one in \\(V\\setminus S\\): \\[\\delta(S) := \\{ ij \\in E: i \\in S,~j\\notin S\\}.\\]\n\n\nDefinition 2.15 (Stars) The star of node \\(i \\in V\\) is the set \\(\\delta(\\{i\\})\\).\n\n\nDefinition 2.16 (Degree) The degree of \\(i \\in V\\) is the cardinality of its star: \\(|\\delta(\\{i\\})|\\).\n\n\nExample 2.12 Consider the graph given in Figure 2.12:\n\nThe cut for \\(S = \\{1,2,5\\}\\) is \\[\n  \\delta(S) = \\{01, 16, 02, 23, 24, 53, 54, 56\\}\n\\]\nThe star for node \\(3\\) is \\[\n  \\delta(\\{3\\}) = \\{23, 53, 63\\}.\n\\]\nNode \\(3\\) has degree \\(3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Undirected graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Undirected cut \\(\\delta(S)\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Undirected star \\(\\delta(\\{3\\})\\)\n\n\n\n\n\n\n\nFigure 2.12: Undirected cut and star in an undirected graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "02_Introduction_to_Graphs.html#adjacency-lists-and-matrices",
    "href": "02_Introduction_to_Graphs.html#adjacency-lists-and-matrices",
    "title": "2  Introduction to Graphs",
    "section": "2.8 Adjacency Lists and Matrices",
    "text": "2.8 Adjacency Lists and Matrices\n\nDefinition 2.17 (Adjacency Lists) An adjacency list \\(L\\) is a list of size \\(n\\) where each component \\(L(i)\\) is a list of nodes adjacent to \\(i\\):\n\n\\(L(i) = \\{j: (i,j) \\in \\delta^+(\\{i\\})\\}\\) for directed graphs;\n\\(L(i) = \\{j: \\{i,j\\} \\in \\delta(\\{i\\})\\}\\) for undirected graphs.\n\n\n\nDefinition 2.18 (Adjacency Matrices) The adjacency matrix \\(A = A(G)\\) of \\(G = (V, E)\\) is a binary matrix \\(A \\in \\{0,1\\}^{n\\times n}\\) such that \\[\na_{ij} =\n\\begin{cases} 1, & \\text{if } (i,j) \\in E, \\\\ \\\\\n    0, & (i,j) \\notin E.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A directed graph\n\n\n\n\n\n\n\n\n\n\n\n(b) An undirected graph\n\n\n\n\n\n\n\nFigure 2.13: Graphs for examples of adjacency lists.\n\n\n\n\nExample 2.13 Consider the directed graph given in Figure 2.13 (a).\nThis graph has the adjacency lists \\[\n\\begin{aligned}\n    L(0) &= \\{1,3\\},  &&& L(1) &= \\{3\\}, \\\\\n    L(2) &= \\{0,1\\}, &&& L(3) &= \\{0\\}.\n\\end{aligned}\n\\] Moreover, the adjacency matrix of this graph is \\[\nA =\n\\begin{pmatrix}\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix}.\n\\]\n\n\nExample 2.14 Consider the directed graph given in Figure 2.13 (b).\nThis graph has adjacency lists \\[\n\\begin{aligned}\n    L(0) &= \\{1\\}, &&& L(1) &= \\{0,3\\},  &&& L(2) &= \\{4\\}\\\\\n    L(3) &= \\{1\\}, &&& L(4) &= \\{2\\}.\n\\end{aligned}\n\\] The adjacency matrix is \\[\nA =\n\\begin{pmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0\n\\end{pmatrix}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Graphs</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html",
    "href": "03_Reachability.html",
    "title": "3  Graph Reachability",
    "section": "",
    "text": "3.1 The Graph Reachability Problem",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#the-graph-reachability-problem",
    "href": "03_Reachability.html#the-graph-reachability-problem",
    "title": "3  Graph Reachability",
    "section": "",
    "text": "Definition 3.1 (The Graph Reachability Problem) Given a directed graph \\(G = (V,A)\\) and a node \\(s\\). Find the set \\(M\\) of all nodes that are reachable from \\(s:\\)\n\ni.e., all nodes that are connected to \\(s\\).\n\n\n\nExample 3.1 Consider the graph \\(G\\) given in Figure 2 (a). The set of reachable nodes from \\(s=1\\) is \\(M = \\{1,2,4,5\\}\\) (see Figure 2 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Directed graph \\(G\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M = \\{1,2,4,5\\}\\) for \\(s=1\\).\n\n\n\n\n\n\n\nFigure 3.1: Graph \\(G\\) and reachable set \\(M\\) from node \\(s=1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#algorithm-idea",
    "href": "03_Reachability.html#algorithm-idea",
    "title": "3  Graph Reachability",
    "section": "3.2 Algorithm Idea",
    "text": "3.2 Algorithm Idea\nThe following steps give an intuitive process for finding reachable nodes from a given node \\(s\\).\n\nExplore \\(s\\): start from \\(s\\) and follow the arcs in its forward star to find its neighbors.\nRepeat with each neighbor of neighbors of \\(s\\).\nRepeat with neighbor of neighbors of neighbors of \\(s\\), etc.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#illustration-of-idea",
    "href": "03_Reachability.html#illustration-of-idea",
    "title": "3  Graph Reachability",
    "section": "3.3 Illustration of Idea",
    "text": "3.3 Illustration of Idea\nLet’s apply this idea for the graph \\(G\\) given in Figure 2 (a) with source node \\(s = 1\\).\n\n3.3.1 Step 1\nThe forward star of \\(s\\) is \\[\n    \\delta^+(\\{1\\}) = \\{(1,2), (1,4)\\}.\n\\] This implies that \\(2\\) and \\(4\\) are both reachable from \\(1\\). That is, \\(M\\) contains \\(\\{1,2,4\\}\\). See Figure 2 (a).\n\n\n3.3.2 Step 2\nWe need to explore further from nodes \\(2\\) and \\(4\\). Let’s start with \\(2\\). The only node adjacent to \\(2\\) is node \\(5\\). We can conclude that \\(M\\) contains \\(\\{1,2,4,5\\}\\). See Figure 2 (b).\n\n\n3.3.3 Step 3\nLet’s explore from node \\(4\\): \\(2\\) is the only node adjacent to \\(4\\). Our partially computed reachable set \\(M\\) is unchanged. See Figure 2 (c).\n\n\n3.3.4 Step 4\nWe haven’t explored from node \\(5\\) yet. Let’s do that now. The only arc incident with \\(5\\) is \\((5,4)\\). Thus, \\(4\\) is adjacent to \\(5\\) and, hence, reachable from \\(1\\). We already knew that \\(4\\) is in \\(M\\), so \\(M\\) is unchanged. See Figure 2 (d).\n\n\n3.3.5 Step 5\nWe have a choice of nodes to explore from: \\(\\{2, 4, 5\\}\\). We have already explored each of these nodes and appear to be stuck in a loop. Can we conclude that \\(M = \\{1,2,4,5\\}\\)?\n\n\n\n\n\n\n\n\n\n\n\n(a) Step 1: \\(M \\supseteq \\{1,2,4\\}\\) so far\n\n\n\n\n\n\n\n\n\n\n\n(b) Step 2: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Step 3: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) Step 4: \\(M \\supseteq \\{1,2,4,5\\}\\)\n\n\n\n\n\n\n\nFigure 3.2: Steps of the intuitive reachability process. We need to revise the algorithm to decide when to terminate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#avoiding-cycles",
    "href": "03_Reachability.html#avoiding-cycles",
    "title": "3  Graph Reachability",
    "section": "3.4 Avoiding Cycles",
    "text": "3.4 Avoiding Cycles\n\n3.4.1 Goal and Notation\nWe need to avoid exploring nodes we have already explored.\nLet’s define \\(M\\) as the set of nodes we have reached and explored.\n\nWhen we reach a node, we can check if it is already in \\(M\\).\nIf it is, we’ve explored it already and shouldn’t again.\n\nLet’s introduce another set \\(Q\\) as a queue of nodes which have been reached but not explored.\n\n\n3.4.2 Algorithm Idea\nInitialize \\(Q\\) as \\(Q = \\{s\\}\\). Each iteration:\n\nChoose vertex \\(i \\in Q\\) to explore.\nAdd to \\(Q\\) any neighbors of \\(i\\) that are not already in \\(Q\\) or \\(M\\).\nAdd \\(i\\) to \\(M\\) since it has been explored.\n\nIf \\(Q = \\{\\} = \\emptyset\\) (empty set), then stop:\n\nWe’ve explored all nodes reachable from \\(s\\).\n\\(M\\) is the set of nodes reachable from \\(s\\).\n\n\n\n3.4.3 Graph Reachability Algorithm (Pseudocode)\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "03_Reachability.html#returning-to-the-example",
    "href": "03_Reachability.html#returning-to-the-example",
    "title": "3  Graph Reachability",
    "section": "3.5 Returning to the Example",
    "text": "3.5 Returning to the Example\nLet’s consider the graph \\(G\\) given in Figure 2 (a) and apply the algorithm to find the set of nodes reachable from \\(s=1\\).\n\n3.5.1 Iteration 1\nWe initialize \\(Q\\) and \\(M\\) as \\[\n    Q = \\{1\\}, \\hspace{0.25in}\n    M = \\emptyset.\n\\] Let’s explore node \\(1\\):\n\nNode 1 has adjacency list \\(L(1) = \\{2,4\\}\\).\nLet’s add {2,4} to \\(Q\\).\nSince we have explored node \\(1\\), we remove \\(1\\) from \\(Q\\) and add \\(1\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.3: Iteration 1: Explore node \\(1\\)\n\n\n\n\n\n3.5.2 Iteration 2\nAfter the first iteration, we have\n\\[\n    Q = \\{2, 4\\}, \\hspace{0.25in}\n    M = \\{1\\}.\n\\]\nWe can continue from node \\(2\\) or node \\(4\\). Let’s explore node \\(2\\):\n\n\\(L(2) = \\{5\\}\\).\nAdd \\(5\\) to \\(Q\\).\nRemove \\(2\\) from \\(Q\\) and add \\(2\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.4: Iteration 2: Explore node \\(2\\)\n\n\n\n\n\n3.5.3 Iteration 3\nWe now have \\[\n    Q = \\{4, 5\\}, \\hspace{0.25in}\n    M = \\{1, 2\\}.\n\\]\nLet’s explore node \\(4\\):\n\n\\(L(4) = \\{2\\}\\).\nWe already have \\(2 \\in M\\); we do not add \\(2\\) to \\(Q\\).\nWe remove \\(4\\) from \\(Q\\) and add \\(4\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.5: Iteration 3: Explore node \\(4\\)\n\n\n\n\n\n3.5.4 Iteration 4\nAfter the first three iterations, we have \\[\n    Q = \\{5\\}, \\hspace{0.25in}\n    M = \\{1, 2, 4\\}.\n\\]\nExploring node \\(5\\), we note:\n\n\\(L(5) = 4\\).\nSince \\(4 \\in M\\) already, we move \\(5\\) to \\(M\\).\n\n\n\n\n\n\n\nFigure 3.6: Iteration 4: Explore node \\(5\\)\n\n\n\n\n\n3.5.5 Iteration 5 – Termination\nAt this stage \\(Q\\) is empty, so we cannot proceed. We conclude that \\(M = \\{1,2,4,5\\}\\)!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graph Reachability</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html",
    "href": "04_Complexity.html",
    "title": "4  Complexity",
    "section": "",
    "text": "4.1 Big-\\(O\\) Notation\nBig-\\(O\\) notation captures the asymptotic behaviour of a function \\(f\\) when compared to another function \\(g\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#big-o-notation",
    "href": "04_Complexity.html#big-o-notation",
    "title": "4  Complexity",
    "section": "",
    "text": "Definition 4.1 (Big-\\(O\\) Notation) Given \\(f,g:\\mathbf{R}\\mapsto\\mathbf{R}\\), we say \\[f = O(g),\\] i.e., \\(f\\) is of order \\(g\\), if there are \\(n_0\\) and \\(c\\in \\mathbf{R}_+\\) such that \\[f(n) \\le c~g(n) \\hspace{0.15in} \\text{for all } n \\ge n_0.\\]\n\n\n\n4.1.1 Properties of Big-\\(O\\) Notation\nLet \\(f\\) and \\(g\\) be positive functions \\((f,g: \\mathbf{R}\\mapsto\\mathbf{R}_+)\\).\n\nIf \\(\\displaystyle \\lim_{n\\rightarrow \\infty}\n\\frac{f(n)}{g(n)}= \\ell \\in (0, \\infty)\\) then \\[\nf = O(g) \\text{  and  } g = O(f).\n\\] For example, \\(f(n) = 2n\\) and \\(g = 3n + \\sqrt{n}\\) satisfy \\(f=O(g)\\) and \\(g=O(f)\\).\nIf \\(\\displaystyle\\lim_{n\\rightarrow \\infty} \\frac{f(n)}{g(n)} = 0\\) then \\[\nf = O(g)\n        \\text{ but }\n        g \\neq O(f).\n\\] For example, \\(f(n)=n\\) and \\(g(n) = 3n^3 + 2n\\) satisfy \\(f=O(g)\\), but \\(g \\neq O(f)\\).\nMultiplicative and additive constants can be ignored: \\[\na f(n) + b = O(f(n)).\n\\] For example, \\(f(n) = 5n^2 + 2 = O(n^2)\\).\nLower order terms can be ignored: if \\(g = O(f)\\) then \\[\nf(n) + g(n) = O(f(n)).\n\\] For example, \\(6n^4 + e^n = O(e^n)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#elementary-operations-and-computational-complexity",
    "href": "04_Complexity.html#elementary-operations-and-computational-complexity",
    "title": "4  Complexity",
    "section": "4.2 Elementary Operations and Computational Complexity",
    "text": "4.2 Elementary Operations and Computational Complexity\n\n4.2.1 Computational Complexity\nWe can count the number of elementary operations or EOs carried out by an algorithm:\n\nArithmetic operations\nAccesses to memory\nWriting operations, etc.\n\nLet \\(f_A(n)\\) and \\(f_B(n)\\) be the number of EOs required in worst case (the instance that requires the most EOs) by algorithms \\(A\\) and \\(B\\) to solve \\(P\\) with instance size \\(n\\).\n\nDefinition 4.2 We call \\(O(f_A)\\) and \\(O(f_B)\\) the computational complexity of \\(A\\) and \\(B\\). We choose \\(O(f_A)\\) and \\(O(f_B)\\) to be as small as possible.\n\n\n\n4.2.2 The Cobham-Edmonds Thesis (1965)\nWe say that problem \\(P\\) is tractable or well-solved if:\n\nThere is an algorithm \\(A\\) for solving \\(P\\);\n\\(A\\) has polynomial computational complexity.\n\n\n\n4.2.3 Converting EOs and Run-Time\nIf each EO takes \\(1\\) \\(\\mu s\\) (1 microsecond):\n\n\n\n\\(n\\)\n\\(f_A(n) = n^2\\)\n\\(f_A(n) = 2^n\\)\n\n\n\n\n\\(10\\)\n\\(0.1~\\text{ms}\\)\n\\(1~\\text{ms}\\)\n\n\n\\(20\\)\n\\(0.4~\\text{ms}\\)\n\\(1.0~\\text{s}\\)\n\n\n\\(30\\)\n\\(0.9~\\text{ms}\\)\n\\(17.9\\text{ min}\\)\n\n\n\\(40\\)\n\\(1.6~\\text{ms}\\)\n\\(12.7\\text{ days}\\)\n\n\n\\(50\\)\n\\(2.5~\\text{ms}\\)\n\\(35.7\\text{ years}\\)\n\n\n\\(60\\)\n\\(3.6~\\text{ms}\\)\n\\(366\\text{ centuries}\\)\n\n\n\n\n\n4.2.4 A Comparison of Complexity and Scaling\nAssume again that each EO requires \\(1~\\mu\\text{s}\\).\nThe following figure compares how complexity scales as a function of \\(n\\) for\n\nlogarithmic,\npolynomial,\npolylogarithmic, and\nexponential functions.\n\n\n\n\n\n\n\nFigure 4.1: Scaling of functions of \\(n\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "04_Complexity.html#examples",
    "href": "04_Complexity.html#examples",
    "title": "4  Complexity",
    "section": "4.3 Examples",
    "text": "4.3 Examples\n\n4.3.1 Complexity of Solving Quadratic Equations\nRecall the following algorithm for solving quadratic equation \\(ax^2 + bx + c = 0\\):\nCompute u = b*b - 4*a*c\n\nCompute v = 2*a\n\nCompute w = sqrt(u)\n\nCompute x_plus = -(w - b)/v\n\nCompute x_minus = - (w + b)/v\nLet’s count the number of operations used in each step of the algorithm:\n\nComputing u requires 3 products and one subtraction (4 arithmetic operations total).\nComputing v requires 1 product.\nComputing w requires 1 call to the sqrt function, which uses a finite number of arithmetic operations. Let’s treat this as 1 operation for the purposes of estimating the complexity of applying the quadratic formula.\nEach of x_plus and x_minus require 3 arithmetic operations.\n\nThe total number of arithmetic operations used by the quadratic formula is \\(12 = O(1)\\).\n\n\n4.3.2 Complexity of Graph Reachability\nLet’s analyse the complexity of the graph reachability algorithm given below:\nInitialize Q = {s} and M = {}. \n\nwhile Q != {}:\n\n    # select a node i in Q\n    Q = Q - {i} # remove i from Q.\n    \n    for j in L(i): # j is adjacent to i.\n    \n        if (j not in M) and (j not in Q):\n            Q = Q + {j} # add j to Q.\n            \n    M = M + {i} # add i to M.\nLet’s first count the number of operations need for each iteration of the algorithm:\n\nChoosing a node i in Q, removing i from Q, and adding i to M requires \\(O(1)\\) operations each.\nThe loop iterating over the adjacency list \\(L(i)\\) runs \\(|L(i)| \\le n-1\\) times. Each occurence of the loop code uses \\(O(1)\\) operations.\nWe perform \\(|Q| \\le |V| = n\\) steps of the outer-most loop.\n\nTherefore, we have the upper bound on complexity \\[\n    O(n(n-1)) = O(|E|).\n\\]\n\n4.3.2.1 An Improved Estimate\nThe steps of the inner for loop is executed for each node \\(i\\) at most once. This implies that each arc \\((i,j)\\) is explored at most once. Therefore, the steps of this loop are executed at most \\(m\\) times total.\nThis implies that the total complexity is \\[\n    O(m) + O(n) = O(m+n).\n\\] This is much smaller than the previous estimate if the graph is sparse, i.e., \\(O(m) &lt;&lt; O(n(n-1))\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Complexity</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html",
    "href": "05a_Shortest_Paths.html",
    "title": "5  The Shortest Path Problem",
    "section": "",
    "text": "5.1 Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#preliminaries",
    "href": "05a_Shortest_Paths.html#preliminaries",
    "title": "5  The Shortest Path Problem",
    "section": "",
    "text": "Definition 5.1 (The Shortest Path Problem – Unrestricted Lengths (SPP-U)) Given a directed graph \\(G = (V,A)\\) with:\n\ntwo nodes \\(s\\) and \\(t\\);\nlength function \\(\\ell:A\n    \\mapsto \\mathbf{R}\\) (unrestricted in sign).\n\nFind an \\((s,t)\\)-path with minimum total length.\n\n\nExample 5.1 Consider the graph given in Figure 5.1 (a). The shortest \\((1,5)\\)-path in this graph is \\((1,2,3,5)\\) with value equal to \\[\n    \\ell_{12} + \\ell_{23} + \\ell_{35} = 4 -6 -2 = -4.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A directed graph \\(G\\) with edge lengths \\(\\ell\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) The shortest \\((1,5)\\)-path\n\n\n\n\n\n\n\nFigure 5.1: The shortest \\((1,5)\\)-path in \\(G\\) is \\((1,2,3,5)\\) with length \\(-4\\).\n\n\n\n\n5.1.1 Applications\n\n5.1.1.1 Logistics and Transportation\nIn this field, we want to find \\((s,t)\\)-paths minimizing:\n\nTravel time; \\(\\ell_{ij}\\) is average time traveling along arc \\((i,j)\\).\nFuel consumption.\nLikelihood of delays; etc.\n\n\n\n5.1.1.2 Minimum Cardinality Path\nWe can find a path with minimum number of arcs by assigning \\(\\ell_{ij} = 1\\) for all \\((i,j)\\in A\\).\n\n\n5.1.1.3 Component of Other Algorithm\nSPP appears as a subproblem when solving other combinatorial optimization problems (more details later).\n\n\n\n5.1.2 Simple Paths and Cycles\n\nDefinition 5.2 An \\((s,t)\\)-path \\(P\\) is simple if it does not visit the same node twice.\n\nNote: if \\(P\\) visits the same node twice then \\(P\\) must contain at least one cycle.\n\nExample 5.2 Figure 5.2 gives an example of a simple path and a not simple path within a directed graph.\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A simple path\n\n\n\n\n\n\n\n\n\n\n\n(c) A not-simple path\n\n\n\n\n\n\n\nFigure 5.2: Simple and not simple \\((4,5)\\)-paths in directed graph \\(G\\).\n\n\n\n\n\n\n5.1.3 Unboundedness and Negative Cycles\n\nTheorem 5.1 Suppose that \\(G\\) contains an \\((s,t)\\)-path \\(P\\) with a negative length cycle. Then SPP-U is unbounded.\n\n\nProof. Suppose that \\(C\\) is a negative length with length \\[\n    \\ell_{C} = \\sum_{ij \\in C} \\ell_{ij} &lt; 0.\n\\]\nSuppose further that \\(P\\) is an \\((s,t)\\)-path that traverses \\(C\\) exactly \\(k\\) times for some integer \\(k&gt; 0\\). That is, \\(C\\) is contained in \\(P\\) exactly \\(k\\) times.\nWe can always augment this path to get a path with strictly smaller length. Indeed, consider the path \\(\\tilde P\\) which contains all arcs of \\(P\\), but traverses \\(C\\) exactly \\(k+1\\) times. Then the length of \\(\\tilde P\\) satisfies \\[\n    \\text{length }\\tilde P = \\text{length } P + \\ell_C\n    &lt; \\text{length } P\n\\] since \\(\\ell_C &lt; 0\\).\n\n\nTheorem 5.2 If no \\((s,t)\\)-path contains a negative-length cycle, then \\(G\\) admits a simple shortest \\((s,t)\\)-path.\n\nIf no \\((s,t)\\)-path in \\(G\\) contains a negative length cycle, then we can solve the SPP-U by restricting our search to simple paths.\n\nProof. Let \\(P\\) be a shortest \\((s,t)\\)-path in \\(G\\). Let’s also assume that every cycle \\(C\\) in \\(P\\) has nonnegative length \\(\\ell_C \\ge 0\\).\nLet’s assume that \\(P\\) contains a cycle \\(C\\) starting and ending with node \\(u\\). That is, we can think of \\(P\\) as the union of the directed \\((s,u)\\)-path \\(P_{su}\\), the cycle \\(C\\), and the \\((u,t)\\)-path \\(P_{ut}\\). Let \\(\\ell^*\\) denote the value of this path.\nNow consider the \\((s,t)\\)-path given by following \\(P_{su}\\) to \\(u\\), then \\(P_{ut}\\) to \\(t\\). This is the path obtained by removing \\(C\\) from \\(P\\). This gives an \\((s,t)\\)-path with length \\[\n    \\ell^* - \\ell_C \\le \\ell^*.\n\\] Thus, removing cycle \\(C\\) from \\(P\\) does not increase the length of the path. We can repeat this process until we have obtained a simple path with minimum path length \\(\\ell^*\\), or we obtain a simple path with length strictly less than \\(\\ell^*\\) (a contradiction).\n\n\nExample 5.3 To illustrate this phenomena, consider the graph given in Figure 5.3. This graph has \\((1,5)\\)-path \\((1,2,3,4, 2,5)\\) with length \\(5\\) (assuming all arcs have length \\(1\\)). However, this path contains the cycle \\(C= (2,3,4,2)\\). Removing the cycle \\(C\\) gives the shorter \\((1,5)\\)-path \\((1,2,5)\\) (with length \\(2\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Path of length \\(5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Cycle \\(C\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) Path with cycle removed\n\n\n\n\n\n\n\nFigure 5.3: Illustration of the cycle removal process to obtain a shorter path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "05a_Shortest_Paths.html#the-bellman-ford-algorithm",
    "href": "05a_Shortest_Paths.html#the-bellman-ford-algorithm",
    "title": "5  The Shortest Path Problem",
    "section": "5.2 The Bellman-Ford Algorithm",
    "text": "5.2 The Bellman-Ford Algorithm\n\n5.2.1 Single Source Shortest Path Problem – Unrestricted Lengths (SSPP-U)\n\nDefinition 5.3 Given a directed graph \\(G = (V,A)\\) with length function \\(\\ell:A     \\mapsto \\mathbf{R}\\) (unrestricted in sign).\nThe Single Source Shortest Path Problem (SSPP-U) aims to find an \\((s,t)\\)-path with minimum total length for every node \\(t\\) in \\(V\\setminus\\{s\\}\\).\n\n\nExample 5.4 Figure 5.4 gives the shortest \\((1,t)\\)-paths in the graph given in Figure 5.4 (a) for \\(t = 2,3,4,5,6\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Shortest \\((1,2)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(c) Shortest \\((1,3)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Shortest \\((1,4)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(e) Shortest \\((1,5)\\)-path\n\n\n\n\n\n\n\n\n\n\n\n(f) Shortest \\((1,6)\\)-path\n\n\n\n\n\n\n\nFigure 5.4: Shortest \\((1,t)\\)-paths in graph \\(G\\).\n\n\n\n\n\n5.2.2 Subpath Optimality\nThe following theorem characterizes a very useful properties of shortest paths: any subpath of a shortest path is itself a shortest path.\n\nTheorem 5.3 Let \\(P = (s,i_2), (i_2, i_3), \\dots, (i_{k-1}, t)\\) be a shortest \\((s,t)\\)-path.\nConsider pair of nodes \\(i_u, i_v\\) visited by \\(P\\) with \\(u &lt; v\\).\nThen the subpath from \\(i_u\\) to \\(i_v\\) is a shortest \\((i_u,i_v)\\) path.\n\n\nProof. Suppose, on the contrary, that the subpath \\(S\\) from \\(i_u\\) to \\(i_v\\) in a shortest \\((s,t)\\)-path is not the shortest \\((i_u, i_v)\\)-path. In particular, suppose that \\(T\\) is the shortest \\((i_u, i_v)\\)-path.\nWe can construct another \\((s,t)\\)-path \\(tilde P\\) using \\(P\\) and \\(T\\). Indeed, let \\(\\tilde P = P \\setminus S \\cup T\\) be the path obtained by removing \\(S\\) from \\(P\\) and adding \\(T\\). Then \\(\\tilde P\\) is also a \\((s,t)\\)-path. The length of \\(\\tilde P\\) is equal to \\[\n    |P| - |S| + |T| &lt; |P| - |S| + |S| = |P|\n\\] since \\(|T| &lt; |S|\\). This implies that \\(\\tilde P\\) is shorter than \\(P\\); this is a contradiction. Therefore, \\(S\\) must be the shortest \\((i_u, i_v)\\)-path.\n\n\n\n5.2.3 Shortest Paths of Fixed Length\n\nDefinition 5.4 For all \\(i\\in V\\), we define \\(f_k(i)\\) as the length of a shortest \\((s,i)\\)-path containing at most k arcs,\n\nWe set \\(f_k(i) = \\infty\\) if there is no \\((s,i)\\)-path with length at most \\(k\\).\n\nLemma 5.1 If \\(k = n-1\\), then \\(f_{n-1}(i)\\) is the length of a shortest \\((s,i)\\)-path (without restriction on the number of arcs).\n\n\nProof. We can ignore paths that aren’t simple. On the other hand, simple paths contain at most \\(n\\) nodes. Indeed, a simple path does not contain a loop and, hence, visits at most \\(n\\) nodes. Therefore, the shortest \\((s,i)\\)-path contains at most \\(n-1\\) arcs.\n\n\n\n5.2.4 The Bellman-Ford Theorem\nThe following theorem is the basis for our algorithm for calculating shortest paths.\n\nTheorem 5.4 \\(f_k(i)\\) can be computed recursively as \\[\nf_k(i) = \\min \\left\\{\n        f_{k-1}(i),\n        \\;\n        \\min_{(j,i) \\in \\delta^-(i)}\n        \\Big\\{ f_{k-1}(j) + \\ell_{ji} \\Big\\}\n        \\right\\}.\n\\]\n\n\nProof. Assume that \\(f_{k-1}(i)\\) has been computed for all \\(i \\in V\\setminus\\{s\\}\\). The shortest \\((s,i)\\)-path with at most \\(k\\) arcs \n\nis the shortest \\((s,i)\\)-path with at most \\(k-1\\) arcs; or\ncontains one more arc than the shortest \\((s,i)\\)-path with at most \\(k-1\\) arcs.\n\nIn the second case, we can decompose the shortest \\((s,i)\\)-path as:\n\nshortest \\((s,j)\\)-path with \\(k-1\\) arcs for some node \\(j\\); and\narc \\((j,i)\\).\n\nWe choose node \\(j\\) so that \\(j\\) gives \\[\n    \\min_{(q,i) \\in \\delta^-(i)} f_{k-1}(q) + \\ell_{qi}.\n\\]\n\n\n\n5.2.5 The Bellman-Ford Algorithm\n\n5.2.5.1 Setup\n\nDefinition 5.5 Let \\(P_k(i)\\) be the predecessor of \\(i\\) in the shortest \\((s,i)\\)-path with at most \\(k\\) arcs found by the algorithm (so far).\n\nWe will maintain two tables:\n\nOne encoding \\(f_k(i)\\) for each \\(i\\) and \\(k\\);\nThe other encoding \\(P_k(i)\\) for all \\(i\\) and \\(k\\).\n\n\n\n5.2.5.2 Updates\nThe algorithm calculates \\(f_k(i)\\) and \\(P_k(i)\\) for all \\(i \\in V\\) recursively from \\(k=1\\) to \\(k=n-1\\).\nWe’ll update \\(f_k(i)\\) from \\(f_{k-1}(i)\\) by:\n\nInitially setting \\(f_k(i) = f_{k-1}(i)\\);\nScanning all arcs \\((j,i)\\) in \\(\\delta^{-}(i)\\) and update \\(f_k(i)\\) if necessary.\n\n\n\n5.2.5.3 The Bellman-Ford Algorithm (Pseudocode)\nf_0(s) = 0 and P_0(s) = ~ \nfor i in V \\{s}:\n    f_0(i) = +inf and  P_0(i) = ~\n\n# Calculate shortest paths of length k.\nfor k in {1, 2, ..., n-1}:\n   \n    # Update shortest si-path.\n    for i in V:\n        f_k(i) = f_{k-1}(i) \n        P_k(i) = P_{k-1}(i)\n        \n        # Check each edge incident at i.\n        for (j,i) in delta^{-}(i):            \n            # Update if length-k path is shorter than k-1.\n            if f_{k-1}(j) + l_{ji} &lt; f_k(i)\n                f_k(i) = f_{k-1}(j) + l_{ji} \n                P_k(i) = j\n\n\n\n5.2.6 Example\n\n5.2.6.1 Iteration 1 (\\(k=1\\))\nNote that nodes \\(2\\) and \\(6\\) are adjacent to \\(1\\). Thus, they are reachable from \\(1\\) by a path with length at most \\(1\\). The only paths from \\(1\\) to another node are the single arcs \\((1,2)\\) and \\((1,6)\\), of lengths \\(4\\) and \\(3\\) respectively. Figure 5.5 highlights these paths, and we update the tables Table 5.1 and Table 5.2 accordingly.\n\n\n\n\n\n\nFigure 5.5: Shortest Paths of length \\(k=1\\)\n\n\n\n\n\n\nTable 5.1: Lengths of shortest paths of length at most \\(1\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\n\n\n\n\n\n\nTable 5.2: Predecessors in shortest paths of length at most \\(1\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\n\n\n\n\n\n5.2.6.2 Iteration 2 (\\(k=2\\))\nNodes \\(3\\) and \\(4\\) can both be reached from \\(1\\) using a path of length \\(2\\) (with lengths \\(-2\\) and \\(1\\) respectively). Similarly, we have path \\((1,2,6)\\) of length \\(2\\): \\[\n    \\ell_{12} + \\ell_{26} = f_1(2) + \\ell_{26} = 4 - 2 = 2 &lt; f_1(6) = 3.\n\\]\nThis implies that the path \\((1,2,6)\\) is the shortest \\((1,6)\\)-path with at most \\(2\\) arcs. We update the shortest path and predecessor tables below.\n\n\n\n\n\n\nFigure 5.6: Shortest Paths of length at most \\(k=2\\)\n\n\n\n\n\n\nTable 5.3: Lengths of shortest paths of length at most \\(2\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\n\n\n\n\n\n\nTable 5.4: Predecessors in shortest paths of length at most \\(2\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\n\n\n\n\n\n5.2.6.3 Iteration 3 (\\(k=3\\))\nNode \\(5\\) is reachable from \\(1\\) by the path \\((1,2,3,5)\\) using \\(3\\) arcs; this path has length \\(-4\\). On the other hand, every other \\((s,i)\\)-path consisting of \\(3\\) arcs is longer than the shortest \\((s,i)\\)-path of length at most \\(2\\).\n\n\n\n\n\n\nFigure 5.7: Shortest Paths of length at most \\(k=3\\)\n\n\n\n\n\n\nTable 5.5: Lengths of shortest paths of length at most \\(3\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\\(f_3\\)\n0\n4\n-2\n1\n-4\n2\n\n\n\n\n\n\n\n\n\nTable 5.6: Predecessors in shortest paths of length at most \\(3\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\\(P_3\\)\n\\(\\sim\\)\n1\n2\n2\n3\n2\n\n\n\n\n\n\n\n\n5.2.6.4 Iteration 4 (\\(k=4\\))\nNote that we have the \\((1,4)\\)-path \\((1,2,3,5,4)\\) containing \\(k=4\\) arcs. This path has length \\[\n    f_3(5) + \\ell_{54} = -4 + 4 = 0 &lt; f_3(4) = 1 = f_2(4).\n\\] We set \\(f_4(4) = 0\\) and \\(P_4(4) = 5\\).\nAfter checking all other paths containing \\(k=4\\) arcs, we do not change the list of shortest paths.\n\n\n\n\n\n\nFigure 5.8: Shortest Paths of length at most \\(k=4\\)\n\n\n\n\n\n\nTable 5.7: Lengths of shortest paths of length at most \\(4\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(f_0\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n3\n\n\n\\(f_2\\)\n0\n4\n-2\n1\n\\(\\infty\\)\n2\n\n\n\\(f_3\\)\n0\n4\n-2\n1\n-4\n2\n\n\n\\(f_4\\)\n0\n4\n-2\n0\n-4\n2\n\n\n\n\n\n\n\n\n\nTable 5.8: Predecessors in shortest paths of length at most \\(4\\).\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_0\\)\n0\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_0\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n1\n\n\n\\(P_2\\)\n\\(\\sim\\)\n1\n2\n2\n\\(\\sim\\)\n2\n\n\n\\(P_3\\)\n\\(\\sim\\)\n1\n2\n2\n3\n2\n\n\n\\(P_4\\)\n\\(\\sim\\)\n1\n2\n5\n3\n2\n\n\n\n\n\n\n\n\n5.2.6.5 Termination\nThe shortest path using at most \\(4\\) arcs are also shortest paths using at most \\(5\\) arcs for this graph. Since \\(k=5 = n-1\\), we have found the shortest paths of any length by @lem.\n\n\n\n5.2.7 Complexity of the Bellman-Ford Algorithm\nLet’s analyses how many operations are needed by the Bellman-Ford Algorithm.\nf_0(s) = 0 and P_0(s) = ~ \nfor i in V \\{s}:\n    f_0(i) = +inf and  P_0(i) = ~\n\n# Calculate shortest paths of length k.\nfor k in {1, 2, ..., n-1}:\n   \n    # Update shortest si-path.\n    for i in V:\n        f_k(i) = f_{k-1}(i) \n        P_k(i) = P_{k-1}(i)\n        \n        # Check each edge incident at i.\n        for (j,i) \\in delta^{-}(i):            \n            # Update if length-k path is shorter than k-1.\n            if f_{k-1}(j) + l_{ji} &lt; f_k(i)\n                f_k(i) = f_{k-1}(j) + l_{ji} \n                P_k(i) = j\nInitiailization (Lines 1-3) requires \\(O(n)\\) elementary operations.\nThe loop from Line 9 to Line 18 is repeated for each \\(i \\in V\\) (\\(O(n)\\) times) and consists of the following steps for each \\(i\\):\n\nInitializing \\(f_k(i) = f_{k-1}(i)\\) and \\(P_k(i) = P_{k-1}(i)\\) (requires \\(O(1)\\) operations).\nChecking if each arc \\((j,i)\\) with tail \\(i\\) yields a shorter path with \\(k\\) arcs:\n\nComparing \\(f_{k-1}(j) + \\ell_{ji}\\) with \\(f_k(i)\\) requires \\(O(1)\\) operations for each \\(j\\).\nWe have \\(|\\delta^-(i)| = O(n)\\) of these arcs.\n\n\nIt follows that the loop from Lines 9–18 requires \\[\n    O(n) \\Big( O(1) + O(n) \\Big) = O(n^2)\n\\] for each \\(k \\in \\{1,2,\\dots, n-1\\}\\). Taking the total over all \\(k\\), the Bellman-Ford Algorithm requires \\(O(n^3)\\) elementary operations.\n\n5.2.7.1 Complexity in Terms of Arcs\nEach arc \\((j,i) \\in A\\) is considered exactly once for each \\(k \\in \\{1,2,\\dots, n-1\\}\\) in the loop from Lines 9–18. This implies that the steps of this loop require \\(O(|A| + n) = O(m + n)\\) elementary operations. This implies that the total complexity of the Bellman-Ford Algorithm require \\[\n    O(nm + n^2)\n\\] elementary operations, which is much smaller than \\(O(n^3)\\) if \\(m &lt;&lt; n^2\\).\n\n\n\n5.2.8 Detecting Negative Length Cycles\nRecall: SPP-U (and SSPP-U) are unbounded if the graph contains a negative length cycle. Indeed, we can endlessly apply rounds of update operations and reduce the shortest path lengths \\(f_k(i)\\) indefinitely.\nWe can detect whether the problem is unbounded by running one more iteration of the algorithm beyond iteration \\(k = n-1\\).\n\nIf the graph contains a negative-length cycle then an update will take place.\nThis indicates the problem is unbounded and we can stop the algorithm.\n\n\n5.2.8.1 Example\nConsider the graph given in Figure 5.9. This graph has cycle \\(C = (2,3,4,2)\\) with length \\(-4 &lt; 0\\).\n\n\n\n\n\n\nFigure 5.9: Graph with negative cycle\n\n\n\nLet’s apply the Bellman-Ford algorithm to find the shortest \\((1,t)\\)-paths in this graph.\n\n5.2.8.1.1 Iteration 1 (\\(k=1\\))\nThe only node adjacent to \\(1\\) is \\(2\\). Thus, the\n\n\n\n\n\n\nFigure 5.10: Shortest Paths of length at most \\(k=1\\)\n\n\n\n\n\n\nTable 5.9: Length of shortest paths of length at most \\(1\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n\n\n\n\n\n\nTable 5.10: Predecessors in shortest paths of length at most \\(1\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\n\n\n\n\n\n5.2.8.1.2 Iteration 2 (\\(k=2\\))\nNext, we can reach \\(3\\) from \\(1\\) via a path consisting of \\(2\\) arcs. This path has length \\(-2\\).\n\n\n\n\n\n\nFigure 5.11: Shortest Paths of length at most \\(k=2\\)\n\n\n\n\n\n\nTable 5.11: Length of shortest paths of length at most \\(2\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\n\n\n\n\n\n\nTable 5.12: Predecessors in shortest paths of length at most \\(2\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\n\n\n\n\n\n5.2.8.1.3 Iteration 3 (\\(k=3\\))\nNode \\(4\\) is the only node that is reachable from \\(1\\) by a path containing \\(3\\) arcs; the path \\((1,2,3,4)\\) has length \\(3\\).\n\n\n\n\n\n\nFigure 5.12: Shortest Paths of length \\(k=3\\)\n\n\n\n\n\n\nTable 5.13: Length of shortest paths of length at most \\(3\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 5.14: Predecessors in shortest paths of length at most \\(3\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\n\n\n\n\n\n5.2.8.1.4 Iteration 4 (\\(k=4\\))\nLet’s try one more iteration!\nNote that \\[\n    f_3(4) + \\ell_{42} = 3 - 3 = 0 &lt; f_2(2).\n\\]\n\n\n\n\n\n\nFigure 5.13: Shortest Paths of length \\(k=4\\)\n\n\n\n\n\n\nTable 5.15: Length of shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(f_0(i)\\)\n0\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_1(i)\\)\n0\n4\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(f_2(i)\\)\n0\n4\n-2\n\\(\\infty\\)\n\n\n\\(f_3(i)\\)\n0\n4\n-2\n3\n\n\n\\(f_4(i)\\)\n0\n0\n-2\n3\n\n\n\n\n\n\n\n\n\nTable 5.16: Predecessors in shortest paths of length at most \\(4\\) in graph with negative cycle.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n\\(P_0(i)\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_1(i)\\)\n\\(\\sim\\)\n1\n\\(\\sim\\)\n\\(\\sim\\)\n\n\n\\(P_2(i)\\)\n\\(\\sim\\)\n1\n2\n\\(\\sim\\)\n\n\n\\(P_3(i)\\)\n\\(\\sim\\)\n1\n2\n3\n\n\n\\(P_4(i)\\)\n\\(\\sim\\)\n4\n2\n3\n\n\n\n\n\n\nThis implies that the path \\((1,2,3,4,2)\\) with \\(5\\) arcs has smaller total length than the path \\((1,2)\\). This contradicts Lemma 5.1 and, hence, implies that the shortest path problem is unbounded for this graph. Indeed, further iterations will lead further decrease in \\(f_k(i)\\) for \\(i\\) in the negative length cycle \\((2,3,4,2)\\). We can stop the Bellman-Ford Algorithm after \\(k=n\\) iterations and declare the problem instance unbounded.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Shortest Path Problem</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html",
    "href": "06_Minimum_Cost_Spanning_Trees.html",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "",
    "text": "6.1 Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#preliminaries",
    "href": "06_Minimum_Cost_Spanning_Trees.html#preliminaries",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "",
    "text": "6.1.1 Subgraphs\n\nDefinition 6.1 Let \\(G = (V,E)\\) be an undirected graph.\nWe call \\(G_S = (V_S, E_S)\\) a subgraph of \\(G\\) if\n\n\\(G_S\\) is a graph;\n\\(V_S \\subseteq V\\) and \\(E_S \\subseteq E\\).\n\n\n\nExample 6.1 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The graph given in Figure 6.2 (b) is a subgraph of \\(G\\). However, the graph given in Figure 6.2 (c) is not a subgraph of \\(G\\) since \\(\\{1,4\\} \\notin E\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A subgraph\n\n\n\n\n\n\n\n\n\n\n\n(c) Not a subgraph\n\n\n\n\n\n\n\nFigure 6.1: A subgraph of \\(G\\) and a graph that is not a subgraph of \\(G\\).\n\n\n\n\n\n6.1.2 Trees\n\nDefinition 6.2 Let \\(G = (V,E)\\) be an undirected graph.\nThe subgraph \\(G_T = (V_T, E_T)\\) is a tree if\n\n\\(G_T\\) is a connected;\n\\(G_T\\) is acyclic, i.e., contains no cycles.\n\nA tree \\(G_T\\) is a spanning tree if \\(V_T = V\\) (\\(G_T\\) spans/reaches all nodes in \\(V\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) A Tree\n\n\n\n\n\n\n\n\n\n\n\n(c) A Spanning Tree\n\n\n\n\n\n\n\nFigure 6.2: Trees in given graph \\(G\\).\n\n\n\n\nExample 6.2 Consider the graph \\(G = (V,E)\\) given in Figure 6.2 (a). The subgraph given in Figure 6.2 (b) is a connected and acyclic; therefore, it is a tree. On the otherhand, the subgraph given in Figure 6.2 (c) is a tree and has node set \\(V\\); therefore, it is a spanning tree.\n\n\n\n6.1.3 Motivating Example\nWe want to build a new high-speed network at the University:\n\nshould connect all buildings; while\ncosting as little as possible.\n\nWe can model this problem as that of finding a minimum cost spanning tree.\nConsider the graph \\(G\\) with:\n\nBuildings as vertices;\nPotential connections as edges.\n\nWe want to find a connected, acyclic subgraph to minimize cost while ensuring all buildings are connected. Such a tree is highlighted in red in Figure 6.3.\n\n\n\n\n\n\nFigure 6.3: Campus map with minimum spanning tree\n\n\n\n\n\n6.1.4 The Minimum Cost Spanning Tree (MST) Problem\n\nDefinition 6.3 Given an undirected graph \\(G = (V,E)\\) and cost function \\(c: E\\mapsto \\mathbf{R}\\).\nThe minimum cost spanning tree problem aims to find a spanning tree \\(G_T = (V_T, E_T)\\) of minimum total cost: \\[\\min~ c(E_T) = \\sum_{e \\in E_T} c_e.\\]\n\n\nTheorem 6.1 (Cayley – 1889) A complete undirected graph with \\(n\\) nodes contains \\(n^{n-2}\\) spanning trees.\n\nNoncomplete graphs contain fewer spanning trees, but the number is still exponentially large in \\(n\\). This implies that we cannot solve MST using complete enumeration for even small graphs.\n\n\n6.1.5 Leaves\n\nDefinition 6.4 The nodes of a graph with degree 1 are called leaves.\n\n\nTheorem 6.2 A tree contains at least one leaf.\n\n\nProof. Suppose, on the contrary, that tree \\(T\\) does not contain a leaf. Then every node in \\(T\\) has degree at least \\(2\\). This implies that \\(T\\) has a cycle; a contradiction.\n\n\n\n\n\n\n\nFigure 6.4: A tree with leaves ${1, 2, 3, 6}\n\n\n\n\n\n6.1.6 The Number of Edges of A Tree\n\nTheorem 6.3 A tree \\(G_T\\) with \\(n\\) nodes has \\(m = n-1\\) edges.\n\n\nProof. We’ll prove Theorem 6.3 by induction.\nAs a base case, consider \\(n=1\\). This tree consists of a single node and \\(0 = n-1\\) edges.\nNow suppose that every tree with \\(k\\) nodes has \\(n-1\\) edges. Consider tree \\(T\\) with \\(k+1\\) nodes. We want to show that \\(T\\) has \\(k\\) edges.\nTo do so, note that \\(T\\) contains at least one leaf by Theorem 6.2. Without loss of generality, let’s assume that node \\(1\\) is a leaf; if not, we can relabel vertices so that \\(1\\) is a leaf. Let’s remove leaf \\(1\\) and its incident edge (there is only one such edge because \\(1\\) has degree-\\(1\\)).\nAfter deleting this node and edge, we have a connected, acyclic graph with \\(k\\) nodes. By the inductive hypothesis, this tree has \\(k-1\\) edges. Since we deleted \\(1\\) node and \\(1\\) edge, we can conclude that \\(T\\) had \\(k\\) edges. This completes the proof.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\) with leaf \\(1\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) After removing \\(\\{1,5\\}\\)\n\n\n\n\n\n\n\nFigure 6.5: Illustration of the pruning process. After removing node \\(1\\) and edge \\(\\{1,5\\}\\), we are left with a tree with \\(5\\) nodes.\n\n\n\n\n\n6.1.7 The Swap Property\n\nLemma 6.1 Given tree \\(G_T = (V_T, E_T)\\), removing an edge \\(e \\in E_T\\) creates two subtrees.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A tree\n\n\n\n\n\n\n\n\n\n\n\n(b) A subtree\n\n\n\n\n\n\n\n\n\n\n\n(c) Another subtree\n\n\n\n\n\n\n\nFigure 6.6: Subtrees obtained after deleting edge \\(\\{4,5\\}\\).\n\n\n\n\n\n6.1.8 Swapping Edges Within a Cut\n\nLemma 6.2 Let \\(S\\) be the vertex set of one of the two subtrees.\nFor every edge \\(f \\in \\delta(S)\\) other than \\(e\\), the set \\[\nE_T' := E_T \\cup \\{f\\} \\setminus \\{e\\}\n\\] is the edge set of a tree spanning the same set of nodes.\n\n\nExample 6.3 Consider the graph \\(G\\) with tree \\(T\\) given by Figure 6.7 (a). The example in Figure 6.6 illustrates that we obtain two subtrees after removing edge \\(e=45\\).\nThe set \\(S = \\{1,2,3, 5\\}\\) is the vertex set of one of these trees. The cut induced by \\(S\\) is \\[\n    \\delta(S) = \\{02, 16, 24\\}.\n\\] Lemma 6.2 implies that exchanging \\(02\\) with \\(45\\) yields a new tree \\(\\tilde{T}\\) with edge set \\[\nE_{\\tilde T} = E_T \\setminus \\{4,5\\} \\cup \\{0,2\\}\n    = \\{15, 25, 35, 46, 02\\}.\n\\] See Figure 6.7 for an illustration of this process.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Tree \\(T\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Tree \\(\\tilde T\\) after exchange\n\n\n\n\n\n\n\nFigure 6.7: Example of swapping edges \\(02\\) and \\(45\\) using cuts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#the-jarnik-prim-dijkstra-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.2 The Jarnik-Prim-Dijkstra Algorithm",
    "text": "6.2 The Jarnik-Prim-Dijkstra Algorithm\n\n6.2.1 Jarnik’s Theorem\n\nTheorem 6.4 (Jarnik’s Theorem)  \n\nLet \\(F\\) be the edge set of a tree strictly contained in an MST.\nLet \\(S\\subseteq V\\) be the set of nodes it spans.\n\nFor every edge \\(e \\in \\delta(S)\\):\n\n\\(F \\cup \\{e\\}\\) is part of a MST if and only if \\(e\\) has minimum cost in \\(\\delta(S)\\).\n\n\n\nProof. We start by proving the “only if” part. Let’s suppose that \\(F \\cup \\{e\\}\\) is part of a MST. We want to show that \\(e\\) has minimum cost in \\(\\delta(S)\\) in this case. We’ll use a proof by contradiction. Let’s assume that there is \\(f\\in \\delta(S)\\) with \\(c_f &lt; c_e\\).\nLet \\(T = (V, E_T)\\) be a minimum spanning tree such that \\(F \\subseteq E_T\\) and \\(F \\cup \\{e\\} \\subseteq E_T\\). By the Swap Property (Lemma 6.1) the subgraph \\[\n\\tilde T = (V, E_T\\setminus\\{e\\} \\cup \\{f\\})\n\\] is also a spanning tree. Moreover, the cost of \\(\\tilde T\\) satisfies \\[\n    c(\\tilde T) = c(T) - c_e + c_f &lt; c(T)\n\\] since \\(c_e &gt; c_f\\).\nThis implies that \\(T\\) is not a minimum cost spanning tree; a contradiction. Therefore, we can conclude that if \\(F \\cup \\{e\\}\\) is part of a minimum spanning tree then \\(c_e \\le c_f\\) for all \\(f \\in \\delta(S)\\).\nThis is illustrated in Figure 6.8. Consider the spanning tree \\(T\\) given in Figure 6.8 (a) and the edge set \\(F = \\{12\\} \\subseteq E_T\\); here, \\(S = \\{1,2\\}\\). Note that \\(e = 15\\) is in the both \\(E_T\\) and \\(\\delta(S)\\), while \\(f=23\\) belongs to \\(\\delta(S)\\), but not \\(E_T\\). The swap property implies that we can obtain a spanning tree \\(\\tilde T\\) by exchanging the edges \\(e\\) and \\(f\\). If \\(c_f &lt; c_e\\) then \\(\\tilde T\\) is a spanning tree with strictly lower cost than \\(T\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(T\\), \\(S = \\{1,2\\}\\), and \\(e=15\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\delta(S)\\) and \\(f=23\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(\\tilde T\\) and \\(f=23\\)\n\n\n\n\n\n\n\nFigure 6.8: Graph with spanning tree \\(T\\) containing \\(F = \\{12\\}\\) and edge \\(e = 15\\) and edge \\(f = 23\\). If \\(c_e &gt; c_f\\), then \\(\\tilde T\\) is a spanning tree is smaller cost than \\(T\\).\n\n\n\nWe next prove the “if” part. That is, we prove that if \\(e\\) has minimum cost in \\(\\delta(S)\\) then \\(F \\cup \\{e\\}\\) is part of a minimum spanning tree. To do so, suppose that \\(c_e \\le c_f\\) for all \\(f \\in \\delta(S)\\). We want to show that \\(F \\cup \\{e\\}\\) is part of a MST.\nWe consider two cases. First, suppose that \\(e \\in E_T\\), where \\(T\\) is the MST containing \\(f\\). This immediately implies that \\(e\\) belongs to a MST and we’re done.\nNext, suppose that \\(e \\notin E_T\\). Specifically, let \\(e = uv\\) for nodes \\(u, v \\in V\\) such that \\(e \\notin E_T\\) and \\(e \\in \\delta(S)\\); we assume that \\(u \\in S\\) and \\(v \\in V \\setminus S\\).\nLet \\(P\\) be the \\((u,v)\\)-path joining the end points of \\(e\\) belonging to \\(T\\). Such a path always exists because \\(T\\) is a spanning tree. Since \\(P\\) starts in \\(S\\) at \\(u\\) and ends in \\(V\\setminus S\\) at \\(v\\) there is at least one edge \\(f\\) in both \\(P\\) and \\(\\delta(S)\\), i.e., there is edge \\(f \\in P \\cap \\delta(S)\\).\nBy the swap property, the subgraph \\[\nT' = (V, E_T \\setminus \\{f\\} \\cup \\{e\\})\n\\] is a spanning tree. Moreover, \\[\nc(T) \\ge c(T') = c(T) - c_f + c_e \\le c(T)\n\\] since \\(T\\) is a minimum spanning tree and \\(c_e \\le c_f\\). This is only possible if \\(c_e = c_f\\) and \\(T'\\) is also a minimum spanning tree.\nTo illustrate this phenomena, consider the edge \\(e = 15\\) in Figure 6.9. This edge is not included in the minimum spanning tree \\(T\\). However, there is a path \\(P = (1,2,3,5)\\) in \\(T\\) containing the edge \\(f = 23 \\in \\delta(\\{1,2\\})\\). Applying the swap property and the assumption the \\(c_e \\le c_f\\) shows that \\(T'\\) is also a minimum cost spanning tree.\n\n\n\n\n\n\n\n\n\n\n\n(a) Minimum spanning tree \\(T\\) not including \\(e = 15\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) Path in \\(T\\) from endpoints of \\(e\\) containing \\(f=23\\).\n\n\n\n\n\n\n\n\n\n\n\n(c) Minimum spanning tree \\(T'\\) obtained by exchanging \\(e\\) and \\(f\\).\n\n\n\n\n\n\n\nFigure 6.9: Minimum spanning trees \\(T\\) and \\(T'\\) obtained by exchanging arcs \\(e\\) and \\(f\\) with minimum cost in \\(\\delta(\\{1,2\\})\\).\n\n\n\n\n\n\n6.2.2 The Jarnik-Prim-Dijkstra (JPD) Algorithm\nTheorem 6.4 suggests the following algorithm for identifying a minimum cost spanning tree in a given graph.\nInitialize E_T = {} and S = {1}.\n\nWhile |E_T| &lt; n-1:\n\n    Choose e = {v,w} in delta(S) of minimum cost c_e.  \n\n    Update E_T = E_T + {e} \n\n    Update S = S + {w}.  \n\n6.2.2.1 Complexity of the JPD Algorithm\nFor each step of the while loop, choosing \\(e\\) in Line 5$ costs \\(O(m)\\) operations to search over the set of edges in \\(\\delta(S)\\). The loop is repeated \\(O(n)\\) times, so the total complexity is \\(O(mn)\\) elementary operations. This may be as large as \\(O(n^3)\\) when the graph is very dense of as small as \\(O(n^2)\\) when the graph is very sparse but still connected.\n\n\n\n6.2.3 Example: MST for Flight Routing\nThe following graph \\(G = (V, E)\\) gives available/potential flight routes between 8 cities, with distances in miles. The MST of \\(G\\) gives an airline a means of servicing each city while minimizing travel distance and fuel costs.\n\n\n\n\n\n\nFigure 6.10: Potential flight routes\n\n\n\nLet’s apply the Jarnik-Prim-Dijsktra Algorithm to find the minimum cost spanning tree in \\(G\\).\n\nStep 1\nWe start with \\(S = \\{1\\}\\), which induces the cut \\(\\delta(S) = \\{14, 18\\}\\). Since \\[\nc_{14} = 355 &lt; 695 = c_{18},\n\\] we add \\(14\\) to \\(E_T\\) and add \\(\\{4\\}\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.11: Step 1 of the JPD Algorithm\n\n\n\n\n\nStep 2\nNext, \\(\\delta(S) = \\{18, 24, 34, 74\\}\\). We add \\(2\\) to \\(S\\) and \\(24\\) to \\(E_T\\) because \\(c_{24} = 74\\) is the minimum cost edge in \\(\\delta(S)\\).\n\n\n\n\n\n\nFigure 6.12: Step 2 of the JPD Algorithm\n\n\n\n\n\nStep 3\nNext, \\(c_{34} = 262\\) has minimum cost among edges in \\(\\delta(S) = \\{18, 34, 47, 27\\}\\). We add \\(34\\) to \\(E_T\\) and \\(3\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.13: Step 3 of the JPD Algorithm\n\n\n\n\n\nStep 4\nWe next add \\(37\\) to \\(E_T\\) and \\(7\\) to \\(S\\) because \\(c_{37} = 242\\) has minimum cost among edges in \\(\\delta(S) = \\{37, 47, 27, 18\\}\\).\n\n\n\n\n\n\nFigure 6.14: Step 4 of the JPD Algorithm\n\n\n\n\n\nStep 5\nWe next add \\(67\\) to \\(E_T\\) and \\(6\\) to \\(S\\) because \\(c_{67} = 83\\) has minimum cost among edges in \\(\\delta(S) = \\{18, 75, 76, 78\\}\\).\n\n\n\n\n\n\nFigure 6.15: Step 5 of the JPD Algorithm\n\n\n\n\n\nStep 6\nNext, \\(c_{78} = 151\\) is minimum cost among edges in \\(\\delta(S) = \\{18, 56, 57, 78\\}\\). Add \\(8\\) to \\(S\\) and \\(78\\) to \\(E_T\\).\n\n\n\n\n\n\nFigure 6.16: Step 6 of the JPD Algorithm\n\n\n\n\n\nStep 7\nFinally, \\(\\delta(S) = \\{56, 57\\}\\). We have \\[\nc_{56} = 230 &lt; 306 = c_{57},\n\\] so we add \\(56\\) to \\(E_T\\), \\(5\\) to \\(S\\).\n\n\n\n\n\n\nFigure 6.17: Step 7 of the JPD Algorithm\n\n\n\n\n\nTermination\nNote that \\(S = \\{V\\}\\) and \\(E_T\\) contains \\(n-1=7\\) edges. This implies that we have constructed the minimum spanning tree \\(T\\).\n\n\n\n\n\n\nFigure 6.18: Minimum spanning tree identified by the JPD Algorithm",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "06_Minimum_Cost_Spanning_Trees.html#improving-the-jpd-algorithm",
    "href": "06_Minimum_Cost_Spanning_Trees.html#improving-the-jpd-algorithm",
    "title": "6  Minimum Cost Spanning Trees",
    "section": "6.3 Improving the JPD Algorithm",
    "text": "6.3 Improving the JPD Algorithm\n\n6.3.1 Inefficiency of Searching over Edges\nProblem: The algorithm scans many edges that will never be selected.\nTo see why this may be the the case, consider the set \\(S = \\{1, 3, 5\\}\\) in the graph given in Figure 6.19.\n\n\n\n\n\n\nFigure 6.19: Graph \\(G\\) with \\(S = \\{1,3,5\\}\\)\n\n\n\nFor \\(S = \\{1,3,5\\}\\), we have \\(\\delta(S) = \\{21, 23, 25, 41, 45 \\}\\). The JPD Algorithm will add edge \\(45\\) with minimum cost to \\(E_T\\), but will consider every edge in \\(G\\). However, we know that the only candidates to be added to \\(S\\) are Nodes \\(2\\) and \\(4\\). Since \\(23\\) and \\(45\\) are the minimum cost edges incident with \\(2\\) and \\(4\\) respectively, we really only need to consider these two edges as candidates for \\(E_T\\).\nThis example implies that if we knew which edge incident with each node has minimum cost, then we can search among nodes, not edges. Since we usually have far fewer nodes than edges, this can lead to substantial improvement in computational cost.\n\n\n6.3.2 Node Labels\nTo facilitate this, let’s introduce two labels for each node \\(j \\in V \\setminus S\\):\n\n\\(C(j) := \\min_{i \\in S}~c_{ij}\\) (minimum cost of edge connecting \\(S\\) to \\(j\\));\n\\(P(j) := \\text{argmin}_{i \\in S}~c_{ij}\\) (endpoint in \\(S\\) of this minimum cost edge).\n\n\nExample 6.4 For the graph given in Figure 6.19, we have \\[\n\\begin{aligned}\n    C(2) &= 4, & C(4) &= 2  \\\\\n    P(2) &= 3, & P(3) &= 5.\n\\end{aligned}\n\\]\n\nIf we have \\(C\\) and \\(P\\), choosing the vertex \\(w\\) to add to minimum spanning tree should only require \\(O(n)\\) EOs to compare the values of \\(C\\) for the vertices in \\(V\\setminus S\\).\nFurther, we must update \\(C\\) and \\(P\\) whenever \\(S\\) is updated. To do so, we search the star of \\(w\\), which requires \\(O(n)\\) EOs.\nThe total complexity is \\(O(n)\\) per iteration and \\(O(n^2)\\) total. This can be much smaller than \\(O(mn)\\)!!\n\n\n6.3.3 Better Algorithm – Pseudocode\nThis suggests the following algorithm.\nLet S = {1}, E_T = {}, C(1) = 0.\n\n% Initialize costs.\nFor j in V - {1}\n    Let C(j) = c_{1j} and P(j) = 1\n    (assume c_{1j} = inf if 1j not in E).\n\nWhile |E_T| &lt; n-1\n\n    Find w in V \\ S minimizing C(w). \n\n    Let S = S + {w} and E_T = E_T + {P(w), w}.  \n\n    % Update costs\n    For j in V \\ S such that wj in delta(w)\n\n        If c_{wj} &lt; C(j): \n            Update C(j) = c_{wj} and P(j) = w.\n\n\n6.3.4 Another Example\nTo illustrate the application of the revised algorithm, let’s consider the graph \\(G\\) given in Figure 6.20.\n\n\n\n\n\n\nFigure 6.20: Graph \\(G\\)\n\n\n\n\nStep 1\nInitially, we have \\(S = \\{1\\}\\). Note that \\[\n\\begin{aligned}\n    C(2) &= c_{12} = 1 \\\\\n    C(3) &= c_{13} = 1 \\\\\n    C(4) &= + \\infty \\\\\n    C(5) &= c_{15} = 8.\n\\end{aligned}\n\\] We choose \\(w=2\\). We add \\(2\\) to \\(S\\) and \\(12\\) to \\(E_T\\). Note that we could also choose to add \\(3\\) to \\(S\\) and \\(13\\) to \\(E_T\\) since \\(c_{12} = c_{13} = 1\\).\nIt remains to update \\(C\\) and \\(P\\). Note that \\[\n\\begin{aligned}\n    C(3) &= c_{13} = 1 &lt; c_{23} = c_{w3} \\\\\n    c_{w5} &=c_{25} = 2 &lt; C(5).\n\\end{aligned}\n\\] Thus, we set \\(C(5) = 2\\) and \\(P(5) = 2\\) and leave \\(C(3)\\), \\(P(3)\\) unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.21: Step 1 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found so far.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\nStep 2\nWe have \\(S = \\{1,2\\}\\). Nodes \\(3\\) and \\(5\\) are adjacent to \\(1\\) and \\(2\\). Since \\[\nC(3) = 1 &lt; 2 = C(5),\n\\] we add \\(3\\) to \\(S\\); since \\(P(3)=1\\), we add \\(13\\) to \\(E_T\\). Finally, we update \\[\nC(4) = 5, P(4) = 3\n\\] since \\(4\\) is adjacent to \\(3\\); \\(C(5)\\) and \\(P(5)\\) are unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.22: Step 2 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after two steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n5\n3\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\nStep 3\nSince \\(C(5) &lt; C(4)\\), we add \\(5\\) to \\(S\\) and \\(25\\) to \\(E_T\\). Note that \\[\n    c_{45} = 4 &lt; C(4) = 5,\n\\] we set \\(C(4) = 4\\) and \\(P(4) = 5\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.23: Step 3 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after three steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n5\n3\n\n\n5\n2\n2\n\n\n\n\n\n\n\n\nStep 4\nThe only edge left to add \\(E_T\\) is \\(54\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.24: Step 4 of the revised JPD algorithm\n\n\n\n\n\n\nPredecessor and value of minimum cost \\((1,v)\\)-path found after four steps of revised JPD algorithm.\n\n\nv\nC(v)\nP(v)\n\n\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n4\n5\n\n\n5\n2\n2\n\n\n\n\n\n\nThis gives the minimum cost spanning tree \\(T\\) found in Figure 6.25.\n\n\n\n\n\n\nFigure 6.25: Mininum spanning tree in \\(G\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Cost Spanning Trees</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html",
    "href": "07_Network_Flows.html",
    "title": "7  Network Flows",
    "section": "",
    "text": "7.1 Model Formulation",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#model-formulation",
    "href": "07_Network_Flows.html#model-formulation",
    "title": "7  Network Flows",
    "section": "",
    "text": "7.1.1 Motivation\nWe want to figure out the maximum amount of some commodity can be transported from a certain node \\(s\\) to another point \\(t\\) of a network.\nThis can take the form of any of the following (among many other examples):\n\nOil, water, really any fluid in pipeline network;\nVehicle traffic in a roadway network;\nInformation in a communication network;\nSpread of disease in pandemic models.\n\n\n\n7.1.2 Standard Constraints\nRegardless of physical application/interpretation of the network, we have the following standard collection of constraints.\n\n7.1.2.1 Capacity constraints\nEach pipe cannot carry more than its capacity. - Let \\(x_{ij}\\) denote the amount of commodity carried from \\(i\\) to \\(j\\) and let \\(k_{ij}\\) denote the capacity of the pipe from \\(i\\) to \\(j\\). - Then we must have \\[\n    0 \\le x_{ij} \\le  k_{ij}.\n\\]\n\n\n7.1.2.2 Balance constraints\nThe commodity cannot enter or leave the network except at source \\(s\\) and sink \\(t\\).\n\nConsider node \\(h\\) (not \\(s\\) or \\(t\\)).\nThe flow of the commodity into \\(h\\) must be equal to the flow out of \\(h\\): \\[\n  \\sum_{(i,h) \\in \\delta^{-}(h)} x_{ih}  =\n  \\sum_{(h,j) \\in \\delta^{+}(h)} x_{hj}.\n\\]\n\n\n\n\n7.1.3 The Maximum Flow Problem\nGiven a directed graph \\(G = (V,A)\\), arc capacity function \\(k: A \\mapsto \\mathbf{R}_+\\), and nodes \\(s,t \\in V\\).\n\nDefinition 7.1 (Maximum Flow Problem) Find a function \\(x: A \\mapsto \\mathbf{R}_+\\) (called a flow) satisfying:\n\ncapacity constraints: \\[\n0 \\le x_{ij} \\le k_{ij}\n\\] for all \\((i,j) \\in A\\);\nbalance constraints: \\[\n\\sum_{(i,h) \\in \\delta^{-}(h)} x_{ih}  =\n\\sum_{(h,j) \\in \\delta^{+}(h)} x_{hj}\n\\] for all $h V {s,t},\nmaximizing the net flow value: \\[\n\\varphi(\\{s\\}) = \\varphi(x) =\n\\sum_{(s,j) \\in \\delta^{+}(s)} x_{sj} -\n\\sum_{(i,s) \\in \\delta^{-}(s)} x_{is}.\n\\]\n\n\n\n\n7.1.4 Example\n\nExample 7.1 Figure 7.1 gives an example of a flow \\(x\\) for the given graph \\(G\\) with capacity \\(k\\).\n\n\n\n\n\n\nFigure 7.1: Graph \\(G\\) with capacity \\(k\\) and flow \\(x\\). Arc \\(ij\\) has label \\(x_{ij}, k_{ij}\\)\n\n\n\nThe flow can be written as \\[\n\\begin{aligned}\nx_{s1} &= 3, &&& x_{s2}&=0 \\\\\nx_{12} &= 2, &&& x_{1t} &= 1 \\\\\nx_{21} &= 0, &&& x_{2t} &= 2.\n\\end{aligned}\n\\] This flow \\(x\\) has value \\(\\varphi(x) = 3.\\)\n\n\n\n7.1.5 Formulation as a Linear Program\nWe can write the maximum flow problem as the linear program or linear optimization problem1: \\[\n\\begin{array}{ll}\n\\max & \\varphi \\\\\n\\text{s.t.} & \\displaystyle\n\\sum_{(h,j) \\in \\delta^+(h)} x_{hj}\n    - \\sum_{(i,h) \\in \\delta^-(h)} x_{ih}=\n\\begin{cases}\n    + \\varphi, & \\text{if } h =s, \\\\\n    - \\varphi, & \\text{if } h = t,\\\\\n    0, &\\text{otherwise},\n\\end{cases} \\\\\n& 0 \\le x_{ij} \\le k_{ij} \\text{ for all } (i,j) \\in A,\n\\\\\n& \\varphi \\ge 0.\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#cuts-and-flow",
    "href": "07_Network_Flows.html#cuts-and-flow",
    "title": "7  Network Flows",
    "section": "7.2 Cuts and Flow",
    "text": "7.2 Cuts and Flow\n\n7.2.1 \\(st\\)-cuts\nGiven a subset of nodes \\(S \\subseteq V\\) with \\(s \\in S\\) and \\(t \\in V \\setminus S\\).\n\nDefinition 7.2 An st-cut is the set of arcs that enter or leave \\(S\\): \\[\n\\delta(S) = \\delta^+(S) \\cup \\delta^{-}(S).\n\\]\n\n\nDefinition 7.3 The (forward) capacity of the \\(st\\)-cut is \\[\nk(\\delta(S)) = \\sum_{(i,j) \\in \\delta^+(S)} k_{ij}.\n\\]\n\n\n\n7.2.2 Net Flow across a Cut\n\nDefinition 7.4 The net flow across \\(\\delta (S)\\) is \\[\n\\varphi(\\delta(S)) := \\sum_{(i,j)\\in \\delta^+(S)} x_{ij}\n    - \\sum_{(i,j) \\in \\delta^{-}(S)} x_{ij}.\n\\]\n\n\n\n7.2.3 Example\n\nExample 7.2 Consider the graph given in Figure 7.2.\n\n\n\n\n\n\nFigure 7.2: Directed graph with \\(S=\\{s,2\\}\\)\n\n\n\nThe forward and backward cuts induced by \\(S\\) are given in Figure 7.3.\n\n\n\n\n\n\n\n\n\n\n\n(a) Forward cut \\(\\delta^{+}(S) = \\{(s,1), (2,1), (2,t)\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Backward cut \\(\\delta^{-}(S) = \\{(1,2)\\}\\)\n\n\n\n\n\n\n\nFigure 7.3: Induced cuts in graph \\(G\\) given in Figure 7.2.\n\n\n\nThe \\(st\\)-cut induced by \\(S\\) is \\[\n    \\delta(S) = \\delta^{+}(S) \\cup \\delta^{-}(S)\n    = \\{(s,1), (2,1), (2,t), (1,2)\\}.\n\\] The forward capacity of this cut is \\[\n    k(\\delta(S)) = 7 + 1 + 5 = 13.\n\\] The value of this cut is \\[\n    \\varphi(\\delta(S)) = 3 + 0 + 2 - 2 = 3.\n\\]\n\n\n\n7.2.4 Conservation of Flow Across Cuts\n\nLemma 7.1 Given a flow \\(x\\) with net value \\(\\varphi\\), we have \\[\n\\varphi(\\delta(S)) = \\varphi\n\\] for every \\(S \\subseteq V\\) with \\(s \\in S\\), \\(t \\in V \\setminus S\\).\n\n\nConsider the balance constraint \\[\n\\sum_{(h,j) \\in \\delta^+(h)} x_{hj}\n    - \\sum_{(i,h) \\in \\delta^-(h)} x_{ih}=\n\\begin{cases}\n    + \\varphi, & \\text{if } h =s, \\\\\n    - \\varphi, & \\text{if } h = t,\\\\\n    0, &\\text{otherwise}.\n\\end{cases}\n\\] Let’s take the sum of both sides over all \\(h \\in S\\); note that \\(s \\in S\\) and \\(t \\notin S\\). First, the sum of the right-hand side over \\(h \\in S\\) is equal to \\(\\varphi\\).\nNext, each arc \\((i,j)\\) with \\(i,j \\in S\\) appears twice in the sum on the left-hand side: - When \\(h =i\\), we add \\(+x_{ij}\\) to the total. - When \\(h = j\\), we add \\(-x_{ij}\\) to the total. These two values cancel when we take the sum over all \\(h\\). On the other hand, if \\(i\\in S\\) and \\(j \\in V \\setminus S\\), then \\((i,j) \\in \\delta^{+}(S)\\) and appears once in the sum of the left-hand side (when \\(h=i\\)). Finally, if \\(i \\in V \\setminus S\\), \\(j \\in S\\) then \\((i,j) \\in \\delta^-(S)\\) and this edge contributes \\(-x_{ij}\\) to the sum in the left-hand side. Putting everything together, we have \\[\n\\sum_{(i,j)\\in\\delta^+(S)} x_{ij} - \\sum_{(i,j)\\in\\delta^-(S)} x_{ij} = \\varphi.\n\\] This completes the proof.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#duality",
    "href": "07_Network_Flows.html#duality",
    "title": "7  Network Flows",
    "section": "7.3 Duality",
    "text": "7.3 Duality\n\n7.3.1 Weak Duality\nThe following theorem gives an upper bound on the value of any flow in terms of the capacity of an arbitrary cut. Specifically, the value of any flow is at most the capacity of any \\(st\\)-cut.\n\nTheorem 7.1 For each \\(S \\subseteq V\\) with \\(s \\in S\\) and \\(t \\in V\\setminus S\\), we have \\[\n\\varphi(x) \\le k(\\delta(S))\n\\] for any flow \\(x\\).\n\n\nLet \\(x\\) be a feasible flow and consider \\(st\\)-cut \\(S\\). Then \\[\n\\sum_{(i,j)\\in\\delta^+(S)} x_{ij} \\le \\sum_{(i,j)\\in\\delta^+(S)} k_{ij}\n\\] since \\(x_{ij}\\) k_{ij}$ for any arc \\((i,j)\\). On the other hand, \\[\n- \\sum_{(i,j)\\in\\delta^-(S)} x_{ij} le 0,\n\\] since \\(x_{ij} \\ge 0\\) for any arc \\((i,j)\\). Applying conservation of flow, Lemma 7.1, we have \\[\n\\varphi = \\sum_{(i,j)\\in\\delta^+(S)} x_{ij}  - \\sum_{(i,j)\\in\\delta^-(S)} x_{ij}\n\\le \\sum_{(i,j)\\in\\delta^+(S)} k_{ij} = k(\\delta(S)).\n\\]\n\n\n\n7.3.2 The Minimum Cut Problem\nTheorem 7.1 holds for every \\(st\\)-cut. In particular, it is true for the cut with minimum capacity. Thus, for any feasible flow \\(x\\), we have \\[\n    \\varphi(x) \\le \\Big\\{ k(\\delta(S)): \\delta(S) \\text{if a $st$-cut}\\Big\\}.\n\\]\n::: {#def-7-min-cut-problem}\n\n7.3.2.1 Minimum Cut Problem\nGiven a directed graph \\(G = (V, A)\\) with arc capacity function \\(k: A \\mapsto \\mathbf{R}_+\\) and two nodes \\(s,t \\in V\\).\n\nExample 7.3 Consider the graph given in Figure 7.1. Find an \\(st\\)-cut of minimum capacity.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Minimum capacity cut \\(\\delta(\\{s\\})\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Minimum capacity cut \\(\\delta(\\{s\\})\\)\n\n\n\n\n\n\n\nFigure 7.4: Two minimum capacity cuts in Figure 7.1.\n\n\n\n\n\n\n7.3.3 Strong Duality: Max-Flow Min-Cut Theorem\nThe relationship \\[\n\\begin{aligned}\n    & \\varphi \\le k(\\delta(S)) \\text{ for all $st$-cuts} \\\\\n    & \\Leftrightarrow \\;\\;\\;\n    \\varphi \\le \\min_S \\Big\\{k(\\delta(S)): \\delta(S) \\text{ is a $st$-cut}\\Big\\}\n\\end{aligned}\n\\] holds for all net flow values \\(\\varphi\\).\nThis implies that for any flow value \\(\\varphi\\) we have \\[\n\\varphi \\le \\max \\Big\\{\\varphi(x): x \\text{ is a flow}\\Big\\}\n\\le\n\\Big\\{ k(\\delta(S)): \\delta(S) \\text{if a $st$-cut}\\Big\\}.\n\\]\nThe maximum flow value and minimum cut capacity are actually guaranteed to agree for every flow network. We have the following two results.\n\nLemma 7.2 Consider flow \\(x\\) of net value \\(\\varphi\\) and \\(st\\)-cut \\(\\delta(S)\\) of capacity \\(k(\\delta(S)).\\)\nIf \\(\\varphi = k(\\delta(S))\\), then \\(x\\) is a maximum flow and \\(\\delta(S)\\) is a minimum cut.\n\n\nTheorem 7.2 The value of a maximum flow is always equal to the capacity of a minimum \\(st\\)-cut.\n\nWe’ll construct an algorithmic proof: we’ll present an algorithm that reveals and constructs the flow and \\(st\\)-cut while solving the problem.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#the-ford-fulkerson-algorithm-idea",
    "href": "07_Network_Flows.html#the-ford-fulkerson-algorithm-idea",
    "title": "7  Network Flows",
    "section": "7.4 The Ford-Fulkerson Algorithm – Idea",
    "text": "7.4 The Ford-Fulkerson Algorithm – Idea\n\n7.4.1 Improving Flows along Forward and Backward Arcs\nIdea: Starting from given flow \\(x\\), we will try to find an \\(st\\)-path along which \\(\\varphi\\) can be increased.\nWe can increase flow using\n\nthe original arcs in the graph (forward arcs), and\narcs whose directions have been flipped (backward arcs).\n\n\nExample 7.4 Consider the graph given in Figure 7.5 (a) with flow \\(x\\) indicated.\nWe can increase the total flow by increasing by two units of flow along the forward arcs comprising the path \\((s2, 21, 1t)\\). This gives a new flow with value \\(\\varphi=3\\).\nSubsequently, we can move an additional \\(1\\) unit of flow by redirecting \\(1\\) unit of flow along arc \\(21\\) to flow along \\(2t\\) and pushing an additional unit of flow along \\(s1\\); see Figure 7.5 (b). This is equivalent to pushing \\(1\\) unit of flow along the path \\((s,1,2,t)\\), where \\(s1, 2t\\) are forward arcs and \\(12\\) is a backward arc; see Figure 7.5 (c).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Initial flow \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Increased flow along path \\((s,2,1,t)\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Increased flow along path \\((s,1,2,t)\\)\n\n\n\n\n\n\n\nFigure 7.5: Increasing flow using forward and backward arcs.\n\n\n\n\n\n7.4.2 Saturated and Empty Arcs\n\nDefinition 7.5 An arc \\((i,j)\\) is saturated if \\(x_{ij} = k_{ij}\\).\n\n\nDefinition 7.6 An arc \\((i,j)\\) is empty if \\(x_{ij} = 0\\).\n\nObservation: Our algorithm for finding the maximum flow and minimum capacity cut hinges on the following:\n\nIf \\((i,j)\\) is not saturated \\((x_{ij} &lt; k_{ij})\\) then \\(x_{ij}\\) can be increased.\nIf \\((i,j)\\) is not empty \\((x_{ij} &gt; 0)\\) then \\(x_{ij}\\) can be decreased.\n\n\nExample 7.5 Consider the graph \\(G\\) with flow \\(x\\) given in @#fig-7-saturated-ex-G.\n\nArcs \\(s2\\), \\(21\\), and \\(1t\\) are saturated;\nArc \\(2t\\) is empty;\nArc \\(s1\\) is not empty or saturated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\) with flow \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Saturated arcs \\(s2\\), \\(21\\), and $1t\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Empty arc \\(2t\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) Arc \\(s1\\) is neither.\n\n\n\n\n\n\n\nFigure 7.6: Saturated and empty arcs for flow \\(x\\) in the given graph.\n\n\n\n\n\n7.4.3 Augmenting Paths\n\nDefinition 7.7 (Augmenting Paths) We call a \\(st\\)-path using a combination of forward and backward arcs an augmenting path if:\n\nAll forward arcs are not saturated \\((x_{ij} &lt; k_{ij})\\); and\nAll backward arcs are not empty \\((x_{ij} &gt; 0)\\).\n\n\nWe can improve net flow value by increasing flow on forward arcs and decreasing flow on backward arcs.\n\nExample 7.6 The graph \\(G\\) and flow \\(x\\) indicated in Figure 7.7 has augmenting path \\(P=(s,1,2,t)\\).\n\n\n\n\n\n\n\nFigure 7.7: An augmenting path \\(P=(s,1,2,t)\\)\n\n\n\n\n\n7.4.4 The Auxiliary Network\nAt each step of our algorithm, we will try to find an augmenting path. We will construct an additional flow network to facilitate finding such an augmenting path.\nWe can construct an auxiliary network \\(G' = (V, A')\\) with auxiliary capacity function \\(k': A \\mapsto \\mathbf{R}_+\\) that accounts for all possible flow variations with respect to current flow \\(x\\).\nFor all \\((i,j) \\in A\\):\n\nIf \\(x_{ij} &lt; k_{ij}\\) (not saturated), we add forward arc \\((i,j)\\) to \\(A'\\) with \\(k'_{ij} = k_{ij} - x_{ij}\\).\n\nThis arc models the possibility of increasing the flow on \\((i,j)\\).\n\nIf \\(x_{ij} &gt; 0\\) (not empty), we add backward arc \\((j,i)\\) to \\(A'\\) with \\(k'_{ji} = x_{ij}\\).\n\nThis models the possibility of decreasing the flow on \\((i,j)\\).\n\nIf \\(0 &lt; x_{ij}&lt; k_{ij}\\) we add both a forward arc \\((i,j)\\) and a backward arc \\((j,i)\\) to \\(A'\\).\n\n\n\n7.4.5 The Augmentation Step\nStart with flow \\(x\\) and corresponding auxiliary network \\(G'\\) with auxiliary capacities \\(k'\\).\n\nIdentify \\(st\\)-path \\(P\\) in \\(G'\\).\n\nLet \\(\\displaystyle\\Delta = \\min_{(i,j)\\in P} k'_{ij}\\).\nAdd \\(\\Delta\\) to all forward arcs.\nSubtract \\(\\Delta\\) from all backward arcs.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#example-2",
    "href": "07_Network_Flows.html#example-2",
    "title": "7  Network Flows",
    "section": "7.5 Example",
    "text": "7.5 Example\n\nExample 7.7 Let’s find the maximum flow for \\(s = 1\\) and \\(t=7\\) for the graph Figure 7.12 (a).\nWe start from \\(x = 0\\) with net flow \\(\\varphi = 0\\). The initial auxiliary graph \\(G'\\) is equal to \\(G\\).\n\n\n\n\n\n\n\nFigure 7.8: Graph \\(G\\)\n\n\n\n\nStep 1\nWe have several augmenting paths to choose from. Consider \\(P = (1,2,3,7)\\) in \\(G'\\). The minimum edge capacity in \\(G'\\) along this augmenting path is \\(\\Delta = 2\\). All arcs in this path are forward arcs, so we add \\(\\Delta\\) to \\(x_{ij}\\) for arcs \\(12, 23, 37\\), and leave all flow on other edges equal to \\(0\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary Graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Augmenting Path\n\n\n\n\n\n\n\n\n\n\n\n(c) Updated Flow\n\n\n\n\n\n\n\nFigure 7.9: Details of Step 1.\n\n\n\n\n\nStep 2\nArcs \\(12\\), \\(23\\), and \\(37\\) are now saturated. We can update the auxiliary graph by reversing each of these edges and setting the capacity equal to the flow value across each arc. This reveals the augmenting path \\(P = (1,5,6,7)\\) (among several possible augmenting paths). This path has minimum capacity arc with capacity \\(\\Delta = 2\\) and every arc is a forward arc on this path. Thus, we can increase flow across each of these arcs by \\(\\Delta=2\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary Graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Augmenting Path\n\n\n\n\n\n\n\n\n\n\n\n(c) Updated Flow\n\n\n\n\n\n\n\nFigure 7.10: Details of Step 2.\n\n\n\n\n\nStep 3\nNext, we have augmenting path \\(P=(1,4,3,2,6,7)\\). Note that this path uses the nonempty backward arc \\(32\\). The minimum capacity arc in the auxiliary graph along this path is \\(43\\), with capacity \\(k'_{43} = 1 = \\Delta\\). We can update our flow by increasing the flow by \\(\\Delta=1\\) along all forward arcs in this path and decreasing flow across backward arcs by \\(\\Delta=1\\)\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary Graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Augmenting Path\n\n\n\n\n\n\n\n\n\n\n\n(c) Updated Flow\n\n\n\n\n\n\n\nFigure 7.11: Details of Step 3.\n\n\n\n\n\nStep 4 and Termination\nAfter updating the flow and auxiliary graph \\(G'\\), we can note that \\(t\\) is not reachable from \\(s\\) in \\(G'\\). Indeed, the reachable set from \\(s\\) in \\(G'\\) is \\(S=\\{s,4\\}\\). Note that the cut \\(\\delta(S)\\) defined by \\(S\\) has capacity \\(k=(\\delta(S)) = 5\\). This is equal to the total value of the flow \\(x\\) (\\(\\varphi = 5\\)). This implies that \\(x\\) is a maximimum value flow and \\(\\delta(S)\\) is a minimum capacity cut; we can stop applying the algorithm.\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary Graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Reachable nodes \\(S\\) and minimum capacity cut \\(\\delta(S)\\)\n\n\n\n\n\n\n\nFigure 7.12: Details of Step 4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#termination-and-correctness",
    "href": "07_Network_Flows.html#termination-and-correctness",
    "title": "7  Network Flows",
    "section": "7.6 Termination and Correctness",
    "text": "7.6 Termination and Correctness\n\n7.6.1 Stopping Condition\nLet \\(S^*\\) be the set of nodes reachable from \\(s \\in G'\\) when the algorithm stops.\nWe can conclude that \\(\\delta_{G'}^+(S^*) = \\emptyset\\).\nBy the construction of \\(G'\\), this implies that:\n\nEvery arc \\((i,j) \\in \\delta_{G}^+(S^*)\\) is saturated!\nEvery arc \\((i,j) \\in \\delta_{G}^-(S^*)\\) is empty!\n\n\n\n7.6.2 Correctness of the FF-Algorithm\nWhen the algorithm terminates, all arcs in \\(\\delta_G^+(S^*)\\) are saturated and all arcs in \\(\\delta_G^-(S^*)\\) are empty: \\[\n\\varphi =\\varphi(\\delta(S^*)) = \\sum_{(i,j) \\in \\delta^+_G(S^*)} x_{ij} - \\sum_{(i,j) \\in \\delta^-_G(S^*)} x_{ij}\n= k(\\delta(S^*)).\n\\] However, we know that \\[\n\\varphi \\le \\text{max flow value} \\le \\text{min cut value}\n\\le k(\\delta(S^*)).\n\\]\nThis shows that when the algorithm terminates, we have found a flow equal in value to the capacity of an \\(s,t\\) cut!.\nThis implies that:\n\nThe flow is optimal for the maximum flow problem.\nThe cut is optimal for the minimum cut problem\n\nThis establishes that the Ford-Fulkerson algorithm always finds an optimal solution!\nThis is an algorithmic proof of the max-flow min-cut theorem.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#the-ford-fulkerson-algorithm-pseudocode",
    "href": "07_Network_Flows.html#the-ford-fulkerson-algorithm-pseudocode",
    "title": "7  Network Flows",
    "section": "7.7 The Ford-Fulkerson Algorithm – Pseudocode",
    "text": "7.7 The Ford-Fulkerson Algorithm – Pseudocode\nInitialize x = 0, phi = 0.\n\nWhile x is not a max flow:\n    Construct auxiliary graph G' = (V, A') with capacities k_{ij}'.\n\n    Find an st-path P in G'. \n\n    If P does not exist: % Terminate\n        STOP! x is a max flow.\n    Else % Update flow.\n        Delta =  min {k_{ij}': (i,j) \\in P }\n        phi = phi + Delta. \n\n        For (i,j) in P: \n            If (i,j) is a forward arcL\n                x_{ij} = x_{ij} + \\Delta\n            Else \n                x_{ij} = x_{ij} - \\Delta",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#complexity-of-the-ford-fulkerson-algorithm",
    "href": "07_Network_Flows.html#complexity-of-the-ford-fulkerson-algorithm",
    "title": "7  Network Flows",
    "section": "7.8 Complexity of the Ford-Fulkerson Algorithm",
    "text": "7.8 Complexity of the Ford-Fulkerson Algorithm\n\n7.8.1 A Rough Bound on Elementary Operations\nWe can count the number of elementary operations used by each iteration of the Ford-Fulkerson Algorithm:\n\nConstructing the auxiliary graph \\(G'\\) in Line 4 costs \\(O(m+n)\\) elementary operations. Indeed, we need to construct a copy of \\(G\\), change direction of arcs (if necessary), and update capacities. This requires a finite number of operations for each arc and node.\nWe can find an \\(st\\)-path, if one exists, using a modification of the graph reachability algorithm. This requires \\(O(m+n)\\) elementary operations.\nThe remaining steps in the loop require \\(O(|P|)\\) elementary operations for augmenting path \\(P\\). Assuming that \\(P\\) is a simple path, we have \\(|P| \\le n-1\\) edges.\n\nTherefore, each iteration of the Ford-Fulkerson Algorithm requires \\(O(m)\\) elementary operations (assuming \\(n = O(m)\\)). If the algorithm uses \\(q\\) iterations, then the total number of operations is \\[\n    O(qm).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#worst-case-analysis-of-the-number-of-iterations",
    "href": "07_Network_Flows.html#worst-case-analysis-of-the-number-of-iterations",
    "title": "7  Network Flows",
    "section": "7.9 Worst-Case Analysis of the Number of Iterations",
    "text": "7.9 Worst-Case Analysis of the Number of Iterations\nTo bound the number of operations needed by the Ford-Fulkerson Algorith, we need to determine how many iterations are needed by the Ford-Fulkerson Algorithm in worst-case?\nThe following lemma will help determine an upper bound on the number of iterations needed.\n\nLemma 7.3 If all capacities \\(k_{ij}\\) are integer then \\(k_{ij}'\\), \\(\\Delta\\), and \\(x\\) are integer at each iteration.\n\nSince \\(\\Delta\\) is integer, we could have unit increment \\(\\Delta = 1\\) each iteration (in worst case). More precisely, there are graphs where \\[\n\\Delta = \\min_{ij} k_{ij}' = 1\n\\] every iteration of the Ford-Fulkerson Algorithm. In this case, if we start with flow value \\(\\varphi=0\\) then we need \\(q = \\varphi^*\\) iterations, where \\(\\varphi^*\\) is the value of the maximum flow.\nOn the other hand, if \\(\\Delta\\) is integral and positive every iteration, then we increment \\(\\varphi\\) by at least \\(\\Delta\\) each iteration. Therefore, \\(q \\le \\varphi^*\\) if the capacities are integral.\nIt follows that the worst-case complexity of the Ford-Fulkerson Algorithm is \\(O(\\varphi^* m )\\).\n\n7.9.1 A Pathologically Bad Example\n\nExample 7.8 Let’s apply the Ford-Fulkerson Algorithm with the graph \\(G\\) given in Figure 7.13 and initial flow \\(x=0\\).\n\nWe increase flow by \\(\\Delta=1\\) along the augmenting path \\(P = (s,2,1,t)\\) (Figure 7.17 (a)) This yields the new flow given in Figure 7.14 (b).\n\n\n\n\n\n\nFigure 7.13: Graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Augmenting path in auxiliary graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Updated flow\n\n\n\n\n\n\n\nFigure 7.14: Details for first step of the Ford-Fulkerson Algorithm.\n\n\n\nAfter updating the flow, we need to update the auxiliary graph. This reveals the augmenting path \\(P=(s,1,2,t)\\). We can increase the flow along this path by \\(\\Delta=1\\). See Figure 7.15.\n\n\n\n\n\n\n\n\n\n\n\n(a) Augmenting path in auxiliary graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Updated flow\n\n\n\n\n\n\n\nFigure 7.15: Details for second step of the Ford-Fulkerson Algorithm.\n\n\n\nWe can repeat this process. In the next step, we have augmenting path \\(P=(s,2,1,t)\\); this is the same augmented path used in Step 1. We can increase flow by \\(\\Delta=1\\). In the fourth step, we increase flow by \\(1\\) along the augmenting path \\(P=(s,1,2,t)\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Augmenting path in auxiliary graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Updated flow\n\n\n\n\n\n\n\nFigure 7.16: Details for third step of the Ford-Fulkerson Algorithm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Augmenting path in auxiliary graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Updated flow\n\n\n\n\n\n\n\nFigure 7.17: Details for fourth step of the Ford-Fulkerson Algorithm.\n\n\n\nWe can repeat this process a total of \\(M\\) times (\\(2M\\) iterations). This gives optimal flow given in\n\n\n\n\n\n\nFigure 7.18: Optimal flow in graph \\(G\\) given in Figure 7.13\n\n\n\nApplying the Ford-Fulkerson Algorithm with this graph requires exactly \\(q = 2M = \\varphi^*\\) iterations.\n\n\n7.9.2 Does the FF Algorithm Run in Polynomial Time?\nWe have deduced that the Ford-Fulkerson algorithm has complexity of \\(O(\\varphi^* m)\\).\nIt is unclear whether \\(O(\\varphi^* m)\\) a polynomial of the instance size.\nUnfortunately, we can’t bound \\(\\varphi^*\\) as a polynomial of \\(n\\) and/or \\(m\\).\nQuestion: how should we represent the size of \\(\\varphi^*\\)?\n\n\n7.9.3 Aside: Binary Encoding\nWe’ll use binary encoding of numbers in our model of complexity.\nBinary encoding is a positional notation system with base \\(2\\):\n\nUses two symbols: \\(0\\) and \\(1\\).\nIt is positional: same symbol represents different orders of magnitude based on relative position.\nDigit/symbol is called a bit.\n\n\nExample 7.9 Let’s convert the base-2 or binary encoding \\(100110\\) to base-10: \\[\n\\begin{aligned}\n(100110)_{10} &= 0 \\cdot 2^0 + 1 \\cdot 2^1 + 0\\cdot 2^3 + 0 \\cdot 2^4 + 1 \\cdot 2^5 \\\\\n&= 2 + 4 + 32 = 38.\n\\end{aligned}\n\\]\n\n\nTheorem 7.3 Given a base-10 number \\(n\\), we need \\[\n    \\lfloor{\\log n \\rfloor} + 1 = O(\\log n)\n\\] bits to encode it in base-2.\n\n\nExample 7.10 For example, \\[\n(38)_{10} = (100110)_{2}\n\\] requires \\[\n    \\lfloor\\log_2(38)\\rfloor \\approx \\lfloor 5.25\\rfloor + 1 = 6\n\\] bits of storage.\n\n\n\n7.9.4 Storage Needs of the Maximum Flow Problem\nFor an instance of the maximum flow problem, we need to store the graph topology, i.e., nodes and adjacencies encoded by arcs, and the capacity value per arc.\nWe can extend our notion of an adjacency list to keep track of indices and arc capacities. We will store the capacity of an arc using tuples in a list for each node. This is illustrated in the following example.\n\nExample 7.11 Consider the graph given in Figure 7.19. We can encode the edges within the adjacency list. For example, nodes \\(2\\) and \\(t\\) are both adjacent \\(1\\), by arcs with capacities \\(2\\) and \\(6\\) respectively. We can encode this using a modified adjacency list \\[\n    L(1) = \\{(2,2), (t,6)\\}.\n\\] We can construct adjacency lists for each remaining node using an identical process: \\[\n\\begin{aligned}\n    L(s) &= \\{(1,7), (2,4)\\}, \\\\\n    L(2) &= \\{(1,1), (t,5)\\}, \\\\\n    L(t) &= \\emptyset.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nFigure 7.19: Graph \\(G\\)\n\n\n\n\n\n7.9.5 Instance Size of Maximum Flow\nArc \\((i,j)\\) corresponds to an entry in adjacency sublist \\(L(i)\\).\nUsing a binary encoding, each entry contains:\n\nThe index \\(j \\in \\{1,\\dots, n\\}\\) encoded using \\[\n\\lfloor{\\log j \\rfloor} + 1 = O(\\log j) = O(\\log n)\\text{ bits,}\n\\] since \\(\\log j = O(\\log n)\\).\nThe capacity \\(k_{ij}\\) of arc \\((i,j)\\) encoded using \\[\\lfloor{\\log k_{ij} \\rfloor} + 1 = O(\\log k_{\\max}) \\text{ bits,}\\] where \\(k_{\\max} := \\max\\left\\{k_{ij}: (i,j) \\in A \\right\\}\\).\n\nThis totals \\(O\\Big(m \\big(\\log n + \\log k_{\\max} \\big) \\Big)\\) bits.\n\nExample 7.12 Consider the graph \\(G\\) given in Figure 7.13. Recall, that solving the maximum flow problem using the Ford-Fulkerson Algorithm has complexity \\(O(\\varphi^*)\\). We’d like to know how this complexity \\(O(\\varphi^*)\\) compares to instance size \\(O(m(\\log n + \\log k_{\\max}))\\).\nRecall that \\(\\varphi^* = 2M\\) for this example. Moreover, \\(k_{\\max} = M\\). Unfortunately, the instance size depends logarithmically on \\(k_{\\max}\\), not linearly. Indeed, \\[\n    k_{\\max} = 2^{\\log_2 (k_{\\max})}.\n\\] This implies that \\[\nO(\\varphi^* m = O(k_{\\max} m) = O\\left( 2^{\\log k_{\\max}}\\cdot m \\right),\n\\] which grows exponentially in \\(\\log (k_{\\max})\\).\nOn the other hand, the instance size \\(O(m(\\log n + \\log k_{\\max}))\\) grows linearly in \\(\\log (k_{\\max})\\).\nTherefore, \\(O(\\varphi^* m)\\) is not a polynomial function of the instance size. Therefore, the Ford-Fulkerson Algorithm is not polynomial in complexity.\n\n\n\n7.9.6 The General Case\nThis example establishes that the Ford-Fulkerson Algorithm is not polynomial.\nWe can generalize the complexity of the FF algorithm without relying on \\(\\varphi^*\\). To avoid using \\(\\varphi^*\\), which is usually not known, we can substitute an upper bound on \\(\\varphi^*\\).\n\nExample 7.13 As a simple example, consider a graph with maximum capacity \\(k_{\\max}\\) and \\(m\\) arcs. Then \\[\n\\varphi^* \\le m k_{\\max}\n\\] since each arc cannot carry more flow than \\(k_{\\max}\\) and there are \\(m\\) arcs. This gives the upper bound on complexity \\[\n    O(\\varphi^* m) \\le O\\left( (k_{\\max} m) \\cdot m \\right)\n    = O\\left(k_{\\max} m^2 \\right).\n\\] Unfortunately, this upper bound is tight: there are graphs \\(G\\) for which the Ford-Fulkerson Algorithm uses \\(\\varphi^* = k_{\\max} m\\) iterations.\n\n\n\n7.9.7 Pseudopolynomiality\nThe complexity \\(O(k_{\\max} m^2)\\) is not polynomial if we assume a binary encoding because \\(k_{\\max} = O(2^{\\log k_{\\max}})\\).\nHowever, the instance size is polynomial with respect to instance size if we use a unary encoding (encode integers using only \\(1\\)s):\n\nFor example, \\(7\\) is encoded in unary as \\(1111111\\).\n\nUsing a unary encoding, we need:\n\n\\(O(mn)\\) symbols to store the graph topology, and\n\\(O(mk_{\\max})\\) symbols to store the arc capacities.\n\nWe call such an algorithm pseudopolynomial.\n\n\n7.9.8 Polynomial-Time Algorithms for Maximum Flow\nStrongly polynomial algorithms do exist for the maximum flow problem. The Edmonds/Karp algorithm23 is the most widely used example, but is beyond the scope of this course.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "07_Network_Flows.html#footnotes",
    "href": "07_Network_Flows.html#footnotes",
    "title": "7  Network Flows",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Linear_programming↩︎\nhttps://cp-algorithms.com/graph/edmonds_karp.html↩︎\nhttps://en.wikipedia.org/wiki/Edmonds-Karp_algorithm]↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Flows</span>"
    ]
  },
  {
    "objectID": "08_Matching.html",
    "href": "08_Matching.html",
    "title": "8  Matching Problems",
    "section": "",
    "text": "8.1 The Unweighted Matching Problem",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matching Problems</span>"
    ]
  },
  {
    "objectID": "08_Matching.html#the-unweighted-matching-problem",
    "href": "08_Matching.html#the-unweighted-matching-problem",
    "title": "8  Matching Problems",
    "section": "",
    "text": "8.1.1 Motivation\nGiven resources and locations, as well as compatibility constraints, we we want to match/assign resources to locations to create as many pairs as possible.\n\nAssignment of workers to jobs, students to internships, etc;\nAssign flights to gates;\nAssignment of physicians to hospitals, and so on.\n\n\n\n8.1.2 Matchings\n\nDefinition 8.1 (Matchings) Given an undirected graph \\(G = (V,E)\\) with \\(n\\) nodes and \\(m\\) edges, we call a subset of edges \\(M \\subseteq E\\) a matching if it satisfies one of the following equivalent properties:\n\nNo two edges in \\(M\\) are incident with the same vertex.\n\\(H = (V, M)\\) has node degrees with values \\(0\\) or \\(1\\).\n\n\n\nExample 8.1 Consider the graph \\(G\\) given in Figure 8.1. The set \\(M = \\{01, 23\\}\\) is a matching. On the other hand \\(\\tilde{M} = \\{01, 03\\}\\) is not a matching since it includes two edges incident with \\(0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A matching in graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Not a matching in graph \\(G\\)\n\n\n\n\n\n\n\nFigure 8.1: Subsets in graph \\(G\\): \\(M = \\{01, 23\\}\\) is a matching, but \\(\\tilde{M} = \\{01, 03\\}\\) is not.\n\n\n\n\n\n8.1.3 Bipartite Graphs\nWe will focus on the task of finding matchings in special class of graphs, namely, bipartite graphs.\n\nDefinition 8.2 (Bipartite Graphs) An undirected graph \\(G = (V,E)\\) is bipartite if \\(V\\) can be partitioned into two disjoint sets \\(A\\) and \\(B\\) such that for each \\((i,j) \\in E\\):\n\nEither \\(i \\in A\\) and \\(j \\in B\\),\nOr \\(j \\in A\\) and \\(i \\in B\\).\n\n\nWe call \\(A\\) and \\(B\\) shores or color classes or independent sets.\n\nExample 8.2 Figure 8.2 gives two representations of the same bipartite graph \\(G\\). The vertices of \\(G\\) consist of two shores \\(A = \\{0,1,2,3\\}\\) and \\(B = \\{4,5,6\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A bipartite graph \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Another representation of \\(G\\)\n\n\n\n\n\n\n\nFigure 8.2: Two visualizations of a bipartite graph \\(G\\).\n\n\n\n\n\n8.1.4 The Unweighted Matching Problem\nGiven a bipartite graph \\(G\\), we want to find the matching of \\(G\\) containing the maximum number of edges.\n\nDefinition 8.3 (The Unweighted Matching Problem) Given an undirected graph \\(G\\), find a matching \\(M \\subseteq E\\) with maximum cardinality.\n\n\nDefinition 8.4 (The Matching Number) We call the cardinality of a maximum matching the matching number \\(\\nu(G)\\).\n\n\nExample 8.3 The graph given in Figure 8.3 has matching number \\(\\nu(G) = 3\\). Indeed, \\(G\\) contains maximum cardinality matchings \\(M_1 = \\{03, 14, 25\\}\\) and $M_2 = {05, 13, 24}\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Maximum cardinality matching in \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Another maximum cardinality matching in \\(G\\)\n\n\n\n\n\n\n\nFigure 8.3: Maximum matchings in a given graph.\n\n\n\n\n\n8.1.5 The Augmenting Path Algorithm\nWe will construct an algorithm for solving the maximum matching problem, based on construction of augmenting paths. We start by defining a set of nodes that are not incident with matching edges.\n\n8.1.5.1 Exposed Nodes\n\nDefinition 8.5 (Exposed nodes) Given a matching \\(M\\), we call any unmatched vertex exposed.\n\n\nExample 8.4 Consider the matching \\(M= \\{ 05, 24\\}\\) for graph \\(G\\) given in Figure 8.4. The nodes \\(1\\) and \\(3\\) are exposed for this matching.\n\n\n\n\n\n\n\nFigure 8.4: Exposed nodes \\(1\\) and \\(3\\)\n\n\n\n\n\n8.1.5.2 Alternating Paths and Cycles\n\nDefinition 8.6 (Alternating Paths) Given a matching \\(M\\), a path \\(P\\) is an alternating path (cycle) if the edges of the path (cycle) alternate between \\(M\\) and \\(E \\setminus M\\).\n\n\nExample 8.5 Consider the graph \\(G\\) with matching \\(M= \\{ 05, 24\\}\\). Figure 8.5 highlights an alternating path \\(P=(0,5,2,4)\\) and an alternating cycle \\(C = (0,5,2,4,0)\\). We use dashed edges to indicate edges in both \\(M\\) and \\(P/C\\) and dash-dotted edges for edges not in \\(M\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) An alternating path\n\n\n\n\n\n\n\n\n\n\n\n(b) An alternating cycle\n\n\n\n\n\n\n\nFigure 8.5: Alternating path and alternating cycle in \\(G\\).\n\n\n\n\n\n8.1.5.3 Augmenting Paths\nWe will iteratively identify alternating paths along which to increase the cardinality of a known matching. To do so, we use the following definition of an augmenting path.\n\nDefinition 8.7 (Augmenting Path) An alternating path is an augmenting path if it starts and ends with an exposed node.\n\n\nExample 8.6 Consider \\(G\\) with matching \\(M = \\{05, 24\\}\\) again. Recall that the nodes \\(1\\) and \\(3\\) are exposed. The path \\(P = (1,5,0,3)\\) is an augmenting (1,3)-path Indeed, the edges of this path alternate between edges not in \\(M\\) and those in \\(M\\), and the path starts and ends with exposed nodes.\n\n\n\n\n\n\n\nFigure 8.6: An augmenting path \\(P=(1,5,0,3)\\)\n\n\n\n\n\n8.1.5.4 Properties of Augmenting Paths\n\nLemma 8.1 Augmenting paths contain an even number of nodes and odd number of edges.\n\n\nProof. Suppose that \\(P\\) is an augmenting path for matching \\(M\\). Since \\(P\\) is augmenting, it starts and ends with an exposed node.\nThis implies that the first and last edges of \\(P\\) are not in \\(M\\). Consequently, if \\(P\\) contains \\(k\\) edges in \\(M\\), then \\(P\\) must contain \\(k+1\\) edges in \\(E\\setminus M\\) since \\(P\\) is an alternating path. Therefore, \\(P\\) contains \\(2k+1\\) edges total, which is odd for every nonnegative integer \\(k\\).\n\n\nTheorem 8.1 (The Augmenting Path Theorem) A matching \\(M\\) has maximum cardinality if and only if it admits no augmenting paths.\n\n\nProof. We start by proving necessity. Suppose that \\(M\\) is a maximum cardinality matching; we want to show that \\(M\\) has not augmenting paths.\nWe’ll do so by contradiction. Suppose that \\(M\\) has augmenting path \\(P\\). Let’s consider the symmetric difference of \\(M\\) and \\(P\\): \\[\n\\begin{aligned}\nM' &= M \\Delta P &= (M\\cup P) \\setminus (M\\cap P) \\\\\n    &= (M \\setminus P) \\cup (P \\setminus M).\n\\end{aligned}\n\\]\nLet \\(P = ((v_1, v_2), (v_2, v_3), \\dots, (v_{2k+1}, v_{2k+2}))\\); we can assume that \\(P\\) contains an even number of nodes by Lemma 8.1.\nWe’ll use this fact to construct a matching with more edges than \\(M\\):\n\nWe know that \\((v_1, v_2) \\notin M\\) since \\(v_1\\) is exposed. This implies that \\((v_1, v_2) \\in M'\\).\nNext, \\((v_2, v_3)\\) is in \\(M\\cap P\\) because \\(P\\) is an alternating path. Therefore, \\((v_2, v_3) \\notin M'\\).\n\nWe can continue this pattern to establish that \\(M'\\) is a matching. If \\(P\\) contains \\(k\\) edges in \\(M\\) and \\(k+1\\) edges in \\(E\\setminus M\\), then \\(M'\\) must consist of the \\(k+1\\) edges in \\(E\\setminus M\\). This implies that \\(M\\) is not a maximum matching; a contradiction. Therefore, if \\(M\\) is a maximum matching then no augmenting path exists.\nLet’s now prove sufficiency. Suppose that there is no augmenting path. Let’s assume that \\(M\\) is not a maximum matching and find a contradiction.\nBy assumption, there is some matching \\(M'\\) with \\(|M'| &gt; |M|\\). Let’s consider the symmetric difference \\[\n    M'' = M \\Delta M' = (M\\setminus M') \\cup (M' \\setminus M).\n\\]\nLet’s consider three subgraphs of \\(G\\):\n\n\\(H = (V, M)\\);\n\\(H' = (V, M')\\); and\n\\(H'' = (V, M'')\\).\n\nFigure 8.7 gives an example illustrating the construction of these three subgraphs.\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(H\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(H'\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(H''\\)\n\n\n\n\n\n\n\nFigure 8.7: Example of bipartite graph \\(G\\), with subgraphs \\(H\\), \\(H'\\), and \\(H''\\).\n\n\n\nLet’s consider maximally connected subgraphs of \\(H''\\); these are subgraphs that adding an node creates a disconnected subgraph of \\(G\\). Note that:\n\nNodes in \\(H\\) and \\(H'\\) have either degree \\(0\\) or \\(1\\), since \\(H\\) and \\(H'\\) are both induced by matchings.\nThis implies that the nodes in \\(H''\\) have degrees in \\(\\{0,1,2\\}\\).\n\nTherefore, each maximally connected component of \\(H''\\) is one of:\n\nan isolated node;\nan alternating path; or\nan alternating cycle.\n\nSince \\(|M'| &gt; |M|\\) there is some maximally connected subgraph \\(C\\) with more edges from \\(M'\\) than edges from \\(M\\). Note that \\(C\\) is not an isolated node. Moreover, \\(C\\) is not an alternating cycle, because an alternating cycle would contain the same number of edges from \\(M\\) and \\(M'\\). Therefore, \\(C\\) is an alternating path. Further, \\(C\\) is an augmenting path. Indeed, Its end points are exposed with respect to \\(M\\), since it is alternating and contains more edges from \\(M'\\) than \\(M\\).\n\n\n\n\n8.1.6 The Augmenting Path Algorithm\nWhile augmenting path \\(P\\) exists:\n\nFind an augmenting path \\(P\\)\nIf \\(P \\neq \\emptyset\\):\n\n\\(M \\gets \\left( M \\cup P\\right) \\setminus \\left(M \\cap P \\right).\\)\n\nElse:\n\nSTOP (exit loop).\n\n\n\n\n8.1.7 Finding an Augmenting Path\nWe need an algorithmic way to identify an augmenting path. To do so, we will work with an auxiliary graph.\n\n8.1.7.1 The Auxiliary Graph\nWe build an auxiliary directed bipartite graph \\(G' = (V, E')\\) where, for each \\(\\{i,j\\} \\in E\\), \\(i \\in A\\), \\(j\\in B\\):\n\nAdd \\((i,j)\\) to \\(E'\\) if \\(\\{i,j\\} \\notin M\\).\nAdd \\((j,i)\\) to \\(E'\\) if \\(\\{i,j\\} \\in M\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(M = \\{05, 24\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Auxiliary graph\n\n\n\n\n\n\n\nFigure 8.8: Construction of the auxiliary graph for graph \\(G\\) and matching \\(M=\\{05, 24\\}\\).\n\n\n\nThe vertex set \\(A\\) can be left only via arcs corresponding to edges not in \\(M\\).\nThe vertex set \\(B\\) can only be left via arcs corresponding to edges in \\(M\\).\nThis implies that paths in \\(G'\\) are alternating.\n\nPaths between exposed nodes \\(i \\in A\\) and \\(j \\in B\\) are augmenting.\n\n\n\n8.1.7.2 A Practical Algorithm for Finding an Augmenting Path\nTo find an augmenting path using \\(G'\\):\n\nIntroduce two extra nodes \\(s,t\\).\nConnect \\(s\\) to all the exposed nodes in \\(A\\).\nConnect \\(t\\) to each exposed node in \\(G\\).\nLook for an \\(st\\)-path in the extended version of \\(G'\\).\n\n\nExample 8.7 Figure 8.9 illustrates the construction of the extended auxiliary graph with graph \\(G\\) containing matching \\(M = \\{05, 24\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\) with matching \\(M = \\{05, 24\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Extended auxiliary graph\n\n\n\n\n\n\n\nFigure 8.9\n\n\n\n\n\n\n8.1.8 Complexity of the Augmenting Path Algorithm\n\n8.1.8.1 Operations per Iteration\nTo find an augmenting path \\(P\\) costs \\(O(m+n)\\). Indeed, we need to construct the auxiliary graph and then apply a variant of graph reachability to find an \\(st\\)-path in the auxiliary; both steps cost \\(O(m+n)\\) elementary operations.\nOn the other hand, updating \\(M\\) as \\(M \\Delta P\\) costs \\(O(n)\\) operations.\nTherefore, the total complexity is \\[\n    \\text{maximum \\# of iterations } \\times O(m + n).\n\\]\n\n\n8.1.8.2 Total Number of Iterations\nNotat that a matching \\(M\\) has at most \\(n/2\\) edges in a bipartite graph. Indeed, \\[\n\\nu(G) \\le \\min \\{|A|, |B|\\} \\le \\frac{n}{2}.\n\\] The augmenting path algorithm increases the cardinality of a matching by \\(1\\) each iteration. Therefore, the augmenting path algorithm has total complexity bounded above by \\[\n    O\\Big( \\nu(G) \\times (m+n)\\Big)\n    = O( n^2 + mn).\n\\] If \\(G\\) is connected, we have \\(n \\le m+1 = O(m)\\). In this case, we have total complexity \\(O(mn)\\).\n\n\n\n8.1.9 Example\n\nExample 8.8 Consider the graph \\(G\\) given in Figure 8.10 with initial matching \\[\nM_0 = \\{16, 27, 39\\}.\n\\]\n\n\n\n\n\n\n\nFigure 8.10: Graph \\(G\\) for Example 8.8 with matching \\(M_0\\).\n\n\n\n\n8.1.9.1 Step 1\nWe start by constructing the auxiliary graph \\(G'\\). After doing so, we can observe that there is an \\(st\\)-path \\(P' = (s,4,7,2,5,t)\\) in \\(G'\\). This gives the augmenting path \\(P = (4,7,2,5)\\).\nWe update the matching \\[\nM_1 = M_0 \\Delta P = (M_0 \\cup P) \\setminus (M_0 \\cap P)\n= \\{16, 39, 25, 47\\}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Augmenting path \\(P'\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) New matching \\(M_1\\)\n\n\n\n\n\n\n\nFigure 8.11: Details of the first step of the augmenting path algorithm.\n\n\n\n\n\n8.1.9.2 Step 2\nAs before, we construct the auxiliary graph \\(G'\\) (Figure 8.12 (a)). Note that \\(t\\) is not reachable from \\(s\\) in \\(G'\\); Figure 8.12 (b) gives the reachable set from \\(s\\) in \\(G'\\).\nThis implies that there are no augmenting paths for matching \\(M_1\\). Therefore, \\(M_1\\) is a maximum matching.\n\n\n\n\n\n\n\n\n\n\n\n(a) Auxiliary graph \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Reachable nodes from \\(s\\)\n\n\n\n\n\n\n\nFigure 8.12: Details of the second and final step of the augmenting path algorithm.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matching Problems</span>"
    ]
  },
  {
    "objectID": "08_Matching.html#the-weighted-matching-problem",
    "href": "08_Matching.html#the-weighted-matching-problem",
    "title": "8  Matching Problems",
    "section": "8.2 The Weighted Matching Problem",
    "text": "8.2 The Weighted Matching Problem\n\n8.2.1 Problem Definition\n\nDefinition 8.8 (Weighted Matching Problem) Given an undirected graph \\(G = (V,E)\\) and a weight function \\(w: E \\mapsto \\mathbf{R}\\), find a matching \\(M \\subseteq E\\) of maximum weight: \\[\nw(M) = \\sum_{\\{i,j\\} \\in M} w_{ij}.\n\\]\n\n\nDefinition 8.9 (Weighted Matching Number) We call the weight of a maximum-weight matching of \\(G\\) the weighted matching number and denote it \\(\\nu_w(G)\\).\n\n\n\n8.2.2 Extremality\n\nDefinition 8.10 (Extreme Matchings) We call a matching extreme if it has maximum weight among all matchings of the same cardinality \\(|M|\\).\n\nQuestion: We know how to construct another matching of cardinality one unit larger using the unweighted matching algorithm.\n\nCan we construct this matching so that it is extreme?\nIf so, we can build an extreme matching \\(M_1\\) from an empty matching \\(M_0\\). From \\(M_1\\), we can build an extreme matching \\(M_2\\) of size \\(2\\), and so on.\n\n\n8.2.2.1 An Observation\n\nTheorem 8.2 If \\(M_1, M_2, \\dots, M_{\\nu(G)}\\) is a sequence of extreme matchings of increasing size, then \\[\nM = \\textrm{arg max} \\Big\\{ w(M_i): i = 1, 2, \\dots, \\nu(G) \\Big\\}\n\\] is a maximum-weight matching.\n\nHere, the \\(\\textrm{argmax}\\) operator returns a extreme matching with maximum weight:\n\nif a maximum weight matching has cardinality \\(1\\), then \\(M_1\\) is a maximum weight matching;\nif a maximum weight matching has cardinality \\(2\\), then \\(M_2\\) is a maximum weight matching; etc.\n\n\n\n\n8.2.3 Augmentation and Weights of Matchings\nLet \\(M\\) be a matching and let be \\(P\\) an augmenting path.\nLet \\[\n    M' := M \\Delta P = (M \\cup P) \\setminus (M\\cap P) = (M\\setminus P) \\cup (P \\setminus M).\n\\] The weight of \\(M'\\) is \\[\nw(M') = \\sum_{ij \\in M'} w_{ij} = w(M) + \\underbrace{ \\sum_{e \\in P\\setminus M} w_i - \\sum_{e \\in P \\cap M} w_e}_{=:(*)}.\n\\] The summand \\((*)\\) is a linear combination of weights of edges in \\(P\\) with\n\ncoefficient \\(+1\\) if \\(e \\notin M\\);\ncoefficient \\(-1\\) if \\(e \\in M\\).\n\nLet’s introduce new weights and rewrite the identity: \\[\n    \\ell_{ij} =\n    \\begin{cases}\n    -w_{ij} & \\text{if } ij \\notin M, \\\\\n    + w_{ij} & \\text{if } ij \\in M.\n    \\end{cases}\n\\] Using these lengths, we have \\[\n    w(M') = w(M) - \\sum_{ij \\in P} \\ell_{ij}.\n\\] This implies that the increase in weight is maximized if the length of \\(P\\) is minimized.\n\n\n8.2.4 The Shortest Augmenting Path Theorem\nQuestion: Suppose that we augment an extreme matching \\(M\\) by a shortest augmenting path.\n\nDoes this yield an extreme matching \\(M'\\) (with cardinality \\(|M'| = |M| + 1\\))?\n\nWe have the following theorem.\n\nTheorem 8.3 Given an extreme matching \\(M\\) and an augmenting path \\(P\\) of minimum length.\nThe matching \\(M' := M \\Delta P\\) is an extreme matching of cardinality \\(|M'| = |M| + 1\\).\n\n\nProof. We’ll use contradiction. Let’s assume that \\(M'\\) is not extreme. Then there is \\(N'\\) with \\(|N'| = |M'|\\) such that \\[\n    w(N') &gt; w(M').\n\\] We know that \\(|N'| = |M'| &gt; |M|\\). By an identical argument to the proof in the unweighted case, the subgraph \\[\n    G_{M \\Delta N'} = (V, M \\Delta N')\n\\] has a maximal connected component \\(C\\) with maximal connected component \\(C\\) with more edges from \\(N'\\) than \\(M\\), i.e., an alternating path. This alternating path augments \\(M\\). (Notes \\(C\\) may not be \\(P\\).)\nEarlier, in the unweighted case, we used \\(C\\) to extend \\(M\\) to a larger matching. Here, we’ll do the opposite: we’ll shrink \\(N'\\) using \\(C\\): \\[\n    N = N' \\Delta C.\n\\] Note that \\[\n|N| = |N'| - 1 = |M|.\n\\] We have\n\nfour matchings \\(M, N, M', N'\\);\ntwo augmenting paths \\(P\\) and \\(C\\).\n\nBy assumption \\(\\ell(P) \\le \\ell(C)\\). Moreover, \\(w(M) \\ge w(N)\\). It follows that \\[\n    w(M') &lt; w(N') = w(N) - \\ell(C) \\le w(M) - \\ell(P) = w(M').\n\\] This is a contradiction. Therefore, \\(M'\\) is an extreme matching.\n\n\n\n8.2.5 The Iterative Shortest Path Algorithm\n\n8.2.5.1 The Algorithm\nInitialize \\(M_0 = \\emptyset\\).\nFor \\(i=0,1,2,\\dots, \\nu(G)\\):\n\nFind a shortest \\(M_i\\)-augmenting path \\(P\\).\nIf \\(P \\neq \\emptyset\\):\n\n\\(M_{i+1} \\gets \\left( M_i \\cup P\\right) \\setminus \\left(M_i \\cap P \\right).\\)\n\\(w(M_{i+1}) \\gets w(M_i) - \\ell(P)\\)\n\nElse:\n\nSTOP (exit loop).\n\n\nReturn \\(M = \\textrm{arg max} \\{w(M_i)\\}\\).\n\n\n8.2.5.2 Finding a Shortest Augmenting Path\nWe need to find augmenting paths. To do so, we construct an auxiliary graph and identify the shortest path, which will yield the desired augmenting path.\n\nBuild an auxiliary directed graph \\(G' = (V \\cup \\{s,t\\}, A')\\) for matching \\(M_i\\):\nFor each \\(\\{i,j\\} \\in E\\):\n\nAdd \\((i,j)\\) to \\(A'\\) if \\(\\{i,j\\} \\notin M_i\\) with length \\(\\ell_{ij} = - w_{ij}\\).\nAdd \\((j,i)\\) to \\(A'\\) if \\(\\{i,j\\} \\in M_i\\) with length \\(\\ell_{ij} = w_{ij}\\)\n\nConnect \\(s\\) and \\(t\\) to/from the exposed nodes in the two shores with zero length arcs.\nFinally, find a shortest \\(st\\)-path in \\(G'\\).\n\n\nExample 8.9 Figure 8.13 illustrates the construction of the auxiliary graph for a given matching \\(M_0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Graph \\(G\\) and matching \\(M_0\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(G'\\)\n\n\n\n\n\n\n\nFigure 8.13: Auxiliary graph \\(G'\\) for graph \\(G\\) and matching \\(M_0 = \\{16\\}\\).\n\n\n\n\n\n8.2.5.3 Cycles in Bipartite Graphs\nQuestion: The auxiliary graph \\(G'\\) has edges with negative lengths.\n\nIs it possible to have cycle with negative length?\nIn this case, the shortest-path problem is ill-posed.\n\nWe will use the following lemma to address this possible issue.\n\nLemma 8.2 Every directed bipartite graph \\(G\\) does not contain odd cycles\n\n\nProof. We use contradiction. Let’s assume that \\(G\\) has a cycle \\(C\\) with odd length. Specifically, assume \\(C\\) has \\(2k+1\\) nodes for some positive integer \\(k\\); label these nodes as \\(v_1, v_2, \\dots, v_{2k+1}\\).\nSince \\(G\\) is bipartite, we change shore with each edge. That is, \\(v_i, v_{i+1}\\) are in different shores. The last edge in the cycle is \\((v_{2k+1}, v_1)\\). Since \\(C\\) has odd length, \\(v_1\\) and \\(v_{2k+1}\\) are in the same shore of \\(G\\). Thus, this edge cannot exist; a contradiction.\nTherefore, any cycle in \\(G\\) has even length.\n\nThe following theorem confirms that the auxiliary graph does not have negative-length cycles if the matching \\(M\\) is extreme.\n\nTheorem 8.4 If \\(M\\) is an extreme matching then \\(G'\\)does not contain negative-length cycles.\n\n\nProof. To obtain a contradiction, suppose \\(C\\) is a cycle with negative length: \\[\n\\ell(C) = \\sum_{e \\in C} \\ell_e &lt; 0.\n\\] Since \\(G'\\) is bipartite, \\(C\\) has even length by Lemma 8.2. Moreover, \\(C\\) is alternating by the orientation of arcs of \\(G'\\).\nNow, consider \\(M' = M \\Delta C\\). Note that \\[\n|M'| = |M|.\n\\] Indeed, \\(C\\) is even so half of the edges in \\(M\\cup C\\) are in \\(M\\cap C\\). Thus, \\[\n    |C| = |M| + |M'|\n\\] is even and \\(|M| = |M'|\\)..\nNow consider the weight of \\(M'\\): \\[\nw(M') = w(M) - \\ell(C) &gt; w(M)\n\\] since \\(\\ell(C) &lt; 0\\). Therefore, \\(M\\) is not an extreme matching.\n\n\n\n\n8.2.6 Complexity\nFor each \\(i = 0, 1, 2, \\dots, \\nu(G)\\), we need \\(O(n+m)\\) operations to form the auxiliary graph \\(G'\\) and \\(O(nm)\\) operations to find the shortest \\(st\\)-path using the Bellman-Ford Algorithm. After identifying a shortest \\(M_i\\)-augmenting path, we need a further \\(O(n)\\) operations to compute \\(M_{i+1}\\). Thus, each iteration has complexity \\(O(mn)\\).\nWe repeat this process \\(\\nu(G) = O(n)\\). Thus, the total complexity is \\[\n    O(\\nu(G) mn) = O(n^2 m).\n\\]\n\n\n8.2.7 Example\n\nExample 8.10 Consider the graph \\(G\\) given in Figure 8.14.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(G\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(G'\\)\n\n\n\n\n\n\n\nFigure 8.14: Graph \\(G\\) with initial matching \\(M_0 = \\emptyset\\) and corresponding auxiliary graph \\(G'\\).\n\n\n\n\nStep 1\nWe start by finding the shortest \\(st\\)-path in \\(G'\\). Here, \\((s,1,4,t)\\) and \\((s,0,5,t)\\) are both shortest \\(st\\)-paths (with length \\(-5\\)). We can use either as an augmenting path. Let’s choose \\((s,1,4,t)\\) to use as an augmenting path and add \\(14\\) to \\(M_0\\) to get \\(M_1:\\) \\[\n    M_1 = M_0 \\Delta P_0 = \\emptyset \\Delta \\{14\\} = \\{14\\};\n    \\;\\;\\;\n    w(M_1) = 5.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M_1\\)\n\n\n\n\n\n\n\nFigure 8.15: Matching \\(M_1\\) following one iteration of augmenting path algorithm.\n\n\n\n\n\nStep 2\nThe shortest \\(st\\)-path in the auxiliary graph \\(G'\\) is \\((s,0,5,t)\\). We use the path \\(P_1 = \\{05\\}\\) to update the matching: \\[\n    M_2 = M_1 \\Delta P_1 = \\{14, 05\\};\n    \\;\\;\\;\n    w(M_2) = 10.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M_2\\)\n\n\n\n\n\n\n\nFigure 8.16: Details of Step 2 of the augmenting path algorithm.\n\n\n\n\n\nStep 3\nNext, the shortest \\(st\\)-path is \\((s,2,4,1,3,t)\\). We compute \\(M_3\\) using \\(P_2 = (2,4,1,3)\\): \\[\n    M_3 = M_2 \\Delta P_2 = \\{05, 24, 13\\};\n    \\;\\;\\; w(M_3) = 13.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M_3\\)\n\n\n\n\n\n\n\nFigure 8.17: Details of Step 3 of the augmenting path algorithm.\n\n\n\n\n\nTermination\nEach shore has three nodes and we have found a matching with three edges. This implies that the unweighted matching number is \\(\\nu(G) = 3\\). We can stop the algorithm.\nNote that \\[\nw(M_3) = 13 = \\max\\{w(M_0), w(M_1), w(M_2), w(M_3)\\}.\n\\] Therefore, \\(M_3\\) is the maximum weight matching.\n\n\n\n8.2.8 A Final Remark\nThe extreme matching of maximum cardinality \\(M_\\nu(G)\\) is not necessarily the maximum weight matching!\n\nExample 8.11 Consider the graph given in Figure 8.18. This graph has maximum weight matching \\(M_1 = \\{12\\}\\) and maximum cardinality matching \\(M_2 = \\{02, 13\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(M_1 = \\{12\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(M_2 = \\{02, 13\\}\\)\n\n\n\n\n\n\n\nFigure 8.18: A graph where the maximum cardinality matching is not the maximum weight matching.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matching Problems</span>"
    ]
  },
  {
    "objectID": "08_Matching.html#the-assignment-problem",
    "href": "08_Matching.html#the-assignment-problem",
    "title": "8  Matching Problems",
    "section": "8.3 The Assignment Problem",
    "text": "8.3 The Assignment Problem\n\n8.3.1 Preliminaries\n\nDefinition 8.11 (The Assignment Problem) Given two groups with \\(k\\) items each and a pairwise cost \\(a_{ij}\\) for each \\(i,j = 1,2,\\dots, k\\), the assigment problem seeks an assignment of the objects from the first group to the second such that:\n\nEach object is assigned from group 1 to exactly one item in group 2.\nThe total assignment cost is minimized.\n\n\n\n8.3.1.1 Permutations\nA permutation is a bijective function \\(\\pi\\) mapping \\(\\{1,2,\\dots, k\\}\\) onto itself.\nThe assignment problem can be expressed in terms of permutations:\n\nGiven a \\(k\\times k\\) matrix \\(A\\), find a permutation \\(\\pi\\) of \\(\\{1,2,\\dots, k\\}\\) maximizing \\[\n  \\sum_{i=1}^k a_{i, \\pi(i)}.\n\\]\n\n\nExample 8.12 Suppose \\(k = 3\\) and consider the permutation \\(\\pi\\) and pairwise costs \\(A\\) given by \\[\n    \\pi(1) = 3, \\; \\pi(2) = 1, \\;\n    \\pi(3) = 2, \\hspace{0.25in}\n    A = \\begin{pmatrix}\n    -5 & 2 & 3 \\\\\n    4 & 1 & 3 \\\\\n    7 & 2 & -4\n    \\end{pmatrix}.\n\\] Then the cost of the assignment according to \\(\\pi\\) is \\[\na_{1, \\pi(1)} + a_{2, \\pi(2)} + a_{3,\\pi(3)}\n= a_{13} + a_{21} + a_{32} = 3 + 4 + 2 = 9.\n\\]\n\n\n\n\n8.3.2 Connection to Maximum Matchings\nWe can cast an instance of the assignment problem as an instance of maximum weighted matching problem:\n\nConstruct complete bipartite graph \\(G = (U \\cup V, E)\\) with \\[\n  U = \\{1,\\dots, k\\}, \\hspace{0.5in}\n  V = \\{1', \\dots, k'\\}.\n\\]\nReverse signs of costs \\(w_{ij} = -a_{ij}\\) to obtain a maximization problem.\nImpose constraint \\(|M| = v(G) = n/2 = k\\) to ensure assignment of all objects.\n\nIt suffices to apply the augmenting path algorithm for the maximum weighted matching problem with one change:\n\nThe minimum cost assignment is the last matching \\(M_{\\nu(G)}\\).\nThis ensures that the assignment constraint is met.\n\n\n\n8.3.3 Complexity of the Assignment Problem\nWe can solve the weighted matching problem using \\(O(n^2 m)\\) EOs using the augmenting path algorithm.\nQuestion: How does this translate to an instance size of the assignment problem?\n\nLet \\(n\\) is the number of vertices \\(G\\) in the bipartite representation of the assignment problem with \\(k\\) nodes in each shore \\(U\\) and \\(V\\): \\[\n  n = |U| + |V| = k + k = 2.\n\\]\n\\(G\\) is complete bipartite, so it has \\[\n  k^2 = |U| \\times |V|\n\\] edges.\n\nThis implies that we need \\[\n    O(n^2 m) = O(k^4)\n\\] elementary operations to solve the assignment problem. This is quadratic in the number, \\(k^2\\), of pairwise assignment costs. Thus, the modified augmenting path algorithm solves the assignment problem using at most a polynomial of its instance size operations.\n\n\n8.3.4 Example\n\nExample 8.13 Let’s solve the instance of the assignment problem given by cost matrix \\[\nA = \\begin{pmatrix}\n    -5 & 2 & 3 \\\\\n    4 & 1 & 3 \\\\\n    7 & 2 & -4\n    \\end{pmatrix}.\n\\]\n\n\nInitialization\nWe start by constructing the weight matrix \\(W\\) and constructing the corresponding instance of the maximum weighted matching problem: \\[\nW = - A = \\begin{pmatrix}\n    +5 & -2 & -3 \\\\\n    -4 & -1 & -3 \\\\\n    -7 & -2 & +4\n    \\end{pmatrix}.\n\\] Associating rows in \\(A\\) and \\(W\\) with nodes \\(0,1,2\\) and columns \\(3,4,5\\) yields an instance of the maximum weight matching problem corresponding to the graph \\(G\\) in Figure 8.19.\n\n\n\n\n\n\nFigure 8.19: Graph \\(G\\) corresponding to the assignment problem with costs \\(A\\).\n\n\n\n\n\nStep 1\nWe construct the auxiliary graph \\(G'\\) corresponding to the initial matching \\(M_0\\) with \\(0\\) edges. The shortest \\(st\\)-path in \\(G'\\) is \\((s,0,3,t)\\). We use the subpath \\(P_0 = (0,3)\\) to compute \\(M_1\\): \\[\n    M_1 = M_0 \\Delta P_0 = \\{03\\}; \\;\\;\\; w(M_1) = 5.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Matching \\(M_1\\)\n\n\n\n\n\n\n\nFigure 8.20: Details of the first iteration of the augmenting path algorithm.\n\n\n\n\n\nStep 2\nAfter updating the auxiliary graph, we see that shortest \\(st\\)-path in \\(G'\\) is \\((s,2,5,t)\\). We let \\(P_1 = (2,5)\\) and set \\[\n    M_2 = M_1 \\Delta P_1 = \\{03, 25\\}; \\;\\;\\ w(M_2) = 9.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Matching \\(M_2\\)\n\n\n\n\n\n\n\nFigure 8.21: Details of the second iteration of the augmenting path algorithm.\n\n\n\n\n\nStep 3\nThe shortest \\(st\\)-path in the auxiliary graph \\(G'\\) with respect to \\(M_2\\) is \\((s, 1, 4, t)\\). We let \\(P_2 = (1,4)\\) and obtain the matching \\[\n    M_3 = M_2 \\Delta P_2 = \\{03, 25, 14\\}\n\\] with \\(w(M_3) = 9 - 1 = 8\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Shortest \\(st\\)-path in \\(G'\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Matching \\(M_3\\)\n\n\n\n\n\n\n\nFigure 8.22: Details of the third iteration of the augmenting path algorithm.\n\n\n\n\n\nTermination\nSince \\(\\nu(G) = 3\\), we stop the algorithm.\n\nThe maximum weight matching is \\(M_2\\) with \\(w(M_2) = 9\\).\nThe minimum cost assignment is given by \\(M_3\\): \\[\n\\pi(0) = 0,\\;\\;\\; \\pi(1) = 1, \\;\\;\\; \\pi(2) = 2\n\\] with minimum cost \\[\n\\sum_{i=0}^2 a_{i, \\pi(i)} = a_{11} + a_{22} + a_{33} = - w(M_3) = -8.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matching Problems</span>"
    ]
  },
  {
    "objectID": "09_Complexity_Classes.html",
    "href": "09_Complexity_Classes.html",
    "title": "9  Complexity of Problems",
    "section": "",
    "text": "9.1 Complexity Classes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Complexity of Problems</span>"
    ]
  },
  {
    "objectID": "09_Complexity_Classes.html#complexity-classes",
    "href": "09_Complexity_Classes.html#complexity-classes",
    "title": "9  Complexity of Problems",
    "section": "",
    "text": "9.1.1 Problem Complexity\nRecall the Cobham-Edmonds thesis1: \\(P\\) is tractable or well-solved if\n\nThere is an algorithm \\(A\\) which solves it.\n\\(A\\) has computational complexity which is a polynomial function of the instance size of \\(P\\).\n\nWe have seen several problems which are tractable:\n\nminimum spanning tree\nshortest path\nmax flow / min cut problem\nmatching (unweighted, weighted, and assignment).\n\nIndeed, we have developed and analysed polynomial time algorithms for each of these problems earlier in the module.\nA foundational question in complexity theory is whether there are inherently difficult problems which cannot be solved in polynomial time?\n\nThere are many problems that nobody knows how to solve in polynomial time.\nThis doesn’t mean that it is impossible to solve them in polynomial time.\nIt just means we haven’t found a polynomial time algorithm yet.\n\n\n\n9.1.2 The PRIME Problem\nPRIME: given an integer \\(k\\), is \\(k\\) prime?\nIt was thought that PRIME did not have a polynomial time algorithm for a long time. However, Agrawak, Kayal, and Saxena2 found one in 2002 that requires \\(O \\Big( (\\log k)^{12} \\big( \\log (\\log k)^{12}\\big)^c\\Big)\\) elementary operations, for some \\(c \\ge 1\\).\n\n\n9.1.3 Goals of Complexity Theory\nWe want to build a theory that allows us to conclude that a problem does not have a polynomial time algorithm.\nThe best we can do today is develop theory that let’s us conclude that a problem is very unlikely to have a polynomial time algorithm.\n\n9.1.3.1 The Traveling Salesperson Problem (TSP)\nWe will repeatedly reference the traveling salesperson problem as an illustrative example.\n\nDefinition 9.1 A Hamiltonian circuit or cycle or tour of direted graph \\(G = (V,A)\\) is a cycle that visits each node exactly once\n\n\nDefinition 9.2 Traveling Salesperson Problem (TSP):3\n\nGiven graph \\(G\\) with arc costs \\(c_{ij} \\in \\mathbf{Z}_+\\).\nFind a Hamiltonian cycle of \\(G\\) with minimum cost.\n\n\nThe TSP has many applications, especially within logistics/transportation, circuit design, and scheduling.\n\n\n\n9.1.4 Types of Problems\nGiven problem instance \\(I\\) defined by set of feasible solutions \\(F\\) and cost function \\(c: F \\mapsto \\mathbf{R}\\).\n\nDefinition 9.3 (Decision Problem)  \n\nGiven \\(L \\in \\mathbf{R}\\), is there \\(X \\in F\\) such that \\(c(X) \\le L?\\)\nAlgorithm \\(A\\) takes input \\(I, L\\) and returns YES or NO.\nSearch version: \\(A\\) returns \\(X \\in F\\) with \\(c(X) \\le L\\) if one exists.\n\n\n\nDefinition 9.4 (Optimization Problems in Search Version)  \n\nAlgorithm \\(A\\) finds \\(X \\in F\\) minimizing \\(c(X)\\).\n\n\n\nExample 9.1 (Versions of TSP) We can formally define two versions of the TSP:\n\nTSP as an Optimization Problem:\n\nGiven directed graph \\(G = (V,A)\\) with costs \\(c_{ij} \\in \\mathbf{Z}_+\\) for each arc \\((i,j) \\in A\\):\nFind a Hamiltonian cycle with minimum total cost.\n\nTSP as a Decision Problem\n\nGiven directed graph \\(G\\), arc costs \\(c_{ij}\\), and integer \\(L\\):\nDoes \\(G\\) contain a Hamiltonian cycle of total cost at most \\(L\\)?\n\n\n\n\n\n9.1.5 Comparison of Complexity of Decision and Optimization Problems\nThis prompts a reasonable question:\n\nWhich is inherently more difficult?\nOptimization or decision problems?\n\nThe following theorem answers this fundamental question.\n\nTheorem 9.1 Any algorithm \\(A\\) for an optimization problem \\(P\\) solves the decision version \\(P_d\\).\n\nTheorem 9.1 has a natural corollary, which establishes that tractable optimization problems correspond to tractable decision problems.\n\nCorollary 9.1 If the optimization problem \\(P\\) is polynomial time solvable, then the decision problem \\(P_d\\) is also polynomial time solvable.\n\n\nProof. We have the following algorithm for solving the decision problem \\(P_d\\) given an algorithm \\(A\\) for solving optimization problem \\(P\\):\n\nSolve \\(P\\) using \\(A\\). Let \\(X^*\\) be the minimizer of \\(P\\) with minimum value \\(c(X^*)\\).\nCompare the optimal value to \\(L\\):\n\nReturn YES if \\(c(X^*) \\le L\\);\nOtherwise, return NO.\n\n\nIf \\(A\\) is polynomial time then this algorithm is also polynomial time.\n\nWe aim to categorize decision problems based on their inherent difficulty.\nWe will start by comparing two complexity classes, \\(\\mathbf{P}\\) and \\(\\mathbf{NP}\\).\n\n\n9.1.6 The Class \\(\\mathbf{P}\\)\n\nDefinition 9.5 We denote by \\(\\mathbf{P}\\) the class of all decision problems which can be solved in polynomial time for every instance \\(I\\)4.\n\nWe have seen several examples of problems belonging to \\(\\mathbf{P}\\). Indeed, the decision versions of the minimum spanning tree, shortest path, max flow, matching, and assignment problems are all in \\(\\mathbf{P}\\).\n\n\n9.1.7 The Class \\(\\mathbf{NP}\\)\n\nDefinition 9.6 We say a problem \\(\\mathbf{NP}\\) belongs to the complexity class P or nondeterministic polynomial5 if:\n\nEvery instance \\(I\\) of \\(P \\in \\mathbf{NP}\\) with answer YES (called a YES-instance), has a certificate which verifies that the instance has answer YES in polynomial time.\n\n\nWe are not interested in how to build the certificate; just whether we can verify in polynomial time.\n\nExample 9.2 (Complexity Class of TSP) The Traveling Salesperson Problem is in \\(\\mathbf{NP}\\).\nTo see that TSP is in \\(\\mathbf{NP}\\), we need to identify a certificate for an arbitrary YES-instance.\nConsider the following proposed certificate for instance of TSP with \\(G = (V,A)\\) and threshold \\(L\\):\n\nLet \\(S = (v_1, v_2, \\dots, v_n)\\) be a sequence of nodes such that\n\n\\(S\\) contains all nodes of \\(G\\) exactly once.\nPairs of consecutive nodes share an arc: \\((v_i, v_{i+1}) \\in A\\). Moreover, \\((v_n, v_1) \\in A\\).\nTotal cost of these arcs is \\[\n\\sum_{i=1}^n c((v_i, v_{i+1})) \\le L;\n  \\] here, we use \\(v_{n+1} = v_1\\).\n\n\nThis is a certificate! If given \\(S\\), we can that check that (1), (2), and (3) are satisfied in polynomial time.\n\nIndeed, \\(S\\) and \\((v_n, v_1)\\) define the desired Hamiltonian cycle in this case.\n\n\n\n\n9.1.8 Differences between \\(\\mathbf{P}\\) and \\(\\mathbf{NP}\\)\nThe class \\(\\mathbf{P}\\) contains polynomial time solvable problems:\n\n\\(\\mathbf{P}\\) contains decision problems to which a YES/NO answer can be given for any instance in polynomial time.\n\nOther other hand, \\(\\mathbf{NP}\\) contains polynomial time certifiable problems:\n\nYES-instances admit a certificate which can be verified in polynomial time.\n\n\n\n9.1.9 Turing Machines\n\nDefinition 9.7 A turing machine (TM)6 is a mathematical model of computer as an abstract machine that:\n\nmanipulates symbols from an infinite tape (can read and write),\naccording to table of rules implementing a finite-state automaton7.\n\nEverything a computer can do can be modeled by a TM.\n\n\\(\\mathbf{P}\\) is the set of problems that can be solved in polynomial time by a TM.\n\\(\\mathbf{NP}\\) is the class of problems that can be solved (not just certified) in polynomial time by a nondeterministic TM8.\n\n\n\n\n9.1.10 The Relationship Between \\(\\mathbf{P}\\) and \\(\\mathbf{NP}\\)\nIt is known that \\(\\mathbf{P}\\) is a subset of \\(\\mathbf{NP}\\). That is, every problem in \\(\\mathbf{P}\\) also belongs to \\(\\mathbf{NP}\\).\nWe have the following theorem.\n\nTheorem 9.2 \\(P \\subseteq NP\\)\n\n\nProof. Suppose that problem \\(Q\\) in \\(\\mathbf{P}\\), i.e., \\(Q \\in \\mathbf{P}\\). This implies that there is a polynomial time algorithm \\(A\\) for \\(Q\\).\nThe steps of \\(A\\) act as a certificate:\n\nIf \\(A\\) solves search version of \\(Q\\), then the solution returned by \\(A\\) satisfies the conditions for YES response.\nIf \\(A\\) only returns YES or NO applying \\(A\\) and obtaining response YES is the certificate.\n\n\n\n9.1.10.1 Does \\(\\mathbf{P} = \\mathbf{NP}\\) or \\(\\mathbf{P} \\neq \\mathbf{NP}?\\)\n\nConjecture 9.1 It is widely conjectured that there is at least one problem in \\(\\mathbf{NP}\\) but not in \\(\\mathbf{P}\\), i.e., \\(\\mathbf{P} \\neq \\mathbf{NP}\\).\n\n\nFalsifying Conjecture 9.1 would prove that \\(\\mathbf{P} = \\mathbf{NP}\\).\nDeterminining if \\(\\mathbf{P} = \\mathbf{NP}\\) or \\(\\mathbf{P} = \\mathbf{NP}\\) is one of the seven Millenium Prize Problems9.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Complexity of Problems</span>"
    ]
  },
  {
    "objectID": "09_Complexity_Classes.html#footnotes",
    "href": "09_Complexity_Classes.html#footnotes",
    "title": "9  Complexity of Problems",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Cobham's_thesis↩︎\nhttps://en.wikipedia.org/wiki/AKS_primality_test,https://people.engr.tamu.edu/andreas-klappenecker/629/aks.pdf↩︎\nhttp://www.math.uwaterloo.ca/~bico/↩︎\nhttps://en.wikipedia.org/wiki/P_(complexity)↩︎\nhttps://en.wikipedia.org/wiki/NP_(complexity)↩︎\nhttps://en.wikipedia.org/wiki/Turing_machine↩︎\nhttps://en.wikipedia.org/ewiki/Finite-state_machine↩︎\nhttps://en.wikipedia.org/wiki/Nondeterministic_Turing_machine↩︎\nhttps://www.claymath.org/millennium-problems/↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Complexity of Problems</span>"
    ]
  }
]